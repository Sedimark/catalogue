{"results": [{"title": "2000 HUB5 English", "short_description": "**2000 HUB5 English Evaluation Transcripts** was developed by the Linguistic Data Consortium (LDC)  and consists of transcripts of 40 English telephone conversations used in the 2000 HUB5 evaluation sponsored by NIST (National Institute of Standards and Technology). \r\n\r\nThe Hub5 evaluation series focused on conversational speech over the telephone with the particular task of transcribing conversational speech into text. Its goals were to explore promising new areas in the recognition of conversational speech, to develop advanced technology incorporating those ideas and to measure the performance of new technology.", "price": 614, "created_at": "2025-01-09 13:44:40.859209", "keyword": ["Speech Recognition"], "id": "b17a2b68-ef5a-47ea-a9bd-bef85e26c684", "image_url": ""}, {"title": "20NewsGroups", "short_description": "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.", "price": 181, "created_at": "2025-01-09 13:44:40.859384", "keyword": ["Topic Models", "Intrusion Detection", "Unsupervised Text Classification"], "id": "2d7411f5-e606-4a6b-ba64-767ec1fcdb5d", "image_url": ""}, {"title": "300W", "short_description": "The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as \u201cparty\u201d, \u201cconference\u201d, \u201cprotests\u201d, \u201cfootball\u201d and \u201ccelebrities\u201d. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common \u201cneutral\u201d or \u201csmile\u201d, such as \u201csurprise\u201d or \u201cscream\u201d.\r\nImages were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases.\r\nMany images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 \u00d7 292) pixels.\r\n\r\nSource: [https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf](https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf)\r\nImage Source: [https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg](https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg)", "price": 148, "created_at": "2025-01-09 13:44:40.859510", "keyword": ["Pose Estimation", "Face Alignment", "3D Reconstruction", "Facial Landmark Detection", "2D Pose Estimation", "Unsupervised Facial Landmark Detection", "Pose Estimation", "Face Alignment", "3D Reconstruction", "Facial Landmark Detection", "2D Pose Estimation", "Unsupervised Facial Landmark Detection"], "id": "b039c32e-ccb3-4042-8421-ded58b6b537b", "image_url": "https://production-media.paperswithcode.com/datasets/300W-0000000526-c58ba041_dullgwb.jpeg"}, {"title": "3DFAW", "short_description": "**3DFAW** contains 23k images with 66 3D face keypoint annotations.\n\nSource: [Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild](https://arxiv.org/abs/1911.11130)\nImage Source: [http://mhug.disi.unitn.it/workshop/3dfaw/](http://mhug.disi.unitn.it/workshop/3dfaw/)", "price": 586, "created_at": "2025-01-09 13:44:40.859623", "keyword": ["Face Alignment", "3D Facial Landmark Localization"], "id": "c2ddc2b5-1464-440b-bd6b-5d116f9675af", "image_url": "https://production-media.paperswithcode.com/datasets/3DFAW-0000001511-a1828690_FNhXQCy.jpg"}, {"title": "3DMatch", "short_description": "The 3DMATCH benchmark evaluates how well descriptors (both 2D and 3D) can establish correspondences between RGB-D frames of different views. The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences. \r\n\r\nThe pixel size of each 2D patch is determined by the projection of the 0.3m3 local 3D patch around the interest point onto the image plane. \r\n\r\nSource: [3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions](/paper/3dmatch-learning-local-geometric-descriptors)", "price": 121, "created_at": "2025-01-09 13:44:40.859731", "keyword": ["Low-Light Image Enhancement", "Point Cloud Registration", "3D Feature Matching"], "id": "5a9b5100-fa1e-49ac-8e1a-30233e87f843", "image_url": "https://production-media.paperswithcode.com/datasets/patch-rgb.jpg"}, {"title": "3DPW", "short_description": "The **3D Poses in the Wild dataset** is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera.\r\n\r\nThe dataset includes:\r\n\r\n* 60 video sequences.\r\n* 2D pose annotations.\r\n* 3D poses obtained with the method introduced in the paper.\r\n* Camera poses for every frame in the sequences.\r\n* 3D body scans and 3D people models (re-poseable and re-shapeable). Each sequence contains its corresponding models.\r\n* 18 3D models in different clothing variations.\r\n\r\nSource: [https://virtualhumans.mpi](http://virtualhumans.mpi-inf.mpg.de/3DPW)\r\nImage Source: [https://virtualhumans.mpi](http://virtualhumans.mpi-inf.mpg.de/3DPW)", "price": 896, "created_at": "2025-01-09 13:44:40.859840", "keyword": ["Pose Estimation", "3D Human Pose Estimation", "Hand Pose Estimation", "Human Pose Forecasting", "Cross-domain 3D Human Pose Estimation"], "id": "f4d40089-b428-469a-be67-0de242334ef3", "image_url": "https://production-media.paperswithcode.com/datasets/3DPW-0000001300-1e5b038c_JeZ4ho8.jpg"}, {"title": "4DFAB", "short_description": "4DFAB is a large scale database of dynamic high-resolution 3D faces which consists of recordings of 180 subjects captured in four different sessions spanning over a five-year period (2012 - 2017), resulting in a total of over 1,800,000 3D meshes. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour.\r\n\r\nSource: [ibug](https://ibug.doc.ic.ac.uk/resources/4dfab/)", "price": 106, "created_at": "2025-01-09 13:44:40.859949", "keyword": ["Facial Expression Recognition (FER)", "3D Shape Reconstruction", "Action Unit Detection"], "id": "f3aae7fd-d8c9-4059-b423-ceb128cef322", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-21_at_16.00.50.png"}, {"title": "7-Scenes", "short_description": "The **7-Scenes** dataset is a collection of tracked RGB-D camera frames. The dataset may be used for evaluation of methods for different applications such as dense tracking and mapping and relocalization techniques.\nAll scenes were recorded from a handheld Kinect RGB-D camera at 640\u00d7480 resolution. The dataset creators use an implementation of the KinectFusion system to obtain the \u2018ground truth\u2019 camera tracks, and a dense 3D model. Several sequences were recorded per scene by different users, and split into distinct training and testing sequence sets.\n\nSource: [https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)\nImage Source: [https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)", "price": 214, "created_at": "2025-01-09 13:44:40.860051", "keyword": [], "id": "5329e2cd-c379-4619-8ac6-7fb131f147d9", "image_url": "https://production-media.paperswithcode.com/datasets/7-Scenes-0000003518-35263e94.jpg"}, {"title": "A2D", "short_description": "A2D (Actor-Action Dataset) is a dataset for simultaneously inferring actors and actions in videos. A2D has seven actor classes (adult, baby, ball, bird, car, cat, and dog) and eight action classes (climb, crawl, eat, fly, jump, roll, run, and walk) not including the no-action class, which we also consider. The A2D has 3,782 videos with at least 99 instances per valid actor-action tuple and videos are labeled with both pixel-level actors and actions for sampled frames. The A2D dataset serves as a large-scale testbed for various vision problems: video-level single- and multiple-label actor-action recognition, instance-level object segmentation/co-segmentation, as well as pixel-level actor-action semantic segmentation to name a few.", "price": 421, "created_at": "2025-01-09 13:44:40.860151", "keyword": ["Object Detection"], "id": "acdda8c5-9aba-49e3-9090-ade48a01ca74", "image_url": "https://production-media.paperswithcode.com/datasets/dataset_montage.jpg"}, {"title": "AADB", "short_description": "Contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. \r\n\r\nSource: [Photo Aesthetics Ranking Network with Attributes and Content Adaptation](/paper/photo-aesthetics-ranking-network-with)", "price": 407, "created_at": "2025-01-09 13:44:40.860252", "keyword": ["Age Estimation", "Aesthetics Quality Assessment", "Image Cropping"], "id": "8be2b01c-cdbe-4bef-a12d-96e214e0c058", "image_url": ""}, {"title": "ACL ARC", "short_description": "ACL Anthology Reference Corpus (ACL ARC) is a collection of 10,920 academic papers from the ACL Anthology. ACL ARC is cleaned to remove:\r\n\r\n- files that look like not full papers, paper fragments, foreign-language papers (e.g., French), or pure junk.\r\n- headers (title and author information; NOT abstract).\r\n- footers (\"References\" line and the actual references).\r\n- some bad characters (spurious characters).\r\n- some page numbers (i.e., a single number appearing on a line, with nothing else attached to it).\r\n- significant foreign-language (e.g., French) content in an otherwise English paper.\r\n\r\nThe cleaned corpus has 10,628 documents.\r\n\r\nSource: [ACL ARC](https://web.eecs.umich.edu/~lahiri/acl_arc.html)", "price": 371, "created_at": "2025-01-09 13:44:40.860352", "keyword": ["Citation Recommendation", "Sentence Classification", "Continual Pretraining", "Citation Intent Classification"], "id": "4b47fde6-5e4e-4d6b-8798-d4cbab98a334", "image_url": ""}, {"title": "ADE20K", "short_description": "The **ADE20K** semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.\r\n\r\nSource: [Cooperative Image Segmentation and Restoration in Adverse Environmental Conditions](https://arxiv.org/abs/1911.00679)\r\nImage Source: [https://groups.csail.mit.edu/vision/datasets/ADE20K/](https://groups.csail.mit.edu/vision/datasets/ADE20K/)", "price": 34, "created_at": "2025-01-09 13:44:40.860450", "keyword": ["Semantic Segmentation", "Instance Segmentation", "Image-to-Image Translation", "Semi-Supervised Semantic Segmentation", "Reconstruction", "Scene Understanding", "Panoptic Segmentation", "Face Detection", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Open Vocabulary Semantic Segmentation", "Scene Recognition", "Pose Transfer", "Zero-Shot Semantic Segmentation", "Continual Semantic Segmentation", "Overlapped 100-50", "Overlapped 50-50", "Overlapped 100-10", "Overlapped 100-5", "Overlapped 25-25", "Open Vocabulary Panoptic Segmentation", "Semantic Segmentation", "Instance Segmentation", "Image-to-Image Translation", "Semi-Supervised Semantic Segmentation", "Reconstruction", "Scene Understanding", "Panoptic Segmentation", "Face Detection", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Open Vocabulary Semantic Segmentation", "Scene Recognition", "Pose Transfer", "Zero-Shot Semantic Segmentation", "Continual Semantic Segmentation", "Overlapped 100-50", "Overlapped 50-50", "Overlapped 100-10", "Overlapped 100-5", "Overlapped 25-25", "Open Vocabulary Panoptic Segmentation"], "id": "c0f83498-7699-46fa-ad92-d767c75e2816", "image_url": "https://production-media.paperswithcode.com/datasets/ADE20K-0000000451-1ddfaab3_axk8A9h.jpg"}, {"title": "AFAD", "short_description": "The Asian Face Age Dataset (AFAD) is a new dataset proposed for evaluating the performance of age estimation, which contains more than 160K facial images and the corresponding age and gender labels. This dataset is oriented to age estimation on Asian faces, so all the facial images are for Asian faces. It is noted that the AFAD is the biggest dataset for age estimation to date. It is well suited to evaluate how deep learning methods can be adopted for age estimation.", "price": 372, "created_at": "2025-01-09 13:44:40.860548", "keyword": ["Age Estimation"], "id": "a0363aac-f7d2-4399-99f7-94301143a5d5", "image_url": ""}, {"title": "AFEW-VA", "short_description": "The AFEW-VA databaset is a collection of highly accurate per-frame annotations levels of valence and arousal, along with per-frame annotations of 68 facial landmarks for 600 challenging video clips. These clips are extracted from feature films and were also annotated in terms of discrete emotion categories in the form of the AFEW database (that can be obtained [there](https://cs.anu.edu.au/few/AFEW.html)).\r\n\r\nSource: [AFEW-VA](https://ibug.doc.ic.ac.uk/resources/afew-va-database/)", "price": 365, "created_at": "2025-01-09 13:44:40.860647", "keyword": [], "id": "9be19ee1-e823-4a9f-a155-c6cc8e1ca4ab", "image_url": "https://production-media.paperswithcode.com/datasets/afew-va_dataset.png"}, {"title": "AFLW", "short_description": "The **Annotated Facial Landmarks in the Wild** (**AFLW**) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image.\r\n\r\nSource: [Nose, Eyes and Ears: Head Pose Estimation by Locating Facial Keypoints](https://arxiv.org/abs/1812.00739)", "price": 902, "created_at": "2025-01-09 13:44:40.860744", "keyword": ["Face Alignment", "Low-Light Image Enhancement", "Facial Landmark Detection", "Head Pose Estimation", "Unsupervised Facial Landmark Detection", "Face Alignment", "Low-Light Image Enhancement", "Facial Landmark Detection", "Head Pose Estimation", "Unsupervised Facial Landmark Detection"], "id": "acdcf346-110c-4446-b266-068102665380", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.06.21_PM.png"}, {"title": "AFLW2000-3D", "short_description": "**AFLW2000-3D** is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.\r\n\r\nSource: [https://www.tensorflow.org/datasets/catalog/aflw2k3d](https://www.tensorflow.org/datasets/catalog/aflw2k3d)\r\nImage Source: [http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm)", "price": 841, "created_at": "2025-01-09 13:44:40.860843", "keyword": ["3D Face Reconstruction", "Face Alignment", "Facial Landmark Detection", "Head Pose Estimation", "3D Facial Landmark Localization", "Face Swapping", "3D Face Alignment"], "id": "2731569e-0893-44bc-b464-7980ee2204e6", "image_url": "https://production-media.paperswithcode.com/datasets/AFLW2000-3D-0000000545-76d6b875_KNCUxRx.jpg"}, {"title": "AFW", "short_description": "**AFW** (**Annotated Faces in the Wild**) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.\r\n\r\nSource: [Face detection, pose estimation, and landmark localization in the wild](https://ieeexplore.ieee.org/document/6248014)", "price": 473, "created_at": "2025-01-09 13:44:40.860941", "keyword": ["Face Alignment", "Facial Landmark Detection", "Face Detection", "Face Alignment", "Facial Landmark Detection", "Face Detection"], "id": "47462184-90eb-4afb-88fd-04318f8fa73c", "image_url": ""}, {"title": "AG News", "short_description": "**AG News** (**AG\u2019s News Corpus**) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (\u201cWorld\u201d, \u201cSports\u201d, \u201cBusiness\u201d, \u201cSci/Tech\u201d) of AG\u2019s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.\r\n\r\nSource: [https://arxiv.org/pdf/1509.01626.pdf](https://arxiv.org/pdf/1509.01626.pdf)", "price": 813, "created_at": "2025-01-09 13:44:40.861039", "keyword": ["Text Classification", "Anomaly Detection", "Stochastic Optimization", "Topic Models", "Zero-Shot Text Classification", "Short Text Clustering", "Unsupervised Text Classification", "Continual Pretraining", "Semi-Supervised Text Classification"], "id": "d19d05a4-d53a-4e2d-aaff-278df2e734e3", "image_url": "https://production-media.paperswithcode.com/datasets/AG_News-0000000315-9d0ee144_8aP13gM.jpg"}, {"title": "AI2-THOR", "short_description": "AI2-Thor is an interactive environment for embodied AI. It contains four types of scenes, including kitchen, living room, bedroom and bathroom, and each scene includes 30 rooms, where each room is unique in terms of furniture placement and item types. There are over 2000 unique objects for AI agents to interact with.\r\n\r\nSource: [Learning Object Relation Graph andTentative Policy for Visual Navigation](https://arxiv.org/abs/2007.11018)\r\nImage Source: [https://ai2thor.allenai.org/](https://ai2thor.allenai.org/)", "price": 964, "created_at": "2025-01-09 13:44:40.861138", "keyword": ["Visual Navigation", "Imitation Learning"], "id": "785edd06-2844-460a-8b2e-2487d62bc94e", "image_url": "https://production-media.paperswithcode.com/datasets/AI2-THOR-0000003539-8447aaa6.jpg"}, {"title": "AI2D", "short_description": "AI2 Diagrams (AI2D) is a dataset of over 5000 grade school science diagrams with over 150000 rich annotations, their ground truth syntactic parses, and more than 15000 corresponding multiple choice questions.\r\n\r\nSource: [A Diagram Is Worth A Dozen Images](https://arxiv.org/pdf/1603.07396.pdf)\r\nImage Source: [Kembhavi et al](https://arxiv.org/pdf/1603.07396.pdf)", "price": 466, "created_at": "2025-01-09 13:44:40.861241", "keyword": ["Image Classification", "Question Answering", "Visual Question Answering (VQA)"], "id": "0fbf00c7-5f19-4f37-a99e-a31c514850b5", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-25_at_17.09.57.png"}, {"title": "AIDS", "short_description": "**AIDS** is a graph dataset. It consists of 2000 graphs representing molecular compounds which are constructed from the AIDS Antiviral Screen Database of Active Compounds. It contains 4395 chemical compounds, of which 423 belong to class CA, 1081 to CM, and the remaining compounds to CI.\r\n\r\nSource: [DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model](https://arxiv.org/abs/1712.03563)\r\nImage Source: [https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222](https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222)", "price": 49, "created_at": "2025-01-09 13:44:40.861340", "keyword": ["Graph Classification"], "id": "8d50e651-dd47-4e6c-8845-bda2b5de1803", "image_url": "https://production-media.paperswithcode.com/datasets/AIDS-0000001921-6df592cc_hUKgGHQ.jpg"}, {"title": "AIRS", "short_description": "The **AIRS** (Aerial Imagery for Roof Segmentation) dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. \r\n\r\nSource: [Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings](/paper/aerial-imagery-for-roof-segmentation-a-large)", "price": 13, "created_at": "2025-01-09 13:44:40.861441", "keyword": ["Semantic Segmentation", "Instance Segmentation", "Scene Understanding"], "id": "01793cbf-f3af-4af8-adfa-44ec7f37326a", "image_url": ""}, {"title": "AISHELL-1", "short_description": "AISHELL-1 is a corpus for speech recognition research and building speech recognition systems for Mandarin. \r\n\r\nSource: [AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline](/paper/aishell-1-an-open-source-mandarin-speech)", "price": 663, "created_at": "2025-01-09 13:44:40.861539", "keyword": ["Speech Recognition", "Language Modelling"], "id": "3a2a31de-64b8-4bde-b3c4-014e7618d900", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_8.49.56_PM.png"}, {"title": "AMASS", "short_description": "AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning.", "price": 73, "created_at": "2025-01-09 13:44:40.861637", "keyword": ["Pose Estimation", "3D Human Pose Estimation", "Human Pose Forecasting"], "id": "422d0004-9383-471f-b30a-243929f74291", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_14.25.22.png"}, {"title": "AMiner", "short_description": "The **AMiner** Dataset is a collection of different relational datasets. It consists of a set of relational networks such as citation networks, academic social networks or topic-paper-autor networks among others.", "price": 116, "created_at": "2025-01-09 13:44:40.861735", "keyword": ["Node Classification", "Link Prediction", "Research Performance Prediction"], "id": "076b39de-4fbc-490e-9b7a-94460d769dfb", "image_url": ""}, {"title": "AOLP", "short_description": "The application-oriented license plate (**AOLP**) benchmark database has 2049 images of Taiwan license plates. This database is categorized into three subsets: access control (AC) with 681 samples, traffic law enforcement (LE) with 757 samples, and road patrol (RP) with 611 samples. AC refers to the cases that a vehicle passes a fixed passage with a lower speed or full stop. This is the easiest situation. The images are captured under different illuminations and different weather conditions. LE refers to the cases that a vehicle violates traffic laws and is captured by roadside camera. The background are really cluttered, with road sign and multiple plates in one image. RP refers to the cases that the camera is held on a patrolling vehicle, and the images are taken with arbitrary viewpoints and distances.\n\nSource: [Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs](https://arxiv.org/abs/1601.05610)\nImage Source: [http://aolpr.ntust.edu.tw/lab/index.html](http://aolpr.ntust.edu.tw/lab/index.html)", "price": 433, "created_at": "2025-01-09 13:44:40.861839", "keyword": ["License Plate Recognition"], "id": "fd0ea7f1-e21b-4605-90f3-77d9df688d5d", "image_url": "https://production-media.paperswithcode.com/datasets/AOLP-0000003399-b3853485.jpg"}, {"title": "AQA-7", "short_description": "Consists of 1106 action samples from seven actions with quality scores as measured by expert human judges.\r\n\r\nSource: [Action Quality Assessment Across Multiple Actions](/paper/action-quality-assessment-across-multiple)", "price": 474, "created_at": "2025-01-09 13:44:40.861937", "keyword": ["Multi-Task Learning", "Action Quality Assessment", "Skills Assessment", "Skills Evaluation"], "id": "f7ccb97e-48d7-43e0-8457-697e05c79d25", "image_url": "https://production-media.paperswithcode.com/datasets/maqa.png"}, {"title": "ASPEC", "short_description": "**ASPEC**, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010.\r\n\r\nSource: [ASPEC](http://lotus.kuee.kyoto-u.ac.jp/ASPEC/)\r\nImage Source: [https://www.aclweb.org/anthology/L16-1350.pdf](https://www.aclweb.org/anthology/L16-1350.pdf)", "price": 165, "created_at": "2025-01-09 13:44:40.862035", "keyword": ["Domain Adaptation", "Machine Translation", "Low-Resource Neural Machine Translation"], "id": "c05a7e04-3c74-40da-a46c-db5886a9de52", "image_url": "https://production-media.paperswithcode.com/datasets/ASPEC-0000003581-b3d7801d.jpg"}, {"title": "ASTD", "short_description": "Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.\r\n\r\nSource: [ASTD: Arabic Sentiment Tweets Dataset](/paper/astd-arabic-sentiment-tweets-dataset)", "price": 744, "created_at": "2025-01-09 13:44:40.862133", "keyword": ["Sentiment Analysis"], "id": "89f4fc01-d47d-4f63-850f-d0dd18da6265", "image_url": ""}, {"title": "ATOMIC", "short_description": "**ATOMIC** is an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., \"if X pays Y a compliment, then Y will likely return the compliment\").\r\n\r\nSource: [ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning](/paper/atomic-an-atlas-of-machine-commonsense-for-if)\r\nImage Source: [https://homes.cs.washington.edu/~msap/atomic/](https://homes.cs.washington.edu/~msap/atomic/)", "price": 662, "created_at": "2025-01-09 13:44:40.862230", "keyword": ["Question Answering", "Language Modelling", "Knowledge Graphs"], "id": "038de4fe-146d-4a97-b3c3-0cab4a008d95", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_3.24.25_PM.png"}, {"title": "AVA", "short_description": "**AVA** is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:\r\n\r\n- Kinetics (AVA-Kinetics) - a crossover between AVA and Kinetics. In order to provide localized action labels on a wider variety of visual scenes, authors provide AVA action labels on videos from Kinetics-700, nearly doubling the number of total annotations, and increasing the number of unique videos by over 500x. \r\n- Actions (AvA Actions) - the AVA dataset densely annotates 80 atomic visual actions in 430 15-minute movie clips, where actions are localized in space and time, resulting in 1.62M action labels with multiple labels per human occurring frequently. \r\n- Spoken Activity (AVA ActiveSpeaker, AVA Speech). AVA ActiveSpeaker: associates speaking activity with a visible face, on the AVA v1.0 videos, resulting in 3.65 million frames labeled across ~39K face tracks. AVA Speech densely annotates audio-based speech activity in AVA v1.0 videos, and explicitly labels 3 background noise conditions, resulting in ~46K labeled segments spanning 45 hours of data.\r\nImage Source: [https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png](https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png)", "price": 337, "created_at": "2025-01-09 13:44:40.862328", "keyword": ["Action Recognition", "Action Detection", "Action Recognition In Videos", "Speech Enhancement", "Video Understanding", "Speaker Diarization", "Gaze Estimation", "Self-Supervised Learning", "Aesthetics Quality Assessment", "Audio-Visual Active Speaker Detection", "Spatio-Temporal Action Localization", "Activity Detection"], "id": "ab5c3ab2-a60d-4d19-989a-e0fea1fe0ff8", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_10.47.15_AM.png"}, {"title": "AVD", "short_description": "AVD focuses on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes.\r\n\r\nSource: [A Dataset for Developing and Benchmarking Active Vision](/paper/a-dataset-for-developing-and-benchmarking)", "price": 72, "created_at": "2025-01-09 13:44:40.862424", "keyword": ["Object Detection", "Domain Adaptation", "Visual Navigation", "Scene Generation"], "id": "5702f367-5aec-46c0-90f0-35012c8ea5a5", "image_url": "https://production-media.paperswithcode.com/datasets/av.jpg"}, {"title": "AVSD", "short_description": "The Audio Visual Scene-Aware Dialog (**AVSD**) dataset, or DSTC7 Track 3, is a audio-visual dataset for dialogue understanding. The goal with the dataset and track was to design systems to generate responses in a dialog about a video, given the dialog history and audio-visual content of the video.\n\nSource: [The Eighth Dialog System Technology Challenge](https://arxiv.org/abs/1911.06394)\nImage Source: [http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf](http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf)", "price": 594, "created_at": "2025-01-09 13:44:40.862527", "keyword": ["Scene-Aware Dialogue"], "id": "3d114d37-8413-4fa1-bd1c-12269c2b58ff", "image_url": "https://production-media.paperswithcode.com/datasets/AVSD-0000002954-6b14dcf7.jpg"}, {"title": "ActivityNet", "short_description": "The **ActivityNet** dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\r\n\r\nSource: [Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection](https://arxiv.org/abs/1808.02536)", "price": 642, "created_at": "2025-01-09 13:44:40.862626", "keyword": ["Action Recognition", "Temporal Action Localization", "Action Detection", "Video Retrieval", "Action Classification", "Action Recognition In Videos", "Weakly Supervised Action Localization", "Zero-Shot Video Retrieval", "Zero-Shot Action Recognition", "GZSL Video Classification", "Temporal Action Proposal Generation", "ZSL Video Classification", "Few Shot Temporal Action Localization", "Semi-Supervised Action Detection", "Zero-Shot Action Detection"], "id": "fb030b2b-ea42-4e43-a11e-fb69f634246c", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_1.03.10_PM.png"}, {"title": "Adience", "short_description": "The **Adience** dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few.\r\n\r\nSource: [Understanding and Comparing Deep Neural Networksfor Age and Gender Classification](https://arxiv.org/abs/1708.07689)\r\nImage Source: [https://talhassner.github.io/home/projects/Adience/Adience-data.html](https://talhassner.github.io/home/projects/Adience/Adience-data.html)", "price": 561, "created_at": "2025-01-09 13:44:40.862724", "keyword": ["Face Recognition", "Age Estimation", "Face Quality Assessement", "Age And Gender Classification"], "id": "9d1a61fd-5f3e-47fb-b376-c3bad819dbea", "image_url": "https://production-media.paperswithcode.com/datasets/Adience-0000002380-56d2b643_cybIPdQ.jpg"}, {"title": "AffectNet", "short_description": "**AffectNet** is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.\r\n\r\nSource: [Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition](https://arxiv.org/abs/2007.10298)\r\nImage Source: [http://mohammadmahoor.com/affectnet/](http://mohammadmahoor.com/affectnet/)", "price": 822, "created_at": "2025-01-09 13:44:40.862821", "keyword": ["Facial Expression Recognition (FER)", "Arousal Estimation", "Valence Estimation"], "id": "a02aa3cd-56ca-4497-882c-63f7a9c7fa9e", "image_url": "https://production-media.paperswithcode.com/datasets/AffectNet-0000001484-9dabde74_5sUXPfJ.jpg"}, {"title": "Ali-CCP", "short_description": "This data set is provided by Alimama", "price": 286, "created_at": "2025-01-09 13:44:40.862920", "keyword": [], "id": "1ed34f35-ccd1-4183-857a-ecf9fe0e01ed", "image_url": ""}, {"title": "Amazon Fine Foods", "short_description": "Amazon Fine Foods is a dataset that consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review.\r\n\r\nSource: [https://snap.stanford.edu/data/web-FineFoods.html](https://snap.stanford.edu/data/web-FineFoods.html)", "price": 775, "created_at": "2025-01-09 13:44:40.863020", "keyword": ["Sentiment Analysis", "Recommendation Systems", "Data Augmentation"], "id": "e1e17fe6-0295-4096-a6bd-3bbdb315add7", "image_url": ""}, {"title": "Amazon Product Data", "short_description": "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\r\n\r\nThis dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).", "price": 954, "created_at": "2025-01-09 13:44:40.863123", "keyword": ["Text Classification", "Domain Adaptation", "Recommendation Systems", "Stochastic Optimization", "Wildly Unsupervised Domain Adaptation", "Topic Classification"], "id": "1549c1ee-2772-4540-bb07-71030fd94753", "image_url": ""}, {"title": "Amazon Review", "short_description": "Amazon Review is a dataset to tackle the task of identifying whether the sentiment of a product review is positive or negative. This dataset includes reviews from four different merchandise categories: Books (B) (2834 samples), DVDs (D) (1199 samples), Electronics (E) (1883 samples), and Kitchen and housewares (K) (1755 samples).", "price": 170, "created_at": "2025-01-09 13:44:40.863222", "keyword": ["Recommendation Systems", "Causal Language Modeling", "Sequential Recommendation"], "id": "90412e6a-4f7a-4bad-a23d-dc89de6561d2", "image_url": ""}, {"title": "Arcade Learning Environment", "short_description": "The **Arcade Learning Environment** (ALE) is an object-oriented framework that allows researchers to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design.\r\n\r\nSource: [https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\r\nImage Source: [https://github.com/muupan/async-rl/blob/master/README.md](https://github.com/muupan/async-rl/blob/master/README.md)", "price": 479, "created_at": "2025-01-09 13:44:40.863320", "keyword": ["Atari Games", "Montezuma's Revenge", "Atari Games", "Montezuma's Revenge"], "id": "7747c0f2-0ce3-4a82-ad7f-aaf6cb612eb4", "image_url": "https://production-media.paperswithcode.com/datasets/Arcade_Learning_Environment-0000003337-6b3cb00a.gif"}, {"title": "Argoverse", "short_description": "**Argoverse** is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called \u201cagent\u201d, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap.\r\n\r\nSource: [Learning Lane Graph Representations for Motion Forecasting](https://arxiv.org/abs/2007.13732)\r\nImage Source: [https://arxiv.org/pdf/1911.02620.pdf](https://arxiv.org/pdf/1911.02620.pdf)", "price": 745, "created_at": "2025-01-09 13:44:40.863418", "keyword": ["3D Object Detection", "Trajectory Prediction", "Motion Forecasting", "Monocular Cross-View Road Scene Parsing(Road)", "3D Object Tracking", "Monocular Cross-View Road Scene Parsing(Vehicle)"], "id": "59f63a2f-76c5-4f4a-8720-90577fc0d362", "image_url": "https://production-media.paperswithcode.com/datasets/Argoverse-0000001465-bf86e95f_s5jJlV5.jpg"}, {"title": "Arxiv GR-QC", "short_description": "**Arxiv GR-QC** (General Relativity and Quantum Cosmology) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to General Relativity and Quantum Cosmology category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.\n\nSource: [https://snap.stanford.edu/data/ca-GrQc.html](https://snap.stanford.edu/data/ca-GrQc.html)", "price": 49, "created_at": "2025-01-09 13:44:40.863517", "keyword": ["Link Prediction", "Clique Prediction", "Active Learning"], "id": "d1bc1e46-a52b-4ce5-9386-9f2cceb08413", "image_url": ""}, {"title": "AudioSet", "short_description": "Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.\r\n\r\nSource: [Curriculum Audiovisual Learning](https://arxiv.org/abs/2001.09414)", "price": 150, "created_at": "2025-01-09 13:44:40.863624", "keyword": ["Audio Classification", "Audio Source Separation", "Multi-modal Classification", "Zero-shot Audio Classification", "Audio Tagging"], "id": "b8ddecc0-3716-4a4e-acd5-0086b0615604", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.31.55_PM.png"}, {"title": "AwA", "short_description": "**Animals with Attributes** (**AwA**) was a dataset for benchmarking transfer-learning algorithms, in particular attribute base classification. It consisted of 30475 images of 50 animals classes with six pre-extracted feature representations for each image. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class. Using the shared attributes, it is possible to transfer information between different classes.\r\nThe Animals with Attributes dataset was suspended. Its images are not available anymore because of copyright restrictions. A drop-in replacement, Animals with Attributes 2, is available instead.\r\n\r\nSource: [Transductive Multi-view Zero-Shot Learning](https://arxiv.org/abs/1501.04560)\r\nImage Source: [https://cvml.ist.ac.at/AwA/](https://cvml.ist.ac.at/AwA/)", "price": 114, "created_at": "2025-01-09 13:44:40.863727", "keyword": ["Zero-Shot Learning", "Few-Shot Image Classification", "Generalized Few-Shot Learning", "Generalized Zero-Shot Learning", "Long-tail learning with class descriptors", "Zero-Shot Learning", "Few-Shot Image Classification", "Generalized Few-Shot Learning", "Generalized Zero-Shot Learning", "Long-tail learning with class descriptors"], "id": "b8654923-5e6a-4563-b208-03ea7d107234", "image_url": "https://production-media.paperswithcode.com/datasets/AwA-0000003605-fc7c16cd.jpg"}, {"title": "BABEL Project", "short_description": "**BABEL** is a multilingual corpus of conversational telephone speech from IARPA, which includes Asian and African languages.", "price": 136, "created_at": "2025-01-09 13:44:40.863831", "keyword": [], "id": "60eb3184-8e20-4d87-a726-699e01174bfe", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-17_at_10.33.19_AM.png"}, {"title": "BC2GM", "short_description": "Created by Smith et al. at 2008, the BioCreative II Gene Mention Recognition (BC2GM) Dataset contains data where participants are asked to identify a gene mention in a sentence by giving its start and end characters. The training set consists of a set of sentences, and for each sentence a set of gene mentions (GENE annotations). [registration required for access], in English language. Containing 20 in n/a file format.", "price": 763, "created_at": "2025-01-09 13:44:40.863931", "keyword": ["Named Entity Recognition (NER)", "Named Entity Recognition", "Medical Named Entity Recognition"], "id": "ee2ec048-6160-401f-9fbe-9a79ced110d9", "image_url": ""}, {"title": "BC4CHEMD", "short_description": "Introduced by Krallinger et al. in [The CHEMDNER corpus of chemicals and drugs and its annotation principles](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-7-S1-S2)\r\n\r\n**BC4CHEMD** is a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators.", "price": 47, "created_at": "2025-01-09 13:44:40.864029", "keyword": ["Token Classification", "Named Entity Recognition (NER)", "Named Entity Recognition", "NER"], "id": "4b0546d6-19b9-4c53-ad3e-08732e042128", "image_url": ""}, {"title": "BC5CDR", "short_description": "**BC5CDR** corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.\r\n\r\nSource: [https://www.ncbi.nlm.nih.gov/research/bionlp/Data/](https://www.ncbi.nlm.nih.gov/research/bionlp/Data/)\r\nImage Source: [https://arxiv.org/pdf/1805.10586.pdf](https://arxiv.org/pdf/1805.10586.pdf)", "price": 175, "created_at": "2025-01-09 13:44:40.864127", "keyword": ["Named Entity Recognition (NER)", "Named Entity Recognition", "Weakly-Supervised Named Entity Recognition", "Named Entity Recognition (NER)", "Named Entity Recognition", "Weakly-Supervised Named Entity Recognition"], "id": "464e4ed3-4442-4823-999a-97b353cbc011", "image_url": "https://production-media.paperswithcode.com/datasets/BC5CDR-0000000820-6fe6bb7f_myPvzEQ.jpg"}, {"title": "BIOSSES", "short_description": "The BIOSSES data set comprises total 100 sentence pairs all of which were selected from the \"[TAC2 Biomedical Summarization Track Training Data Set](https://tac.nist.gov/2014/BiomedSumm/)\" .\r\n\r\nThe sentence pairs were evaluated by five different human experts that judged their similarity and gave scores in a range [0-4]. Our guideline was prepared based on SemEval 2012 Task 6 Guideline.\r\n\r\nImage source: [BIOSSES](https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html)", "price": 644, "created_at": "2025-01-09 13:44:40.864225", "keyword": ["Semantic Similarity", "Sentence Embeddings For Biomedical Texts", "Sentence Similarity"], "id": "c2c4094d-cca1-4421-a863-f1aac0aaf83f", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-15_at_10.19.55.png"}, {"title": "BIWI", "short_description": "The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. Ground truth is provided in the form of the 3D location of the head and its rotation.\r\n\r\nSource: [https://www.kaggle.com/kmader/biwi-kinect-head-pose-database](https://www.kaggle.com/kmader/biwi-kinect-head-pose-database)\r\nImage Source: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)", "price": 917, "created_at": "2025-01-09 13:44:40.864322", "keyword": ["Head Pose Estimation"], "id": "d9b25764-1160-4aee-b5d4-d423926ce247", "image_url": "https://production-media.paperswithcode.com/datasets/BIWI-0000000394-8bb434ad_wQRte7C.jpg"}, {"title": "BP4D", "short_description": "The **BP4D**-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.\r\nThe database includes forty-one participants (23 women, 18 men). They were 18 \u2013 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.\r\nThe database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).\r\n\r\nSource: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)\r\nImage Source: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)", "price": 95, "created_at": "2025-01-09 13:44:40.864420", "keyword": ["Facial Expression Recognition (FER)", "Facial Action Unit Detection", "Action Unit Detection"], "id": "26bb712d-5e91-4add-bfab-8e16f4663d7b", "image_url": "https://production-media.paperswithcode.com/datasets/BP4D-0000000105-5e449356_C9DEbHH.jpg"}, {"title": "BSD", "short_description": "**BSD** is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.\r\n\r\nSource: [A Deep Journey into Super-resolution: A Survey](https://arxiv.org/abs/1904.07523)\r\nImage Source: [https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015](https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015)", "price": 384, "created_at": "2025-01-09 13:44:40.864517", "keyword": ["Image Super-Resolution", "Color Image Denoising", "Grayscale Image Denoising", "Image Denoising", "Blind Super-Resolution", "Density Estimation", "Salt-And-Pepper Noise Removal", "Compressive Sensing"], "id": "a9319633-7d74-42fa-a2c2-8f14f3817026", "image_url": "https://production-media.paperswithcode.com/datasets/BSD-0000003596-d2b867f5.jpg"}, {"title": "BSDS500", "short_description": "Berkeley Segmentation Data Set 500 (**BSDS500**) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.\r\n\r\nSource: [Object Contour Detection with a Fully Convolutional Encoder-Decoder Network](https://arxiv.org/abs/1603.04530)\r\nImage Source: [https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)", "price": 37, "created_at": "2025-01-09 13:44:40.864616", "keyword": ["JPEG Artifact Correction", "Image Compression", "Edge Detection", "JPEG Artifact Correction", "Image Compression", "Edge Detection"], "id": "fa6a0d68-54ff-4dd2-b102-a757c52e5f49", "image_url": "https://production-media.paperswithcode.com/datasets/BSDS500-0000000052-8b3207d9_akJWK1W.jpg"}, {"title": "BUCC", "short_description": "The **BUCC** mining task is a shared task on parallel sentence extraction from two monolingual corpora with a subset of them assumed to be parallel, and that has been available since 2016. For each language pair, the shared task provides a monolingual corpus for each language and a gold mapping list containing true translation pairs. These pairs are the ground truth. The task is to construct a list of translation pairs from the monolingual corpora. The constructed list is compared to the ground truth, and evaluated in terms of the F1 measure.\r\n\r\nSource: [Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)\r\nImage Source: [https://comparable.limsi.fr/bucc2017/](https://comparable.limsi.fr/bucc2017/)", "price": 922, "created_at": "2025-01-09 13:44:40.864714", "keyword": ["Cross-Lingual Bitext Mining"], "id": "8169c150-6375-49a6-b692-d7492c2ddd03", "image_url": "https://production-media.paperswithcode.com/datasets/BUCC-0000003611-553dbeec.jpg"}, {"title": "BUFF", "short_description": "**BUFF** consists of 5 subjects, 3 male and 2 female wearing 2 clothing styles: a) t-shirt and long pants and b) a soccer outfit.\r\nThey perform 3 different motions i) hips ii) tilt_twist_left iii) shoulders_mill.\r\n\r\nSource: [http://buff.is.tue.mpg.de/](http://buff.is.tue.mpg.de/)\r\nImage Source: [http://buff.is.tue.mpg.de/](http://buff.is.tue.mpg.de/)", "price": 716, "created_at": "2025-01-09 13:44:40.864812", "keyword": ["3D Object Reconstruction From A Single Image"], "id": "d6690335-e703-40f6-875b-8559afb8190c", "image_url": "https://production-media.paperswithcode.com/datasets/BUFF-0000002482-ae8f4039_lNetOTX.jpg"}, {"title": "BeerAdvocate", "short_description": "BeerAdvocate is a dataset that consists of beer reviews from beeradvocate. The data span a period of more than 10 years, including all ~1.5 million reviews up to November 2011. Each review includes ratings in terms of five \"aspects\": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review.\r\n\r\nSource: [https://snap.stanford.edu/data/web-BeerAdvocate.html](https://snap.stanford.edu/data/web-BeerAdvocate.html)", "price": 576, "created_at": "2025-01-09 13:44:40.864909", "keyword": ["Text Classification", "Language Modelling", "Recommendation Systems"], "id": "050949ad-3597-4bc9-b11a-044eeb214d86", "image_url": ""}, {"title": "BigEarthNet", "short_description": "BigEarthNet consists of 590,326 Sentinel-2 image patches, each of which is a section of i) 120x120 pixels for 10m bands; ii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. \r\n\r\nSource: [BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding](/paper/bigearthnet-a-large-scale-benchmark-archive)", "price": 640, "created_at": "2025-01-09 13:44:40.865008", "keyword": ["Image Classification", "Semantic Segmentation", "Multi-Label Image Classification", "Scene Classification"], "id": "5e45581a-e57a-48d4-af97-3ac61a3f849c", "image_url": ""}, {"title": "BigHand2.2M Benchmark", "short_description": "A large-scale hand pose dataset, collected using a novel capture method.\r\n\r\nSource: [BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis](/paper/bighand22m-benchmark-hand-pose-dataset-and)", "price": 32, "created_at": "2025-01-09 13:44:40.865111", "keyword": ["Pose Estimation", "Hand Pose Estimation", "3D Hand Pose Estimation"], "id": "7a83d094-7ef3-4e12-8fe4-51f070f7283e", "image_url": ""}, {"title": "Billion Word Benchmark", "short_description": "The **One Billion Word** dataset is a dataset for language modeling. The training/held-out data was produced from the WMT 2011 News Crawl data using a combination of Bash shell and Perl scripts.", "price": 754, "created_at": "2025-01-09 13:44:40.865210", "keyword": ["Text Generation", "Language Modelling", "Word Embeddings"], "id": "c57ab734-38a6-497a-b7c0-70522201208c", "image_url": ""}, {"title": "BioASQ", "short_description": "**BioASQ** is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets).\r\n\r\nSource: [Transferability of Natural Language Inference to Biomedical Question Answering](https://arxiv.org/abs/2007.00217)\r\nImage Source: [http://participants-area.bioasq.org/datasets/](http://participants-area.bioasq.org/datasets/)", "price": 414, "created_at": "2025-01-09 13:44:40.865307", "keyword": ["Question Answering", "Zero-shot Text Search", "Information Retrieval", "Word Embeddings"], "id": "a1d4c36b-d9c9-4bbe-860f-bc3a486e3e25", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.41.15_PM.png"}, {"title": "BioGRID", "short_description": "**BioGRID** is a biomedical interaction repository with data compiled through comprehensive curation efforts. The current index is version 4.2.192 and searches 75,868 publications for 1,997,840 protein and genetic interactions, 29,093 chemical interactions and 959,750 post translational modifications from major model organism species.\r\n\r\nSource: [https://thebiogrid.org/](https://thebiogrid.org/)", "price": 42, "created_at": "2025-01-09 13:44:40.865404", "keyword": ["Node Classification", "Link Prediction", "Graph Embedding", "Gene Interaction Prediction"], "id": "9a420991-9b8e-49c8-bb9c-4082a989cc94", "image_url": ""}, {"title": "Birdsnap", "short_description": "**Birdsnap** is a large bird dataset consisting of 49,829 images from 500 bird species with 47,386 images used for training and 2,443 images used for testing.\r\n\r\nSource: [Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks](https://arxiv.org/abs/1511.09209)\r\nImage Source: [http://thomasberg.org/](http://thomasberg.org/)", "price": 371, "created_at": "2025-01-09 13:44:40.865501", "keyword": ["Fine-Grained Image Classification"], "id": "8ee52377-5b83-46e1-a7ab-ad5c0f8c19c7", "image_url": "https://production-media.paperswithcode.com/datasets/Birdsnap-0000001091-80c2f87a_LXHQL1g.jpg"}, {"title": "BookCorpus", "short_description": "**BookCorpus** is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).\r\n\r\nSource: [Temporal Event Knowledge Acquisition via Identifying Narratives](https://arxiv.org/abs/1805.10956)", "price": 479, "created_at": "2025-01-09 13:44:40.865598", "keyword": ["Text Generation", "Language Modelling", "Word Embeddings"], "id": "2d2fdbcc-5942-41f9-b1c4-28ba117aef74", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.15.20_PM.png"}, {"title": "BraTS 2013", "short_description": "BRATS 2013 is a brain tumor segmentation dataset consists of synthetic and real images, where each of them is further divided into high-grade gliomas (HG) and low-grade gliomas (LG). There are 25 patients with both synthetic HG and LG images and 20 patients with real HG and 10 patients with real LG images. For each patient, FLAIR, T1, T2, and post-Gadolinium T1 magnetic resonance (MR) image sequences are available.\r\n\r\nSource: [Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization](https://arxiv.org/abs/1908.06965)\r\nImage Source: [https://arxiv.org/pdf/1708.00377.pdf](https://arxiv.org/pdf/1708.00377.pdf)", "price": 694, "created_at": "2025-01-09 13:44:40.865695", "keyword": ["Brain Tumor Segmentation"], "id": "098e05d1-db8e-484b-939e-ef7c6e60acee", "image_url": "https://production-media.paperswithcode.com/datasets/BraTS_2013-0000003746-02eb5a9a.jpg"}, {"title": "BraTS 2015", "short_description": "The **BraTS 2015** dataset is a dataset for brain tumor image segmentation. It consists of 220 high grade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs. The four MRI modalities are T1, T1c, T2, and T2FLAIR. Segmented \u201cground truth\u201d is provide about four intra-tumoral classes, viz. edema, enhancing tumor, non-enhancing tumor, and necrosis.\r\n\r\nSource: [Brain MRI Tumor Segmentation with Adversarial Networks](https://arxiv.org/abs/1910.02717)\r\nImage Source: [https://sites.google.com/site/braintumorsegmentation/home/brats2015](https://sites.google.com/site/braintumorsegmentation/home/brats2015)", "price": 118, "created_at": "2025-01-09 13:44:40.865800", "keyword": ["Semantic Segmentation", "Brain Tumor Segmentation", "Tumor Segmentation"], "id": "f5665a62-2ca5-43d9-bbbc-561a1b011dbe", "image_url": "https://production-media.paperswithcode.com/datasets/BraTS_2015-0000003743-37efd99b.jpg"}, {"title": "Breakfast", "short_description": "The **Breakfast** Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \u201cin the wild\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\r\n\r\nSource: [https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)\r\nImage Source: [https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)", "price": 217, "created_at": "2025-01-09 13:44:40.865899", "keyword": ["Action Segmentation", "Weakly Supervised Action Segmentation (Transcript)", "Weakly Supervised Action Segmentation (Action Set))", "Long-video Activity Recognition"], "id": "1aa366f1-a4f0-4db5-b8a0-29e88452c3f6", "image_url": "https://production-media.paperswithcode.com/datasets/Breakfast-0000001514-21782285_nDbGL3Q.jpg"}, {"title": "CACD", "short_description": "The **Cross-Age Celebrity Dataset** (**CACD**) contains 163,446 images from 2,000 celebrities collected from the Internet. The images are collected from search engines using celebrity name and year (2004-2013) as keywords. Therefore, it is possible to estimate the ages of the celebrities on the images by simply subtract the birth year from the year of which the photo was taken.\r\n\r\nSource: [https://bcsiriuschen.github.io/CARC/](https://bcsiriuschen.github.io/CARC/)\r\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)", "price": 277, "created_at": "2025-01-09 13:44:40.865997", "keyword": ["Age Estimation", "Age Estimation"], "id": "16507f94-7d70-4f6c-8f6b-9ea8517ced41", "image_url": "https://production-media.paperswithcode.com/datasets/CACD-0000000712-60c23d14_LUQ9MCB.jpg"}, {"title": "CAD-120", "short_description": "The CAD-60 and **CAD-120** data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor. Being able to detect human activities is important for making personal assistant robots useful in performing assistive tasks. The CAD dataset comprises twelve different activities (composed of several sub-activities) performed by four people in different environments, such as a kitchen, a living room, and office, etc.\r\n\r\nSource: [https://www.re3data.org/repository/r3d100012216](https://www.re3data.org/repository/r3d100012216)\r\nImage Source: [https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041](https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041)", "price": 841, "created_at": "2025-01-09 13:44:40.866095", "keyword": ["Skeleton Based Action Recognition"], "id": "30245409-051c-49f0-b185-0fc05f9be7aa", "image_url": "https://production-media.paperswithcode.com/datasets/CAD-120-0000001203-a4a6c983_zA7AT9k.jpg"}, {"title": "CADP", "short_description": "A novel dataset for traffic accidents analysis. \r\n\r\nSource: [CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis](/paper/cadp-a-novel-dataset-for-cctv-traffic-camera)", "price": 728, "created_at": "2025-01-09 13:44:40.866195", "keyword": ["Object Detection", "Pedestrian Detection"], "id": "064f644e-b6b2-4353-97f5-1c7ecaf58ca5", "image_url": ""}, {"title": "CAL500", "short_description": "**CAL500** (**Computer Audition Lab 500**) is a dataset aimed for evaluation of music information retrieval systems. It consists of 502 songs picked from western popular music. The audio is represented as a time series of the first 13 Mel-frequency cepstral coefficients (and their first and second derivatives) extracted by sliding a 12 ms half-overlapping short-time window over the waveform of each song. Each song has been annotated by at least 3 people with 135 musically-relevant concepts spanning six semantic categories:\r\n\r\n* 29 instruments were annotated as present in the song or not,\r\n* 22 vocal characteristics were annotated as relevant to the singer or not,\r\n* 36 genres,\r\n* 18 emotions were rated on a scale from one to three (e.g., ``not happy\", ``neutral\", ``happy\"),\r\n* 15 song concepts describing the acoustic qualities of the song, artist and recording (e.g., tempo, energy, sound quality),\r\n* 15 usage terms (e.g., \"I would listen to this song while driving, sleeping, etc.\").\r\n\r\nSource: [http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt](http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt)\r\nAudio Source: [http://calab1.ucsd.edu/~datasets/cal500/cal500data/](http://calab1.ucsd.edu/~datasets/cal500/cal500data/)", "price": 282, "created_at": "2025-01-09 13:44:40.866294", "keyword": ["Multi-Label Classification", "Multi-Task Learning", "Matrix Completion"], "id": "a7d8e61c-e0bf-46cb-ba2b-0cac5cf8b962", "image_url": ""}, {"title": "CARLA", "short_description": "**CARLA** (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).\r\n\r\nSource: [Synthetic Data for Deep Learning](https://arxiv.org/abs/1909.11512)", "price": 247, "created_at": "2025-01-09 13:44:40.866396", "keyword": ["Autonomous Vehicles", "Autonomous Driving", "CARLA MAP Leaderboard", "CARLA longest6", "Imitation Learning"], "id": "4288e2b5-1f5b-4920-b444-da07ed02e37c", "image_url": "https://production-media.paperswithcode.com/datasets/carla.gif"}, {"title": "CARPK", "short_description": "The Car Parking Lot Dataset (**CARPK**) contains nearly 90,000 cars from 4 different parking lots collected by means of drone (PHANTOM 3 PROFESSIONAL). The images are collected with the drone-view at approximate 40 meters height. The image set is annotated by bounding box per car. All labeled bounding boxes have been well recorded with the top-left points and the bottom-right points. It is supporting object counting, object localizing, and further investigations with the annotation format in bounding boxes.\r\n\r\nSource: [https://lafi.github.io/LPN/](https://lafi.github.io/LPN/)\r\nImage Source: [https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610](https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610)", "price": 128, "created_at": "2025-01-09 13:44:40.866495", "keyword": ["Object Counting"], "id": "b4ebc981-c432-4290-b0eb-5a362522d44b", "image_url": "https://production-media.paperswithcode.com/datasets/CARPK-0000001963-38e88bb1_1jm7Git.jpg"}, {"title": "CASIA-HWDB", "short_description": "**CASIA-HWDB** is a dataset for handwritten Chinese character recognition. It contains 300 files (240 in HWDB1.1 training set and 60 in HWDB1.1 test set). Each file contains about 3000 isolated gray-scale Chinese character images written by one writer, as well as their corresponding labels.\r\n\r\nSource: [Generating Handwritten Chinese Characters using CycleGAN](https://arxiv.org/abs/1801.08624)\r\nImage Source: [http://www.nlpr.ia.ac.cn/databases/handwriting/Touching_Characters_Databases.html](http://www.nlpr.ia.ac.cn/databases/handwriting/Touching_Characters_Databases.html)", "price": 807, "created_at": "2025-01-09 13:44:40.866593", "keyword": ["Language Modelling", "Handwritten Chinese Text Recognition", "Offline Handwritten Chinese Character Recognition"], "id": "a9e575f8-265c-40ce-92f1-9eda4298c36c", "image_url": "https://production-media.paperswithcode.com/datasets/CASIA-HWDB-0000003630-64017e07.jpeg"}, {"title": "CASIA-WebFace", "short_description": "The **CASIA-WebFace** dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web.\r\n\r\nSource: [On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs](https://arxiv.org/abs/1811.07104)", "price": 655, "created_at": "2025-01-09 13:44:40.866691", "keyword": ["Image Super-Resolution", "Face Recognition", "Face Verification", "Metric Learning", "Facial Inpainting", "Image Super-Resolution", "Face Recognition", "Face Verification", "Metric Learning", "Facial Inpainting"], "id": "b02fd2bd-e449-4ca7-8197-be8883bc81bc", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.02.29_PM.png"}, {"title": "CAT2000", "short_description": "Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings.\r\n\r\nSource: [CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research](https://arxiv.org/pdf/1505.03581v1.pdf)", "price": 152, "created_at": "2025-01-09 13:44:40.866789", "keyword": ["Saliency Detection"], "id": "65e9fe83-daa3-49ed-b1ca-df81eb18721d", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.57.14_PM.png"}, {"title": "CATER", "short_description": "Rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning.\r\n\r\nSource: [CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning](/paper/cater-a-diagnostic-dataset-for-compositional)\r\n\r\nImage Source: [CATER](https://rohitgirdhar.github.io/CATER/)", "price": 621, "created_at": "2025-01-09 13:44:40.866886", "keyword": ["Action Recognition", "Visual Reasoning", "Self-Supervised Learning", "Video Object Tracking", "Atomic action recognition", "Composite action recognition"], "id": "636a6293-76bf-420b-94f8-29c88005e47f", "image_url": "https://production-media.paperswithcode.com/datasets/cater.gif"}, {"title": "CBSD68", "short_description": "**Color BSD68** dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images.\r\n\r\nSource: [https://github.com/clausmichele/CBSD68-dataset](https://github.com/clausmichele/CBSD68-dataset)", "price": 532, "created_at": "2025-01-09 13:44:40.866986", "keyword": ["Denoising", "Color Image Denoising", "Image Restoration", "Image Denoising", "Image Compressed Sensing"], "id": "8d599872-8080-4e8d-9163-e65332459690", "image_url": ""}, {"title": "CBT", "short_description": "Children\u2019s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg. \r\n\r\nSource: [CBT](https://research.fb.com/downloads/babi/)\r\nImage Source: [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)", "price": 447, "created_at": "2025-01-09 13:44:40.867088", "keyword": ["Question Answering", "Reading Comprehension", "Click-Through Rate Prediction", "Machine Reading Comprehension", "Question Answering", "Reading Comprehension", "Click-Through Rate Prediction", "Machine Reading Comprehension"], "id": "543de141-efeb-4c39-b831-554bb61fe70a", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_13.46.56.png"}, {"title": "CCGbank", "short_description": "**CCGbank** is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar derivations. It pairs syntactic derivations with sets of word-word dependencies which approximate the underlying predicate-argument structure.\r\nThe dataset contains 99.44% of the sentences in the Penn Treebank, for which it corrects a number of inconsistencies and errors in the original annotation.\r\n\r\nSource: [CCGbank](https://catalog.ldc.upenn.edu/LDC2005T13)", "price": 762, "created_at": "2025-01-09 13:44:40.867187", "keyword": ["CCG Supertagging"], "id": "4921e8d4-ba07-41d6-957f-d8b96c6c9db5", "image_url": ""}, {"title": "CHASE_DB1", "short_description": "**CHASE_DB1** is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999\u00d7960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts.\r\n\r\nSource: [MixModule: Mixed CNN Kernel Module for Medical Image Segmentation](https://arxiv.org/abs/1910.08728)\r\nImage Source: [https://www.mdpi.com/2073-8994/9/11/276](https://www.mdpi.com/2073-8994/9/11/276)", "price": 542, "created_at": "2025-01-09 13:44:40.867285", "keyword": ["Medical Image Segmentation", "Retinal Vessel Segmentation", "Medical Image Segmentation", "Retinal Vessel Segmentation"], "id": "e3dce087-f0cb-4431-b35c-86d984a47f8e", "image_url": "https://production-media.paperswithcode.com/datasets/CHASE_DB1-0000000647-d5cc08be_IGqEUGQ.jpg"}, {"title": "CHiME-5", "short_description": "The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning.\r\n\r\nSource: [The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines](https://arxiv.org/pdf/1803.10609v1.pdf)", "price": 326, "created_at": "2025-01-09 13:44:40.867383", "keyword": ["Speech Recognition", "Speech Enhancement", "Speaker Diarization"], "id": "22769edc-4b27-42e2-b915-118125bc04e1", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_8.52.06_PM.png"}, {"title": "CIFAR-10", "short_description": "The **CIFAR-10** dataset (Canadian Institute for Advanced Research, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck). There are 6000 images per class with 5000 training and 1000 testing images per class.\r\n\r\nThe criteria for deciding whether an image belongs to a class were as follows:\r\n\r\n* The class name should be high on the list of likely answers to the question \u201cWhat is in this picture?\u201d\r\n* The image should be photo-realistic. Labelers were instructed to reject line drawings.\r\n* The image should contain only one prominent instance of the object to which the class refers.\r\nThe object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.\r\n\r\nSource: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\nImage Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)", "price": 998, "created_at": "2025-01-09 13:44:40.867481", "keyword": ["Image Classification", "Image Generation", "Anomaly Detection", "Image Retrieval", "Graph Classification", "Semi-Supervised Image Classification", "Continual Learning", "Out-of-Distribution Detection", "Image Clustering", "Neural Architecture Search", "Long-tail Learning", "Learning with noisy labels", "Binarization", "Image Classification with Label Noise", "Density Estimation", "Adversarial Defense", "Stochastic Optimization", "Small Data Image Classification", "Image Compression", "Quantization", "Partial Label Learning", "Conditional Image Generation", "Object Recognition", "Semi-Supervised Image Classification (Cold Start)", "Unsupervised Image Classification", "Personalized Federated Learning", "Adversarial Robustness", "Self-Supervised Learning", "Network Pruning", "Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly", "Domain-IL Continual Learning", "Sequential Image Classification", "Active Learning", "Data Augmentation", "Classification with Binary Weight Network", "Classification with Binary Neural Network", "Model Poisoning", "Sparse Learning and binarization", "Novel Class Discovery", "Open-World Semi-Supervised Learning", "Adversarial Attack", "Hard-label Attack", "Provable Adversarial Defense", "Robust classification", "Clean-label Backdoor Attack (0.05%)", "Nature-Inspired Optimization Algorithm", "Neural Network Compression", "Image Classification with Human Noise", "Long-tail Learning on CIFAR-10-LT (\u03c1=100)", "Image Classification", "Image Generation", "Anomaly Detection", "Image Retrieval", "Graph Classification", "Semi-Supervised Image Classification", "Continual Learning", "Out-of-Distribution Detection", "Image Clustering", "Neural Architecture Search", "Long-tail Learning", "Learning with noisy labels", "Binarization", "Image Classification with Label Noise", "Density Estimation", "Adversarial Defense", "Stochastic Optimization", "Small Data Image Classification", "Image Compression", "Quantization", "Partial Label Learning", "Conditional Image Generation", "Object Recognition", "Semi-Supervised Image Classification (Cold Start)", "Unsupervised Image Classification", "Personalized Federated Learning", "Adversarial Robustness", "Self-Supervised Learning", "Network Pruning", "Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly", "Domain-IL Continual Learning", "Sequential Image Classification", "Active Learning", "Data Augmentation", "Classification with Binary Weight Network", "Classification with Binary Neural Network", "Model Poisoning", "Sparse Learning and binarization", "Novel Class Discovery", "Open-World Semi-Supervised Learning", "Adversarial Attack", "Hard-label Attack", "Provable Adversarial Defense", "Robust classification", "Clean-label Backdoor Attack (0.05%)", "Nature-Inspired Optimization Algorithm", "Neural Network Compression", "Image Classification with Human Noise", "Long-tail Learning on CIFAR-10-LT (\u03c1=100)"], "id": "1f8d8c97-8d0d-47f5-9fd1-a388472deb8a", "image_url": "https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png"}, {"title": "CIFAR-100", "short_description": "The **CIFAR-100** dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.\r\n\r\nThe criteria for deciding whether an image belongs to a class were as follows:\r\n\r\n* The class name should be high on the list of likely answers to the question \u201cWhat is in this picture?\u201d\r\n* The image should be photo-realistic. Labelers were instructed to reject line drawings.\r\n* The image should contain only one prominent instance of the object to which the class refers.\r\n* The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.\r\n\r\nSource: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\r\nImage Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)", "price": 372, "created_at": "2025-01-09 13:44:40.867578", "keyword": ["Image Classification", "Image Generation", "Anomaly Detection", "Few-Shot Image Classification", "Semi-Supervised Image Classification", "Continual Learning", "Out-of-Distribution Detection", "Image Clustering", "Neural Architecture Search", "Long-tail Learning", "Incremental Learning", "Class Incremental Learning", "Learning with noisy labels", "Binarization", "Image Classification with Label Noise", "Adversarial Defense", "Stochastic Optimization", "Small Data Image Classification", "Conditional Image Generation", "Personalized Federated Learning", "Adversarial Robustness", "Self-Supervised Learning", "Network Pruning", "Knowledge Distillation", "Learning with coarse labels", "Classification with Binary Weight Network", "Classification with Binary Neural Network", "Sparse Learning and binarization", "Novel Class Discovery", "Open-World Semi-Supervised Learning", "Few-Shot Class-Incremental Learning", "Non-exemplar-based Class Incremental Learning", "Provable Adversarial Defense", "Data Free Quantization", "Bayesian Inference", "Variational Inference", "Classifier calibration"], "id": "105e13cd-518e-43fb-81a2-54978df88e24", "image_url": "https://production-media.paperswithcode.com/datasets/CIFAR-100-0000000433-b71f61c0_hPEzMRg.jpg"}, {"title": "CIFAR-FS", "short_description": "**CIFAR100 few-shots** (**CIFAR-FS**) is randomly sampled from CIFAR-100 (Krizhevsky & Hinton, 2009) by using the same criteria with which miniImageNet has been generated. The average inter-class similarity is sufficiently high to represent a challenge for the current state of the art. Moreover, the limited original resolution of 32\u00d732 makes the task harder and at the same time allows fast prototyping.\r\n\r\nSource: [Bertinetto et al.](https://arxiv.org/pdf/1805.08136.pdf)\r\nImage source: [Bertinetto et al.](https://www.robots.ox.ac.uk/~luca/r2d2.html)", "price": 730, "created_at": "2025-01-09 13:44:40.867680", "keyword": ["Few-Shot Image Classification"], "id": "05f0881d-2c83-403e-84c1-e618080f065e", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-11_at_09.55.24.png"}, {"title": "CIHP", "short_description": "The **Crowd Instance-level Human Parsing** (**CIHP**) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations on 20 categories and instance-level identification. The dataset can be used for the human part segmentation task.\r\n\r\nSource: [Parsing R-CNN for Instance-Level Human Analysis](https://arxiv.org/abs/1811.12596)\r\nImage Source: [https://arxiv.org/abs/1808.00157](https://arxiv.org/abs/1808.00157)", "price": 233, "created_at": "2025-01-09 13:44:40.867780", "keyword": ["Human Part Segmentation"], "id": "06a9dd31-837d-4076-8f88-4e6bf7f07867", "image_url": "https://production-media.paperswithcode.com/datasets/CIHP-0000000480-f3e9c3af_2TtgSkf.jpg"}, {"title": "CINIC-10", "short_description": "**CINIC-10** is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images.\r\n\r\nSource: [Group Knowledge Transfer:Collaborative Training of Large CNNs on the Edge](https://arxiv.org/abs/2007.14513)\r\nImage Source: [https://arxiv.org/abs/1810.03505](https://arxiv.org/abs/1810.03505)", "price": 200, "created_at": "2025-01-09 13:44:40.867879", "keyword": ["Image Classification", "Neural Architecture Search", "Sparse Learning"], "id": "54121c2d-de81-4ac8-ae75-612649569167", "image_url": "https://production-media.paperswithcode.com/datasets/CINIC-10-0000000044-9182f4b1_otT5BUh.jpg"}, {"title": "CK+", "short_description": "The Extended Cohn-Kanade (**CK+**) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods.\r\n\r\nSource: [EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-time Facial Expression Recognition](https://arxiv.org/abs/2006.15759)", "price": 201, "created_at": "2025-01-09 13:44:40.867977", "keyword": ["Facial Expression Recognition (FER)", "Face Verification", "Facial Expression Recognition (FER)", "Face Verification"], "id": "75f1f27c-7ccb-4e91-a172-62a20bb7787a", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_13.44.53.png"}, {"title": "CLEVR", "short_description": "**CLEVR** (**Compositional Language and Elementary Visual Reasoning**) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, a test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.\r\n\r\nSource: [On transfer learning using a MAC model variant](https://arxiv.org/abs/1811.06529)\r\nImage Source: [Johnson et al](https://arxiv.org/pdf/1612.06890v1.pdf)", "price": 783, "created_at": "2025-01-09 13:44:40.868074", "keyword": ["Image Generation", "Visual Question Answering (VQA)"], "id": "fbb4d94b-df80-40fc-bab5-c9f972b8b61f", "image_url": "https://production-media.paperswithcode.com/datasets/clevr.jpg"}, {"title": "CLINC150", "short_description": "This dataset is for evaluating the performance of intent classification systems in the presence of \"out-of-scope\" queries, i.e., queries that do not fall into any of the system-supported intent classes. The dataset includes both in-scope and out-of-scope data.\r\n\r\nSource: [CLINC150](https://github.com/clinc/oos-eval)", "price": 905, "created_at": "2025-01-09 13:44:40.868171", "keyword": ["Text Classification", "Intent Detection", "Open Intent Discovery", "Intent Classification"], "id": "b6175f8d-c891-413e-bb80-805568139bdd", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_20.02.26.png"}, {"title": "CNN/Daily Mail", "short_description": "**CNN/Daily Mail** is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites.\r\n\r\nIn all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences. \r\n\r\nSource: [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](https://arxiv.org/pdf/1602.06023v5.pdf)", "price": 869, "created_at": "2025-01-09 13:44:40.868268", "keyword": ["Text Generation", "Question Answering", "Text Summarization", "Sequence-to-sequence Language Modeling", "Summarization", "Abstractive Text Summarization", "Document Summarization", "Extractive Text Summarization", "Extractive Document Summarization"], "id": "a455ca40-9e17-463e-8d23-f4c87e7e2044", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-07_at_17.21.54.png"}, {"title": "COCO Captions", "short_description": "COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image.\r\n\r\nSource: [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/abs/1504.00325)", "price": 737, "created_at": "2025-01-09 13:44:40.868364", "keyword": ["Text Generation", "Image Captioning", "Concept-To-Text Generation"], "id": "6096f69f-8c79-4d00-96b3-88c2f0b99750", "image_url": "https://production-media.paperswithcode.com/datasets/cococ.jpg"}, {"title": "COCO-QA", "short_description": "**COCO-QA** is a dataset for visual question answering. It consists of:\r\n\r\n- 123287 images\r\n- 78736 train questions\r\n- 38948 test questions\r\n- 4 types of questions: object, number, color, location\r\n- Answers are all one-word.\r\n\r\nSource: [Exploring Models and Data for Image Question Answering](https://arxiv.org/pdf/1505.02074v4.pdf)", "price": 368, "created_at": "2025-01-09 13:44:40.868461", "keyword": ["Question Answering", "Visual Question Answering (VQA)"], "id": "8529292c-415d-4ebb-b1f8-3d91d94c0542", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_11.28.16.png"}, {"title": "COCO-Stuff", "short_description": "The **Common Objects in COntext-stuff** (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.\r\n\r\nSource: [Image Colorization: A Survey and Dataset](https://arxiv.org/abs/2008.10774)\r\nImage Source: [https://github.com/nightrome/cocostuff](https://github.com/nightrome/cocostuff)", "price": 147, "created_at": "2025-01-09 13:44:40.868563", "keyword": ["Semantic Segmentation", "Image-to-Image Translation", "Unsupervised Semantic Segmentation", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Open Vocabulary Semantic Segmentation", "Real-Time Semantic Segmentation", "Layout-to-Image Generation", "Zero-Shot Semantic Segmentation", "Sketch-to-Image Translation", "Unsupervised Image Segmentation"], "id": "ccd7018f-4b36-40a9-a66c-8ea39d9306b6", "image_url": "https://production-media.paperswithcode.com/datasets/COCO-Stuff-0000000088-b034a949_hldOiur.jpg"}, {"title": "COFW", "short_description": "The **Caltech Occluded Faces in the Wild** (**COFW**) dataset is designed to present faces in real-world conditions. Faces show large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones,\u2028etc.). All images were hand annotated using the same 29 landmarks as in LFPW. Both the landmark positions as well as their occluded/unoccluded state were annotated. The faces are occluded to different degrees, with large variations in the type of occlusions encountered. COFW has an average occlusion of over 23.\r\n\r\nSource: [http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset](http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset)\r\nImage Source: [http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset](http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset)", "price": 546, "created_at": "2025-01-09 13:44:40.868662", "keyword": ["Face Alignment", "Head Pose Estimation", "Face Alignment", "Head Pose Estimation"], "id": "58f315b5-6b8f-41c1-9353-5958c5ee5887", "image_url": "https://production-media.paperswithcode.com/datasets/COFW-0000000896-dc8b53c0_q5aTcBX.jpg"}, {"title": "COIN", "short_description": "The **COIN** dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\r\n\r\nSource: [COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis](/paper/coin-a-large-scale-dataset-for-comprehensive)", "price": 935, "created_at": "2025-01-09 13:44:40.868762", "keyword": ["Action Recognition", "Action Localization", "Temporal Action Localization", "Action Segmentation"], "id": "3e676dda-bb42-4308-ac1e-918a1cbe83d6", "image_url": ""}, {"title": "COLLAB", "short_description": "**COLLAB** is a scientific collaboration dataset. A graph corresponds to a researcher\u2019s ego network, i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. A researcher\u2019s ego network has three possible labels, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics, which are the fields that the researcher belongs to. The dataset has 5,000 graphs and each graph has label 0, 1, or 2.\r\n\r\nSource: [1 Introduction](https://arxiv.org/abs/2006.11165)", "price": 893, "created_at": "2025-01-09 13:44:40.868862", "keyword": ["Link Prediction", "Graph Classification", "Link Prediction", "Graph Classification"], "id": "a40bde6c-1e4f-4835-a91f-6533671d9e51", "image_url": ""}, {"title": "COMA", "short_description": "CoMA contains 17,794 meshes of the human face in various expressions\r\n\r\nSource: [DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects](https://arxiv.org/abs/1905.10290)\r\nImage Source: [https://coma.is.tue.mpg.de/](https://coma.is.tue.mpg.de/)", "price": 976, "created_at": "2025-01-09 13:44:40.868964", "keyword": ["Graph Representation Learning"], "id": "d1e741a2-11f7-45d1-aa7f-00a8202e9d26", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-06-02_at_09.40.45.png"}, {"title": "CONVERSE", "short_description": "A novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors.\r\n\r\nSource: [From Pose to Activity: Surveying Datasets and Introducing CONVERSE](/paper/from-pose-to-activity-surveying-datasets-and)", "price": 738, "created_at": "2025-01-09 13:44:40.869061", "keyword": ["Action Recognition", "Temporal Action Localization", "Video Understanding"], "id": "c01caf96-d304-4aca-965d-a604e228ff4c", "image_url": ""}, {"title": "CORe50", "short_description": "CORe50 is a dataset designed for assessing Continual Learning techniques in an Object Recognition context.\r\n\r\nSource: [CORe50: a New Dataset and Benchmark for Continuous Object Recognition](/paper/core50-a-new-dataset-and-benchmark-for)", "price": 156, "created_at": "2025-01-09 13:44:40.869158", "keyword": ["Continual Learning", "Incremental Learning", "Object Recognition"], "id": "2bbc5c11-47be-4242-a4b5-f6ec6052d6be", "image_url": "https://production-media.paperswithcode.com/datasets/classes.gif"}, {"title": "COWC", "short_description": "The Cars Overhead With Context (COWC) data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars.\r\n\r\nSource: [A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning](/paper/a-large-contextual-dataset-for-classification)\r\nImage Source: [https://gdo152.llnl.gov/cowc/](https://gdo152.llnl.gov/cowc/)", "price": 788, "created_at": "2025-01-09 13:44:40.869255", "keyword": ["Object Detection", "Density Estimation", "Object Counting"], "id": "a0d743a5-c0a4-47b7-a702-16cab51ba324", "image_url": "https://production-media.paperswithcode.com/datasets/cow.jpg"}, {"title": "CQR", "short_description": "CQR is an extension to the Stanford Dialogue Corpus. It contains crowd-sourced rewrites to facilitate research in dialogue state tracking using natural language as the interface.\r\n\r\nSource: [A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR)](/paper/a-dataset-for-resolving-referring-expressions)", "price": 570, "created_at": "2025-01-09 13:44:40.869354", "keyword": ["Spoken Language Understanding", "Multi-Task Learning", "Spoken Dialogue Systems"], "id": "ad112ec9-83ea-4400-a8d1-7c30a691e67a", "image_url": "https://production-media.paperswithcode.com/datasets/cqr.jpg"}, {"title": "CUB-200-2011", "short_description": "The **Caltech-UCSD Birds-200-2011** (**CUB-200-2011**) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from [Reed et al.]( https://paperswithcode.com/paper/learning-deep-representations-of-fine-grained). They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.\r\n\r\nSource: [Fine-grained Visual-textual Representation Learning](https://arxiv.org/abs/1709.00340)\r\nImage Source: [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)", "price": 189, "created_at": "2025-01-09 13:44:40.869451", "keyword": ["Image Classification", "Image Generation", "Text-to-Image Generation", "Zero-Shot Learning", "Few-Shot Image Classification", "Image Retrieval", "Few-Shot Learning", "Image Clustering", "Fine-Grained Image Classification", "Generalized Few-Shot Learning", "Document Text Classification", "Multimodal Deep Learning", "Multimodal Text and Image Classification", "Generalized Zero-Shot Learning", "Small Data Image Classification", "Single-View 3D Reconstruction", "Cross-Domain Few-Shot", "Weakly-Supervised Object Localization", "Metric Learning", "Semantic correspondence", "Graph Matching", "Multi-Modal Document Classification", "Long-tail learning with class descriptors", "Fine-Grained Image Recognition", "Point-interactive Image Colorization", "Few-Shot Class-Incremental Learning", "Interpretable Machine Learning", "Unsupervised Keypoint Estimation", "Bird Species Classification With Audio-Visual Data"], "id": "14ccf439-ef58-4427-9704-c92deaa53cb5", "image_url": "https://production-media.paperswithcode.com/datasets/CUB-200-2011-0000000109-6e01ce73_vMleyYb.jpeg"}, {"title": "CUHK01", "short_description": "This dataset contains 971 identities from two disjoint camera views. Each identity has two samples per camera view. It is used for Person Re-identification.\r\n\r\nPaper: [Li W., Zhao R., Wang X. (2013) Human Reidentification with Transferred Metric Learning. In: Lee K.M., Matsushita Y., Rehg J.M., Hu Z. (eds) Computer Vision \u2013 ACCV 2012. ACCV 2012. Lecture Notes in Computer Science, vol 7724. Springer, Berlin, Heidelberg](https://doi.org/10.1007/978-3-642-37331-2_3)", "price": 630, "created_at": "2025-01-09 13:44:40.869547", "keyword": ["Person Re-Identification"], "id": "ab9871ff-a045-4523-a308-6556e8950f96", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_09.53.19.png"}, {"title": "CUHK03", "short_description": "The **CUHK03** consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training\r\n\r\nSource: [Attention Driven Person Re-identification](https://arxiv.org/abs/1810.05866)\r\n\r\nImage Source: [Person Re-Identification Techniques for Intelligent Video Surveillance Systems\r\n](https://www.researchgate.net/publication/324031366_Person_Re-Identification_Techniques_for_Intelligent_Video_Surveillance_Systems)", "price": 656, "created_at": "2025-01-09 13:44:40.869644", "keyword": ["Person Re-Identification", "Face Sketch Synthesis", "Generalizable Person Re-identification", "Defocus Blur Detection", "Defocus Estimation", "Person Re-Identification", "Face Sketch Synthesis", "Generalizable Person Re-identification", "Defocus Blur Detection", "Defocus Estimation"], "id": "3342c911-fdf3-4f4b-b58d-6069b63b8a2f", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.23.41_PM.png"}, {"title": "CULane", "short_description": "**CULane** is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. The dataset is divided into 88880 images for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories.\r\n\r\nSource: [https://xingangpan.github.io/projects/CULane.html](https://xingangpan.github.io/projects/CULane.html)\r\nImage Source: [https://xingangpan.github.io/projects/CULane.html](https://xingangpan.github.io/projects/CULane.html)", "price": 712, "created_at": "2025-01-09 13:44:40.869740", "keyword": ["Lane Detection"], "id": "0d0a5ff4-b9fa-4728-9501-9f759c23b9d0", "image_url": "https://production-media.paperswithcode.com/datasets/CULane-0000000933-1394cf9a_Ag9p6b5.jpg"}, {"title": "Caltech Pedestrian Dataset", "short_description": "The Caltech Pedestrian Dataset consists of approximately 10 hours of 640x480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total of 350,000 bounding boxes and 2300 unique pedestrians were annotated. The annotation includes temporal correspondence between bounding boxes and detailed occlusion labels.\r\n\r\nSource: [Caltech Pedestrian Dataset](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)", "price": 850, "created_at": "2025-01-09 13:44:40.869837", "keyword": ["Pedestrian Detection"], "id": "0aff81de-0e25-416b-b2e0-887a9c7cb1d4", "image_url": ""}, {"title": "Caltech-101", "short_description": "The Caltech101 dataset contains images from 101 object categories (e.g., \u201chelicopter\u201d, \u201celephant\u201d and \u201cchair\u201d etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300\u00d7200 pixels.\r\n\r\nSource: [Simple and Efficient Learning using Privileged Information](https://arxiv.org/abs/1604.01518)", "price": 542, "created_at": "2025-01-09 13:44:40.869934", "keyword": ["Semi-Supervised Image Classification", "Fine-Grained Image Classification", "Unsupervised Anomaly Detection", "Prompt Engineering", "Density Estimation", "Semantic correspondence", "Semi-Supervised Image Classification", "Fine-Grained Image Classification", "Unsupervised Anomaly Detection", "Prompt Engineering", "Density Estimation", "Semantic correspondence"], "id": "6f4acddc-d482-40d2-915e-4b4813f0e7e5", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.27.45_PM.png"}, {"title": "Caltech-256", "short_description": "**Caltech-256** is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset.\r\n\r\nSource: [Exploiting Non-Linear Redundancy for Neural Model Compression](https://arxiv.org/abs/2005.14070)\r\n\r\nImage Source: [ML4A](https://twitter.com/ml4a_/status/934796379171512322)", "price": 33, "created_at": "2025-01-09 13:44:40.870029", "keyword": ["Image Classification", "Few-Shot Image Classification", "Semi-Supervised Image Classification"], "id": "03aecda7-ace8-484d-889f-5c1785c18c75", "image_url": "https://production-media.paperswithcode.com/datasets/CALTECH.jpg"}, {"title": "CamVid", "short_description": "**CamVid** (**Cambridge-driving Labeled Video Database**) is a road/driving scene understanding database which was originally captured as five video sequences with a 960\u00d7720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object\r\n\r\nSource: [A Review on Deep Learning TechniquesApplied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)", "price": 580, "created_at": "2025-01-09 13:44:40.870127", "keyword": ["Semantic Segmentation", "Real-Time Semantic Segmentation", "Video Semantic Segmentation"], "id": "d9a840fd-5315-4587-a323-f3cc9a458a43", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.59.33_PM.png"}, {"title": "CelebA", "short_description": "CelebFaces Attributes dataset contains 202,599 face images of the size 178\u00d7218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.\r\n\r\nSource: [Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention](https://arxiv.org/abs/1811.07483)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)", "price": 268, "created_at": "2025-01-09 13:44:40.870228", "keyword": ["Image Classification", "Image Generation", "Image Super-Resolution", "Face Alignment", "Long-tail Learning", "Image Inpainting", "Multi-Task Learning", "Image Deblurring", "Blind Face Restoration", "Image Compressed Sensing", "Facial Expression Translation", "Image Colorization", "HairColor/Unbiased", "HairColor/Bias-conflicting", "HeavyMakeup/Unbiased", "HeavyMakeup/Bias-conflicting", "Physical Attribute Prediction", "Image Classification", "Image Generation", "Image Super-Resolution", "Face Alignment", "Long-tail Learning", "Image Inpainting", "Multi-Task Learning", "Image Deblurring", "Blind Face Restoration", "Image Compressed Sensing", "Facial Expression Translation", "Image Colorization", "HairColor/Unbiased", "HairColor/Bias-conflicting", "HeavyMakeup/Unbiased", "HeavyMakeup/Bias-conflicting", "Physical Attribute Prediction"], "id": "d01575b4-0939-4d6e-a6bf-4430b9d0ade0", "image_url": "https://production-media.paperswithcode.com/datasets/CelebA-0000000002-eeb5e196_D0ltvot.jpg"}, {"title": "ChaLearn Pose", "short_description": "**ChaLearn Pose** is a subset of the ChaLearn 2013 Multi-modal gesture dataset from Escalera et al. ICMI'13, which contains 23 hours of Kinect data of 27 persons performing 20 Italian gestures. The data includes RGB, depth, foreground segmentations and full body skeletons. In this dataset, both the training and testing labels are noisy (from Kinect).\n\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation)\nImage Source: [http://sunai.uoc.edu/chalearnLAP/](http://sunai.uoc.edu/chalearnLAP/)", "price": 216, "created_at": "2025-01-09 13:44:40.870325", "keyword": [], "id": "573712a2-535a-4d8a-8ea5-837a660a874e", "image_url": "https://production-media.paperswithcode.com/datasets/ChaLearn_Pose-0000003282-0e9e2511.jpg"}, {"title": "Chairs", "short_description": "The **Chairs** dataset contains rendered images of around 1000 different three-dimensional chair models.\r\n\r\nSource: [Adversarial Disentanglement with Grouped Observations](https://arxiv.org/abs/2001.04761)\r\nImage Source: [https://www.di.ens.fr/willow/research/seeing3Dchairs/](https://www.di.ens.fr/willow/research/seeing3Dchairs/)", "price": 961, "created_at": "2025-01-09 13:44:40.870422", "keyword": ["Sketch-Based Image Retrieval", "Sketch-Based Image Retrieval"], "id": "a77fce49-567d-49f6-8436-24cfe23b88e4", "image_url": "https://production-media.paperswithcode.com/datasets/Chairs-0000002254-16a7565c_fUVACbY.jpg"}, {"title": "Charades", "short_description": "The **Charades** dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\r\n\r\nSource: [Temporal Reasoning Graph for Activity Recognition](https://arxiv.org/abs/1908.09995)", "price": 674, "created_at": "2025-01-09 13:44:40.870520", "keyword": ["Action Recognition", "Temporal Action Localization", "Action Detection", "Action Classification", "Weakly Supervised Object Detection", "Video Understanding", "Video Classification", "Zero-Shot Action Recognition"], "id": "c2cb358f-8cf5-4e0b-955f-812fc785a32d", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.28.10_PM.png"}, {"title": "CheXpert", "short_description": "The **CheXpert** dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets.\r\n\r\nSource: [Deep Mining External Imperfect Data for Chest X-ray Disease Screening](https://arxiv.org/abs/2006.03796)\r\nImage Source: [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)", "price": 338, "created_at": "2025-01-09 13:44:40.870616", "keyword": ["Multi-Label Classification"], "id": "c3bb2e6f-424e-412c-85a9-5cfc019f3649", "image_url": "https://production-media.paperswithcode.com/datasets/CheXpert-0000002920-2f0a0950_fCtVCsz.jpg"}, {"title": "Cholec80", "short_description": "Cholec80 is an endoscopic video dataset containing 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps and downsampled to 1 fps for processing. The whole dataset is labeled with the phase and tool presence annotations. The phases have been defined by a senior surgeon in Strasbourg hospital, France. Since the tools are sometimes hardly visible in the images and thus difficult to be recognized visually, a tool is defined as present in an image if at least half of the tool tip is visible.\r\n\r\nSource: [EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos](/paper/endonet-a-deep-architecture-for-recognition)[https://arxiv.org/pdf/1602.03012.pdf]", "price": 62, "created_at": "2025-01-09 13:44:40.870713", "keyword": ["Surgical tool detection", "Surgical phase recognition", "Online surgical phase recognition", "Offline surgical phase recognition"], "id": "32e4cf23-40f7-40dd-ae98-cdd55f2c08e2", "image_url": "https://production-media.paperswithcode.com/datasets/cholec80.png"}, {"title": "Ciao", "short_description": "The **Ciao** dataset contains rating information of users given to items, and also contain item category information. The data comes from the Epinions dataset.\r\n\r\nSource: [Collaborative Translational Metric Learning](https://arxiv.org/abs/1906.01637)", "price": 565, "created_at": "2025-01-09 13:44:40.870810", "keyword": ["Recommendation Systems"], "id": "c3afdfca-bada-4d60-a54c-ff70f1cb72b6", "image_url": ""}, {"title": "Citeseer", "short_description": "The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.\r\n\r\nSource: [https://linqs.soe.ucsc.edu/data](https://linqs.soe.ucsc.edu/data)", "price": 67, "created_at": "2025-01-09 13:44:40.870908", "keyword": ["Node Classification", "Link Prediction", "Graph Classification", "Node Clustering", "Community Detection", "Graph Clustering", "Node Classification", "Link Prediction", "Graph Classification", "Node Clustering", "Community Detection", "Graph Clustering"], "id": "c22c8d09-f28f-4d59-9c29-08b1b947fe4e", "image_url": ""}, {"title": "CityPersons", "short_description": "The **CityPersons** dataset is a subset of Cityscapes which only consists of person annotations. There are 2975 images for training, 500 and 1575 images for validation and testing. The average of the number of pedestrians in an image is 7. The visible-region and full-body annotations are provided.\r\n\r\nSource: [NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing](https://arxiv.org/abs/2003.12729)\r\nImage Source: [https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons](https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons)", "price": 165, "created_at": "2025-01-09 13:44:40.871004", "keyword": ["Object Detection", "Pedestrian Detection", "Object Detection", "Pedestrian Detection"], "id": "36580405-394c-48f3-9557-eac8a1062f5a", "image_url": "https://production-media.paperswithcode.com/datasets/CityPersons-0000000865-555caf6c_SYqFKMQ.jpg"}, {"title": "Cityscapes", "short_description": "**Cityscapes** is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.\r\n\r\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [https://www.cityscapes-dataset.com/dataset-overview/](https://www.cityscapes-dataset.com/dataset-overview/)", "price": 146, "created_at": "2025-01-09 13:44:40.871102", "keyword": ["Image Generation", "Semantic Segmentation", "Instance Segmentation", "Image-to-Image Translation", "Scene Parsing", "Depth Estimation", "Semi-Supervised Semantic Segmentation", "Unsupervised Semantic Segmentation", "Video Prediction", "Panoptic Segmentation", "Federated Learning", "Monocular Depth Estimation", "Interactive Segmentation", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Multi-Task Learning", "Open Vocabulary Semantic Segmentation", "Edge Detection", "Real-Time Semantic Segmentation", "Real-time Instance Segmentation", "Video Semantic Segmentation", "Robust Object Detection", "Unsupervised Monocular Depth Estimation", "Overlapped 10-1", "Domain 11-5", "Domain 11-1", "Domain 1-1", "Overlapped 14-1", "Image Generation", "Semantic Segmentation", "Instance Segmentation", "Image-to-Image Translation", "Scene Parsing", "Depth Estimation", "Semi-Supervised Semantic Segmentation", "Unsupervised Semantic Segmentation", "Video Prediction", "Panoptic Segmentation", "Federated Learning", "Monocular Depth Estimation", "Interactive Segmentation", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Multi-Task Learning", "Open Vocabulary Semantic Segmentation", "Edge Detection", "Real-Time Semantic Segmentation", "Real-time Instance Segmentation", "Video Semantic Segmentation", "Robust Object Detection", "Unsupervised Monocular Depth Estimation", "Overlapped 10-1", "Domain 11-5", "Domain 11-1", "Domain 1-1", "Overlapped 14-1"], "id": "d46321fc-5475-4f4c-b476-b168ad8b6cf2", "image_url": "https://production-media.paperswithcode.com/datasets/Cityscapes-0000003437-d7b741b4.jpg"}, {"title": "CliCR", "short_description": "CliCR is a new dataset for domain specific reading comprehension used to construct around 100,000 cloze queries from clinical case reports.\r\n\r\nSource: [CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension](https://arxiv.org/pdf/1803.09720v1.pdf)", "price": 792, "created_at": "2025-01-09 13:44:40.871204", "keyword": ["Question Answering", "Reading Comprehension"], "id": "6b0f31dc-24d0-4e17-9ea4-ed44d08f3fe4", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_18.58.03.png"}, {"title": "CoLA", "short_description": "The **Corpus of Linguistic Acceptability** (**CoLA**) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.\r\n\r\nSource: [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)\r\nImage Source: [https://arxiv.org/pdf/1805.12471.pdf](https://arxiv.org/pdf/1805.12471.pdf)", "price": 716, "created_at": "2025-01-09 13:44:40.871301", "keyword": ["Text Generation", "Stochastic Optimization", "Linguistic Acceptability", "CoLA"], "id": "de2dd364-174d-49b6-9fce-114723c1f9ba", "image_url": "https://production-media.paperswithcode.com/datasets/CoLA-0000001097-ef534b16_ibEVODH.jpg"}, {"title": "CoNLL 2002", "short_description": "The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task.\r\n\r\nSource: [CoNLL 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\r\nImage Source: [https://www.aclweb.org/anthology/W02-2024.pdf](https://www.aclweb.org/anthology/W02-2024.pdf)", "price": 281, "created_at": "2025-01-09 13:44:40.871398", "keyword": ["Token Classification", "Named Entity Recognition (NER)", "Cross-Lingual Transfer", "Part-Of-Speech Tagging"], "id": "88a32352-91f8-4f0f-8cd7-934701a96ecd", "image_url": "https://production-media.paperswithcode.com/datasets/CoNLL_2002-0000003554-c2c3dd2e.jpg"}, {"title": "CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings", "short_description": "Automatic segmentation, tokenization and morphological and syntactic annotations of raw texts in 45 languages, generated by UDPipe (http://ufal.mff.cuni.cz/udpipe), together with word embeddings of dimension 100 computed from lowercased texts by word2vec (https://code.google.com/archive/p/word2vec/).", "price": 151, "created_at": "2025-01-09 13:44:40.871499", "keyword": ["Dependency Parsing", "Part-Of-Speech Tagging", "Sequential sentence segmentation", "Text Segmentation", "Sentence segmentation", "Morphological Tagging"], "id": "f5979885-1d64-4d26-9cc9-da7298329de2", "image_url": ""}, {"title": "CoNLL-2000", "short_description": "CoNLL-2000 is a dataset for dividing text into syntactically related non-overlapping groups of words, so-called text chunking. \r\n\r\nSource: [https://www.clips.uantwerpen.be/conll2000/chunking/](https://www.clips.uantwerpen.be/conll2000/chunking/)", "price": 415, "created_at": "2025-01-09 13:44:40.871597", "keyword": ["Named Entity Recognition (NER)", "Chunking"], "id": "61c88e76-4ca0-4885-9b08-03d99e94f850", "image_url": ""}, {"title": "CoNLL-2009", "short_description": "The task builds on the CoNLL-2008 task and extends it to multiple languages. The core of the task is to predict syntactic and semantic dependencies and their labeling. Data is provided for both statistical training and evaluation, which extract these labeled dependencies from manually annotated treebanks such as the Penn Treebank for English, the Prague Dependency Treebank for Czech and similar treebanks for Catalan, Chinese, German, Japanese and Spanish languages, enriched with semantic relations (such as those captured in the Prop/Nombank and similar resources). Great effort has been devoted to provide the participants with a common and relatively simple data representation for all the languages, similar to the last year's English data.", "price": 999, "created_at": "2025-01-09 13:44:40.871694", "keyword": ["Dependency Parsing", "Semantic Role Labeling", "Chinese Semantic Role Labeling"], "id": "e89eac98-8381-4b6a-9dc4-1dc047a5b170", "image_url": ""}, {"title": "CoNLL-2012", "short_description": "The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus. It was a follow-on to the English-only task organized in 2011.\r\n\r\nSource: [Pradhan et al.](https://www.aclweb.org/anthology/W12-4501.pdf)", "price": 704, "created_at": "2025-01-09 13:44:40.871791", "keyword": ["Coreference Resolution", "Semantic Role Labeling", "Predicate Detection", "Semantic Role Labeling (predicted predicates)"], "id": "4fb55727-fbe5-4dab-8cf2-7e8be4d2af42", "image_url": ""}, {"title": "CoNLL-2014 Shared Task: Grammatical Error Correction", "short_description": "CoNLL-2014 will continue the CoNLL tradition of having a high profile shared task in natural language processing. This year's shared task will be grammatical error correction, a continuation of the CoNLL shared task in 2013. A participating system in this shared task is given short English texts written by non-native speakers of English. The system detects the grammatical errors present in the input texts, and returns the corrected essays. The shared task in 2014 will require a participating system to correct all errors present in an essay (i.e., not restricted to just five error types in 2013). Also, the evaluation metric will be changed to F0.5, weighting precision twice as much as recall.\r\n\r\nThe grammatical error correction task is impactful since it is estimated that hundreds of millions of people in the world are learning English and they benefit directly from an automated grammar checker. However, for many error types, current grammatical error correction methods do not achieve a high performance and thus more research is needed.\r\n\r\nSource: [CoNLL-2014 Shared Task: Grammatical Error Correction](https://www.comp.nus.edu.sg/~nlp/conll14st.html)\r\n\r\nImage source: [Tou Ng et al.](https://www.aclweb.org/anthology/W14-1701.pdf)", "price": 394, "created_at": "2025-01-09 13:44:40.871890", "keyword": ["Grammatical Error Correction"], "id": "8d90a1aa-06ef-44ec-9303-f2ca14b9c7dd", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-17_at_15.15.30.png"}, {"title": "CoQA", "short_description": "**CoQA** is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.\r\n\r\nCoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.\r\n\r\nSource: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)\r\nImage Source: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)", "price": 915, "created_at": "2025-01-09 13:44:40.871992", "keyword": ["Question Answering", "Reading Comprehension", "Generative Question Answering", "Conversational Question Answering"], "id": "24e52c1f-cbea-45a7-9701-6749b525f526", "image_url": "https://production-media.paperswithcode.com/datasets/CoQA-0000000178-5cfa9088_c6EFuhZ.jpg"}, {"title": "CodeSearchNet", "short_description": "The **CodeSearchNet** Corpus is a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. The CodeSearchNet Corpus includes:\r\n* Six million methods overall\r\n* Two million of which have associated documentation (docstrings, JavaDoc, and more)\r\n* Metadata that indicates the original location (repository or line number, for example) where the data was found\r\n\r\nSource: [https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/)", "price": 461, "created_at": "2025-01-09 13:44:40.872094", "keyword": ["Source Code Summarization", "Code Search", "Code Documentation Generation", "Method name prediction"], "id": "15a63377-f728-4311-9c74-62cd3b61477b", "image_url": ""}, {"title": "Comic2k", "short_description": "**Comic2k** is a dataset used for cross-domain object detection which contains 2k comic images with image and instance-level annotations.\r\nImage Source: [https://naoto0804.github.io/cross_domain_detection/](https://naoto0804.github.io/cross_domain_detection/)", "price": 874, "created_at": "2025-01-09 13:44:40.872194", "keyword": ["Domain Adaptation", "Weakly Supervised Object Detection", "Unsupervised Object Detection", "Class-agnostic Object Detection", "Body Detection", "Object Proposal Generation"], "id": "3e87b937-c192-462e-b060-1402c1638ce9", "image_url": "https://production-media.paperswithcode.com/datasets/Comic2k-0000000890-bfd125cd_dDnoKjH.jpg"}, {"title": "CommonsenseQA", "short_description": "The **CommonsenseQA** is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each.\r\nThe dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):\r\n\r\n1. a crowd worker observes a source concept from ConceptNet (\u201cRiver\u201d) and three target concepts (\u201cWaterfall\u201d, \u201cBridge\u201d, \u201cValley\u201d) that are all related by the same ConceptNet relation (\u201cAtLocation\u201d),\r\n2. the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (\u201cWhere on a river can you hold a cup upright to catch water on a sunny day?\u201d, \u201cWhere can I stand on a river to see water falling without getting wet?\u201d, \u201cI\u2019m crossing the river, my feet are wet but my body is dry, where am I?\u201d)\r\n3. for each question, another worker chooses one additional distractor from Concept Net (\u201cpebble\u201d, \u201cstream\u201d, \u201cbank\u201d), and the author another distractor (\u201cmountain\u201d, \u201cbottom\u201d, \u201cisland\u201d) manually.\r\n\r\nSource: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge/)\r\nImage Source: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge/)", "price": 33, "created_at": "2025-01-09 13:44:40.872292", "keyword": ["Question Answering", "Common Sense Reasoning"], "id": "90d31235-1c57-438f-854b-4ab4d3168e04", "image_url": "https://production-media.paperswithcode.com/datasets/CommonsenseQA-0000001291-20b023fe_ZaXpfVv.jpg"}, {"title": "CompCars", "short_description": "The **Comprehensive Cars (CompCars)** dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. The surveillance-nature data contains 50,000 car images captured in the front view. \r\n\r\nThe dataset can be used for the tasks of:\r\n\r\n- Fine-grained classification\r\n- Attribute prediction\r\n- Car model verification\r\n\r\nThe dataset can be also used for other tasks such as image ranking, multi-task learning, and 3D reconstruction.", "price": 632, "created_at": "2025-01-09 13:44:40.872390", "keyword": ["Fine-Grained Image Classification", "Fine-Grained Image Classification"], "id": "0785a009-cab1-4e2d-a69f-b54517088d24", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_4.44.03_PM.png"}, {"title": "ConceptNet", "short_description": "ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. \r\n\r\nSource: [ConceptNet 5.5: An Open Multilingual Graph of General Knowledge](https://arxiv.org/pdf/1612.03975v2.pdf)\r\nImage Source: [Speer et al](https://arxiv.org/pdf/1612.03975v2.pdf)", "price": 508, "created_at": "2025-01-09 13:44:40.872490", "keyword": ["Question Answering", "Knowledge Graphs", "Word Embeddings"], "id": "7a766b14-34e5-483f-837b-2d212afce733", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_14.26.11.png"}, {"title": "ContactDB", "short_description": "**ContactDB** is a dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. ContactDB includes 3,750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images.\r\n\r\nSource: [https://arxiv.org/abs/1904.06830](https://arxiv.org/abs/1904.06830)\r\nImage Source: [https://github.com/samarth-robo/contactdb_utils](https://github.com/samarth-robo/contactdb_utils)", "price": 145, "created_at": "2025-01-09 13:44:40.872587", "keyword": ["Grasp Contact Prediction"], "id": "5d9afde9-cd36-4ccf-9237-ca30c1271dd6", "image_url": "https://production-media.paperswithcode.com/datasets/ContactDB-0000000883-cc919031.gif"}, {"title": "ConvAI2", "short_description": "The **ConvAI2** NeurIPS competition aimed at finding approaches to creating high-quality dialogue agents capable of meaningful open domain conversation. The ConvAI2 dataset for training models is based on the PERSONA-CHAT dataset. The speaker pairs each have assigned profiles coming from a set of 1155 possible personas (at training time), each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation. As the original PERSONA-CHAT test set was released, a new hidden test set consisted of 100 new personas and over 1,015 dialogs was created by crowdsourced workers.\r\n\r\nTo avoid modeling that takes advantage of trivial word overlap, additional rewritten sets of the same train and test personas were crowdsourced, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging. For example \u201cI just got my nails done\u201d is revised as \u201cI love to pamper myself on a regular basis\u201d and \u201cI am on a diet now\u201d is revised as \u201cI need to lose weight.\u201d\r\n\r\nThe training, validation and hidden test sets consists of 17,878, 1,000 and 1,015 dialogues, respectively.\r\n\r\nSource: [The Second Conversational Intelligence Challenge (ConvAI2)](https://paperswithcode.com/paper/the-second-conversational-intelligence/)\r\nImage Source: [The Second Conversational Intelligence Challenge (ConvAI2)](https://paperswithcode.com/paper/the-second-conversational-intelligence/)", "price": 420, "created_at": "2025-01-09 13:44:40.872684", "keyword": ["Visual Dialog"], "id": "65ac3363-1102-4ba6-b3b0-cecda3532608", "image_url": "https://production-media.paperswithcode.com/datasets/ConvAI2-0000003063-95759eda.jpg"}, {"title": "Cora", "short_description": "The **Cora** dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\r\n\r\nSource: [https://relational.fit.cvut.cz/dataset/CORA](https://relational.fit.cvut.cz/dataset/CORA)\r\nImage Source: [https://arxiv.org/abs/1611.08402](https://arxiv.org/abs/1611.08402)", "price": 911, "created_at": "2025-01-09 13:44:40.872785", "keyword": ["Node Classification", "Link Prediction", "Graph Classification", "Document Classification", "Node Clustering", "Community Detection", "Graph Clustering", "Graph structure learning", "Node Classification", "Link Prediction", "Graph Classification", "Document Classification", "Node Clustering", "Community Detection", "Graph Clustering", "Graph structure learning"], "id": "795f384b-d7f4-44d9-b724-f4a3a6758ba4", "image_url": "https://production-media.paperswithcode.com/datasets/Cora-0000000700-ce1c5ec7_LD7pZnT.jpg"}, {"title": "Criteo", "short_description": "**Criteo** contains 7 days of click-through data, which is widely used for CTR prediction benchmarking. There are 26 anonymous categorical fields and 13 continuous fields in Criteo dataset.\n\nSource: [AMER: Automatic Behavior Modeling and Interaction Exploration in Recommender System](https://arxiv.org/abs/2006.05933)\nImage Source: [https://www.kaggle.com/c/criteo-display-ad-challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)", "price": 938, "created_at": "2025-01-09 13:44:40.872884", "keyword": ["Click-Through Rate Prediction"], "id": "c48400c2-93a2-4802-bd16-9b76008ea5f6", "image_url": "https://production-media.paperswithcode.com/datasets/Criteo-0000000671-6394e953_cClQwMb.jpeg"}, {"title": "CrowdFlow", "short_description": "The **TUB CrowdFlow** is a synthetic dataset that contains 10 sequences showing 5 scenes. Each scene is rendered twice: with a static point of view and a dynamic camera to simulate drone/UAV based surveillance. The scenes are render using Unreal Engine at HD resolution (1280x720) at 25 fps, which is typical for current commercial CCTV surveillance systems. The total number of frames is 3200.\r\n\r\nEach sequence has the following ground-truth data:\r\n\r\n* Optical flow fields\r\n* Person trajectories (up to 1451)\r\n* Dense pixel trajectories\r\n\r\nSource: [Optical Flow Dataset and Benchmark for Visual Crowd Analysis](/paper/optical-flow-dataset-and-benchmark-for-visual)", "price": 311, "created_at": "2025-01-09 13:44:40.872981", "keyword": ["Optical Flow Estimation", "Crowd Counting", "Visual Crowd Analysis"], "id": "199380d5-ec81-4fa0-a4bb-e1ed62ae9ffa", "image_url": "https://production-media.paperswithcode.com/datasets/tub.jpg"}, {"title": "CrowdHuman", "short_description": "**CrowdHuman** is a large and rich-annotated human detection dataset, which contains 15,000, 4,370 and 5,000 images collected from the Internet for training, validation and testing respectively. The number is more than 10\u00d7 boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others with \u223c340k person and \u223c99k ignore region annotations in the CrowdHuman training subset.\r\n\r\nSource: [SADet: Learning An Efficient and Accurate Pedestrian Detector](https://arxiv.org/abs/2007.13119)\r\nImage Source: [http://www.crowdhuman.org/](http://www.crowdhuman.org/)", "price": 560, "created_at": "2025-01-09 13:44:40.873079", "keyword": ["Object Detection"], "id": "016f37e9-72e9-4ba6-a306-31b365554216", "image_url": "https://production-media.paperswithcode.com/datasets/CrowdHuman-0000003420-7651fd76.jpg"}, {"title": "CrowdPose", "short_description": "The **CrowdPose** dataset contains about 20,000 images and a total of 80,000 human poses with 14 labeled keypoints. The test set includes 8,000 images. The crowded images containing homes are extracted from MSCOCO, MPII and AI Challenger.\r\n\r\nSource: [Human Pose Estimation for Real-World Crowded Scenarios](https://arxiv.org/abs/1907.06922)\r\nImage Source: [https://github.com/Jeff-sjtu/CrowdPose](https://github.com/Jeff-sjtu/CrowdPose)", "price": 809, "created_at": "2025-01-09 13:44:40.873181", "keyword": ["Pose Estimation", "Multi-Person Pose Estimation"], "id": "be13203c-4f84-4aa1-92f5-5f6a6701bec8", "image_url": "https://production-media.paperswithcode.com/datasets/CrowdPose-0000001017-3bd44145_peu0SYO.gif"}, {"title": "DAQUAR", "short_description": "DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images.\r\n\r\nSource: [A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input](/paper/a-multi-world-approach-to-question-answering)\r\nImage Source: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)", "price": 178, "created_at": "2025-01-09 13:44:40.873279", "keyword": ["Object Detection", "Question Answering", "Visual Question Answering (VQA)", "Object Detection", "Question Answering", "Visual Question Answering (VQA)"], "id": "6e6e2351-8f6b-4864-9a99-1704f5ccfaa2", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_17.19.18.png"}, {"title": "DAVIS", "short_description": "The Densely Annotation Video Segmentation dataset (**DAVIS**) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation.\r\n\r\nSource: [TENet: Triple Excitation Network for Video Salient Object Detection](https://arxiv.org/abs/2007.09943)", "price": 435, "created_at": "2025-01-09 13:44:40.873376", "keyword": ["Semantic Segmentation", "Video Object Segmentation", "Video Prediction", "Video Frame Interpolation", "Semi-Supervised Video Object Segmentation", "Interactive Segmentation", "Video Denoising", "Visual Tracking", "Video Inpainting", "Interactive Video Object Segmentation"], "id": "a6644211-98bb-401a-95af-0f33c3dc47f9", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_5.55.43_PM.png"}, {"title": "DAVIS 2016", "short_description": "DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered.\r\n\r\nSource: [Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2008.01270)\r\nImage Source: [https://davischallenge.org/](https://davischallenge.org/)", "price": 367, "created_at": "2025-01-09 13:44:40.873478", "keyword": ["Video Object Segmentation", "Semi-Supervised Video Object Segmentation", "Unsupervised Object Segmentation", "Unsupervised Video Object Segmentation", "Video Object Segmentation", "Semi-Supervised Video Object Segmentation", "Unsupervised Object Segmentation", "Unsupervised Video Object Segmentation"], "id": "05ca76e5-5ee4-45ee-bdd7-914653c695d8", "image_url": "https://production-media.paperswithcode.com/datasets/DAVIS_2016-0000000415-f8d14256_N8lF8W9.jpg"}, {"title": "DAVIS 2017", "short_description": "DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing\r\n\r\nSource: [Siam R-CNN: Visual Tracking by Re-Detection](https://arxiv.org/abs/1911.12836)\r\nImage Source: [https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902](https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902)", "price": 214, "created_at": "2025-01-09 13:44:40.873577", "keyword": ["Semantic Segmentation", "Video Object Segmentation", "Referring Expression Segmentation", "Video Prediction", "Semi-Supervised Video Object Segmentation", "Unsupervised Video Object Segmentation", "Interactive Video Object Segmentation", "Semantic Segmentation", "Video Object Segmentation", "Referring Expression Segmentation", "Video Prediction", "Semi-Supervised Video Object Segmentation", "Unsupervised Video Object Segmentation", "Interactive Video Object Segmentation"], "id": "31c61946-9b46-4971-9548-a94b62ffc5e0", "image_url": "https://production-media.paperswithcode.com/datasets/DAVIS_2017-0000000418-ffe1b4d1_hFOqNd4.jpg"}, {"title": "DBLP", "short_description": "The **DBLP** is a citation network dataset. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title.\r\nThe data set can be used for clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis, etc.\r\n\r\nSource: [https://www.aminer.org/citation](https://www.aminer.org/citation)", "price": 266, "created_at": "2025-01-09 13:44:40.873678", "keyword": ["Node Classification", "Link Prediction", "Node Clustering", "Community Detection", "Heterogeneous Node Classification", "Node Classification", "Link Prediction", "Node Clustering", "Community Detection", "Heterogeneous Node Classification"], "id": "03f84fac-69b3-44ba-9440-d53d941c8015", "image_url": ""}, {"title": "DBpedia", "short_description": "**DBpedia** (from \"DB\" for \"database\") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.\r\n\r\nSource: [https://en.wikipedia.org/wiki/DBpedia](https://en.wikipedia.org/wiki/DBpedia)", "price": 537, "created_at": "2025-01-09 13:44:40.873776", "keyword": ["Text Classification", "Zero-shot Text Search", "Open Intent Discovery", "Text Classification", "Zero-shot Text Search", "Open Intent Discovery"], "id": "3d025231-f2b4-40f2-8bab-ee4840cc140e", "image_url": "https://production-media.paperswithcode.com/datasets/dbpedia.png"}, {"title": "DDI", "short_description": "The **DDI**Extraction 2013 task relies on the DDI corpus which contains MedLine abstracts on drug-drug interactions as well as documents describing drug-drug interactions from the DrugBank database.\r\n\r\nSource: [DDIExtraction 2013](https://www.cs.york.ac.uk/semeval-2013/task9/)\r\nImage Source: [https://www.aclweb.org/anthology/S13-2056.pdf](https://www.aclweb.org/anthology/S13-2056.pdf)", "price": 614, "created_at": "2025-01-09 13:44:40.873874", "keyword": ["Relation Extraction", "Medical Relation Extraction", "Drug\u2013drug Interaction Extraction"], "id": "0e1d1c8f-8330-4bb5-9527-3b900f57c733", "image_url": "https://production-media.paperswithcode.com/datasets/DDI-0000003589-2d33fea9.jpg"}, {"title": "DICM", "short_description": "**DICM** is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras.\n\nSource: [Deep Retinex Decomposition for Low-Light Enhancement](https://arxiv.org/abs/1808.04560)\nImage Source: [GLADNet: Low-Light Enhancement Network with Global Awareness](https://ieeexplore.ieee.org/document/8373911)", "price": 563, "created_at": "2025-01-09 13:44:40.873971", "keyword": ["Low-Light Image Enhancement"], "id": "6f5651de-bd51-41b4-b6a8-ee6901953d35", "image_url": "https://production-media.paperswithcode.com/datasets/DICM-0000002270-00810389_FHDjAAf.jpg"}, {"title": "DIRHA", "short_description": "**DIRHA**-English is a multi-microphone database composed of real and simulated sequences of 1-minute. The overall corpus is composed of different types of sequences including: 1) Phonetically-rich sentences; 2) WSJ 5-k utterances; 3) WSJ 20-k utterances; 4) Conversational speech (also including keywords and commands).\r\nThe sequences are available for both UK and US English at 48 kHz. The DIRHA-English dataset offers the possibility to work with a very large number of microphone channels, to use of microphone arrays having different characteristics and to work considering different speech recognition tasks (e.g., phone-loop, keyword spotting, ASR with small and very large language models).\r\n\r\nSource: [The DIRHA-English Corpus](http://dirha.fbk.eu/DIRHA_English)\r\nImage Source: [https://arxiv.org/pdf/1710.02560v1.pdf](https://arxiv.org/pdf/1710.02560v1.pdf)", "price": 812, "created_at": "2025-01-09 13:44:40.874075", "keyword": ["Distant Speech Recognition", "Distant Speech Recognition"], "id": "97930278-c241-45ee-a22d-cb9a32c1226f", "image_url": "https://production-media.paperswithcode.com/datasets/DIRHA-0000003285-55bf480e.jpg"}, {"title": "DISFA", "short_description": "The **Denver Intensity of Spontaneous Facial Action** (**DISFA**) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.\r\n\r\nSource: [Deep Learning For Smile Recognition](https://arxiv.org/abs/1602.00172)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237](https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237)", "price": 865, "created_at": "2025-01-09 13:44:40.874173", "keyword": ["Facial Expression Recognition (FER)", "Facial Action Unit Detection", "Smile Recognition", "Facial Expression Recognition (FER)", "Facial Action Unit Detection", "Smile Recognition"], "id": "a82f51db-7215-4872-84fc-1d67de6551f3", "image_url": "https://production-media.paperswithcode.com/datasets/DISFA-0000000106-e2d4b038_drE05Fn.jpg"}, {"title": "DIV2K", "short_description": "**DIV2K** is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild \u00d74 and realistic wild \u00d74 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image.\r\n\r\nSource: [Unsupervised Image Super-Resolution with an Indirect Supervised Path](https://arxiv.org/abs/1910.02593)", "price": 434, "created_at": "2025-01-09 13:44:40.874269", "keyword": ["Image Super-Resolution", "JPEG Artifact Correction", "Jpeg Compression Artifact Reduction", "Image Super-Resolution", "JPEG Artifact Correction", "Jpeg Compression Artifact Reduction"], "id": "b078dada-0303-4996-8906-79ba0287d4da", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.34.07_PM.png"}, {"title": "DOTA", "short_description": "DOTA is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. The images are collected from different sensors and platforms. Each image is of the size in the range from 800 \u00d7 800 to 20,000 \u00d7 20,000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. The instances in DOTA images are annotated by experts in aerial image interpretation by arbitrary (8 d.o.f.) quadrilateral. We will continue to update DOTA, to grow in size and scope to reflect evolving real-world conditions. Now it has three versions:\r\n\r\nDOTA-v1.0 contains 15 common categories, 2,806 images and 188, 282 instances. The proportions of the training set, validation set, and testing set in DOTA-v1.0 are 1/2, 1/6, and 1/3, respectively.\r\n\r\nDOTA-v1.5 uses the same images as DOTA-v1.0, but the extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category, \u201dcontainer crane\u201d is added. It contains 403,318 instances in total. The number of images and dataset splits are the same as DOTA-v1.0. This version was released for the DOAI Challenge 2019 on Object Detection in Aerial Images in conjunction with IEEE CVPR 2019.\r\n\r\nDOTA-v2.0 collects more Google Earth, GF-2 Satellite, and aerial images. There are 18 common categories, 11,268 images and 1,793,658 instances in DOTA-v2.0. Compared to DOTA-v1.5, it further adds the new categories of \u201dairport\u201d and \u201dhelipad\u201d. The 11,268 images of DOTA are split into training, validation, test-dev, and test-challenge sets. To avoid the problem of overfitting, the proportion of training and validation set is smaller than the test set. Furthermore, we have two test sets, namely test-dev and test-challenge. Training contains 1,830 images and 268,627 instances. Validation contains 593 images and 81,048 instances. We released the images and ground truths for training and validation sets. Test-dev contains 2,792 images and 353,346 instances. We released the images but not the ground truths. Test-challenge contains 6,053 images and 1,090,637 instances.\r\n\r\nSource: [https://captain-whu.github.io/DOTA/index.html](https://captain-whu.github.io/DOTA/index.html)\r\nImage Source: [https://captain-whu.github.io/DOTA/](https://captain-whu.github.io/DOTA/)", "price": 392, "created_at": "2025-01-09 13:44:40.874367", "keyword": ["Small Object Detection", "Object Detection In Aerial Images", "Oriented Object Detection", "Dense Object Detection", "Oriented Object Detction"], "id": "9c5fa2d4-71d9-4f27-9ba3-8a548dc22ae8", "image_url": "https://production-media.paperswithcode.com/datasets/DOTA-0000000287-8115825a_h3m67GA.jpg"}, {"title": "DRCD", "short_description": "Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.\r\n\r\nSource: [https://github.com/DRCKnowledgeTeam/DRCD](https://github.com/DRCKnowledgeTeam/DRCD)\r\nImage Source: [https://arxiv.org/pdf/1806.00920.pdf](https://arxiv.org/pdf/1806.00920.pdf)", "price": 222, "created_at": "2025-01-09 13:44:40.874464", "keyword": ["Question Answering", "Reading Comprehension", "Chinese Reading Comprehension", "Machine Reading Comprehension", "Reading Comprehension (Zero-Shot)", "Reading Comprehension (One-Shot)", "Reading Comprehension (Few-Shot)"], "id": "7e34d721-0e3d-4692-a81d-c6d79a36d232", "image_url": "https://production-media.paperswithcode.com/datasets/DRCD-0000003571-e560757d.jpg"}, {"title": "DREAM", "short_description": "DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.\r\n\r\nDREAM contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.\r\n\r\nSource: [DREAM](https://dataset.org/dream/)", "price": 563, "created_at": "2025-01-09 13:44:40.874562", "keyword": ["Question Answering", "Reading Comprehension", "Machine Reading Comprehension", "Sleep spindles detection"], "id": "9f8d2e47-124d-48c3-853b-30f294fd0f89", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_14.20.54.png"}, {"title": "DRIVE", "short_description": "The **Digital Retinal Images for Vessel Extraction** (**DRIVE**) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). \r\n\r\nThe set of 40 images was equally divided into 20 images for the training set and 20 images for the testing set. Inside both sets, for each image, there is circular field of view (FOV) mask of diameter that is approximately 540 pixels. Inside training set, for each image, one manual segmentation by an ophthalmological expert has been applied. Inside testing set, for each image, two manual segmentations have been applied by two different observers, where the first observer segmentation is accepted as the ground-truth for performance evaluation.\r\n\r\nSource: [Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation](https://arxiv.org/abs/1403.1735)\r\nImage Source: [https://drive.grand-challenge.org/](https://drive.grand-challenge.org/)", "price": 984, "created_at": "2025-01-09 13:44:40.874659", "keyword": ["Medical Image Segmentation", "Retinal Vessel Segmentation", "Medical Image Segmentation", "Retinal Vessel Segmentation"], "id": "ea2601cd-f892-4490-9613-a12fa5e0bd05", "image_url": "https://production-media.paperswithcode.com/datasets/DRIVE-0000000645-35ad801c_XfFAMac.jpg"}, {"title": "DROP", "short_description": "**Discrete Reasoning Over Paragraphs** **DROP** is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. The questions consist of passages extracted from Wikipedia articles. The dataset is split into a training set of about 77,000 questions, a development set of around 9,500 questions and a hidden test set similar in size to the development set.\r\n\r\nSource: [https://allennlp.org/drop](https://allennlp.org/drop)\r\nImage Source: [DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://paperswithcode.com/paper/drop-a-reading-comprehension-benchmark/)", "price": 711, "created_at": "2025-01-09 13:44:40.874755", "keyword": ["Question Answering"], "id": "cbb63f85-20e3-4806-8e39-4e25860918a7", "image_url": "https://production-media.paperswithcode.com/datasets/DROP-0000003366-d86f78a5.jpg"}, {"title": "DTD", "short_description": "The **Describable Textures Dataset** (**DTD**) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.\r\n\r\nSource: [Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting](https://arxiv.org/abs/1911.02274)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/dtd/](https://www.robots.ox.ac.uk/~vgg/data/dtd/)", "price": 903, "created_at": "2025-01-09 13:44:40.874852", "keyword": ["Image Classification", "Few-Shot Learning", "Neural Architecture Search", "Prompt Engineering"], "id": "b5449248-e6fc-4973-8caa-d0c5f541ac36", "image_url": "https://production-media.paperswithcode.com/datasets/DTD-0000002377-abe5e400_AubcN36.jpg"}, {"title": "DUC 2004", "short_description": "The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents.\r\n\r\nSource: [Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction](https://arxiv.org/abs/2005.01791)\r\nImage Source: [https://duc.nist.gov/duc2004/](https://duc.nist.gov/duc2004/)", "price": 352, "created_at": "2025-01-09 13:44:40.874949", "keyword": ["Text Summarization", "Extractive Text Summarization", "Multi-Document Summarization"], "id": "c8523bf3-52d6-4db2-8164-52c7737ac2dd", "image_url": "https://production-media.paperswithcode.com/datasets/DUC_2004-0000002930-802538ef_yqCgZoj.jpg"}, {"title": "DUT-OMRON", "short_description": "The **DUT-OMRON** dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background.\r\n\r\nSource: [Global Context-Aware Progressive Aggregation Network for Salient Object Detection](https://arxiv.org/abs/2003.00651)", "price": 94, "created_at": "2025-01-09 13:44:40.875046", "keyword": ["RGB Salient Object Detection", "Saliency Detection", "Salient Object Detection", "Unsupervised Saliency Detection"], "id": "fc16dbae-07d8-49f0-9484-a30837d79053", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.13.13_PM.png"}, {"title": "DUTS", "short_description": "**DUTS** is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.\r\n\r\nSource: [http://saliencydetection.net/duts/](http://saliencydetection.net/duts/)\r\nImage Source: [https://ieeexplore.ieee.org/document/8099887](https://ieeexplore.ieee.org/document/8099887)", "price": 735, "created_at": "2025-01-09 13:44:40.875143", "keyword": ["RGB Salient Object Detection", "Saliency Detection", "Unsupervised Object Segmentation", "Salient Object Detection", "Unsupervised Saliency Detection"], "id": "7d829da6-df4f-462b-8115-8f5d2033f410", "image_url": "https://production-media.paperswithcode.com/datasets/DUTS-0000003406-babab16e.jpg"}, {"title": "DVQA", "short_description": "DVQA is a synthetic question-answering dataset on images of bar-charts.", "price": 82, "created_at": "2025-01-09 13:44:40.875240", "keyword": ["Object Detection", "Question Answering", "Visual Question Answering (VQA)", "Chart Question Answering"], "id": "63a93532-510f-450a-8188-7b44d51c8d4d", "image_url": "https://production-media.paperswithcode.com/datasets/dvqa.jpg"}, {"title": "DailyDialog", "short_description": "**DailyDialog** is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.\r\n\r\nSource: [http://yanran.li/dailydialog](http://yanran.li/dailydialog)\r\nImage Source: [https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/](https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/)", "price": 572, "created_at": "2025-01-09 13:44:40.875343", "keyword": ["Text Generation", "Emotion Recognition in Conversation"], "id": "8ae81d28-f77a-4fb6-86f2-ce7fce566ff0", "image_url": "https://production-media.paperswithcode.com/datasets/DailyDialog-0000000519-98628e60_NBZosb7.jpg"}, {"title": "Dayton", "short_description": "The **Dayton** dataset is a dataset for ground-to-aerial (or aerial-to-ground) image translation, or cross-view image synthesis. It contains images of road views and aerial views of roads. There are 76,048 images in total and the train/test split is 55,000/21,048. The images in the original dataset have 354\u00d7354 resolution.\n\nSource: [Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation](https://arxiv.org/abs/2002.01048)\nImage Source: [https://arxiv.org/abs/1912.06112](https://arxiv.org/abs/1912.06112)", "price": 511, "created_at": "2025-01-09 13:44:40.875441", "keyword": ["Cross-View Image-to-Image Translation"], "id": "c7def143-70c1-43a6-8d20-bb244924f01b", "image_url": "https://production-media.paperswithcode.com/datasets/Dayton-0000003398-6d9d8240.jpeg"}, {"title": "DeepFashion", "short_description": "**DeepFashion** is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.\r\n\r\nSource: [A Benchmark for Inpainting of Clothing Images with Irregular Holes](https://arxiv.org/abs/2007.05080)", "price": 591, "created_at": "2025-01-09 13:44:40.875539", "keyword": ["Image Retrieval", "Image-to-Image Translation", "Pose Transfer", "Virtual Try-on", "Unsupervised Human Pose Estimation", "Text-to-3D-Human Generation"], "id": "59dd91c0-e2ff-4c8b-91aa-4cdda51ddf62", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_3.57.21_PM.png"}, {"title": "DeepLoc", "short_description": "**DeepLoc** is a large-scale urban outdoor localization dataset. The dataset is currently comprised of one scene spanning an area of 110 x 130 m, that a robot traverses multiple times with different driving patterns. The dataset creators use a LiDAR-based SLAM system with sub-centimeter and sub-degree accuracy to compute the pose labels that provided as groundtruth. Poses in the dataset are approximately spaced by 0.5 m which is twice as dense as other relocalization datasets.\r\n\r\nFurthermore, for each image the dataset creators provide pixel-wise semantic segmentation annotations for ten categories: Background, Sky, Road, Sidewalk, Grass, Vegetation, Building, Poles & Fences, Dynamic and Void. The dataset is divided into a train and test splits such that the train set comprises seven loops with alternating driving styles amounting to 2737 images, while the test set comprises three loops with a total of 1173 images. The dataset also contains global GPS/INS data and LiDAR measurements.\r\n\r\nThis dataset can be very challenging for vision based applications such as global localization, camera relocalization, semantic segmentation, visual odometry and loop closure detection, as it contains substantial lighting, weather changes, repeating structures, reflective and transparent glass buildings.\r\n\r\nSource: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)\nImage Source: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)", "price": 908, "created_at": "2025-01-09 13:44:40.875638", "keyword": ["Scene Understanding", "Camera Relocalization", "Visual Localization"], "id": "8e273d12-5caa-42a7-a099-ccbca221dd23", "image_url": "https://production-media.paperswithcode.com/datasets/DeepLoc-0000003515-74cb76cb.jpg"}, {"title": "DeepScores", "short_description": "DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. For advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding.\r\n\r\nSource: [DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects](/paper/deepscores-a-dataset-for-segmentation)\r\nImage Source: [https://tuggeluk.github.io/deepscores/](https://tuggeluk.github.io/deepscores/)", "price": 848, "created_at": "2025-01-09 13:44:40.875735", "keyword": ["Object Detection", "Scene Understanding", "Object Recognition"], "id": "7ed9f14a-b27f-46f4-b233-615b78bce60b", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_16.03.54.png"}, {"title": "DiDeMo", "short_description": "The **Distinct Describable Moments** (**DiDeMo**) dataset is one of the largest and most diverse datasets for the temporal localization of events in videos given natural language descriptions. The videos are collected from Flickr and each video is trimmed to a maximum of 30 seconds. The videos in the dataset are divided into 5-second segments to reduce the complexity of annotation. The dataset is split into training, validation and test sets containing 8,395, 1,065 and 1,004 videos respectively. The dataset contains a total of 26,892 moments and one moment could be associated with descriptions from multiple annotators. The descriptions in DiDeMo dataset are detailed and contain camera movement, temporal transition indicators, and activities. Moreover, the descriptions in DiDeMo are verified so that each description refers to a single moment.\r\n\r\nSource: [Weakly Supervised Video Moment Retrieval From Text Queries](https://arxiv.org/abs/1904.03282)\r\nImage Source: [https://www.di.ens.fr/~miech/datasetviz/](https://www.di.ens.fr/~miech/datasetviz/)", "price": 241, "created_at": "2025-01-09 13:44:40.875832", "keyword": ["Video Retrieval", "Zero-Shot Video Retrieval", "Natural Language Moment Retrieval"], "id": "274d4fe2-219a-4e2c-83c7-156ad24f046f", "image_url": "https://production-media.paperswithcode.com/datasets/DiDeMo-0000001110-b8e157c7_dXI7ukr.jpg"}, {"title": "Dialogue State Tracking Challenge", "short_description": "The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.\r\nIn these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.\r\n\r\nThe corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, caf\u00e9s etc. as well as multiple new slots. There were two rounds of evaluation using this data:\r\n\r\nDSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.\r\nDSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.\r\nDialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)\r\n\r\nSource: [https://github.com/matthen/dstc](https://github.com/matthen/dstc)\r\nImage Source: [https://www.aclweb.org/anthology/W13-4065.pdf](https://www.aclweb.org/anthology/W13-4065.pdf)", "price": 773, "created_at": "2025-01-09 13:44:40.875929", "keyword": ["Spoken Language Understanding", "Deblurring", "Dialogue State Tracking", "Spoken Dialogue Systems"], "id": "0e1b617e-cbe3-433f-a41c-4fe354c0c7a6", "image_url": "https://production-media.paperswithcode.com/datasets/Dialogue_State_Tracking_Challenge-0000003584-81326ddd.jpg"}, {"title": "DiscoFuse", "short_description": "DiscoFuse was created by applying a rule-based splitting method on two corpora -\r\nsports articles crawled from the Web, and Wikipedia. See the paper for a detailed\r\ndescription of the dataset generation process and evaluation.\r\n\r\nDiscoFuse has two parts with 44,177,443 and 16,642,323 examples sourced from Sports articles and Wikipedia, respectively.\r\n\r\nFor each part, a random split is provided to train (98% of the examples), development (1%) and test (1%) sets. In addition, as the original data distribution is highly skewed (see details in the paper), a balanced version for each part is also provided.\r\n\r\nSource: [Google Research](https://github.com/google-research-datasets/discofuse)", "price": 684, "created_at": "2025-01-09 13:44:40.876027", "keyword": ["Sentence Fusion"], "id": "f6a487a8-63b7-49ac-b86a-582d25907660", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_18.51.08.png"}, {"title": "Django", "short_description": "The **Django** dataset is a dataset for code generation comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created natural language description.\r\n\r\nSource: [Latent Predictor Networks for Code Generation](https://arxiv.org/abs/1603.06744)\r\nImage Source: [https://github.com/microsoft/vscode-docs/issues/2696](https://github.com/microsoft/vscode-docs/issues/2696)", "price": 780, "created_at": "2025-01-09 13:44:40.876125", "keyword": ["Code Generation"], "id": "310133e4-dc73-4d56-904e-04f8c99c1352", "image_url": "https://production-media.paperswithcode.com/datasets/Django-0000001059-57707917_hCcTMpx.jpg"}, {"title": "DocRED", "short_description": "**DocRED** (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. Along with the human-annotated data, the dataset provides large-scale distantly supervised data.\r\n\r\nDocRED contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. In addition to the human-annotated data, the dataset provides large-scale distantly supervised data over 101,873 documents.\r\n\r\nSource: [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation/)\r\nImage Source: [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation/)", "price": 923, "created_at": "2025-01-09 13:44:40.876222", "keyword": ["Relation Extraction", "Joint Entity and Relation Extraction", "Few-Shot Relation Classification"], "id": "16710f28-a07b-4aae-8c62-c0e2c8ab1fbc", "image_url": "https://production-media.paperswithcode.com/datasets/DocRED-0000002310-0743d975_JeWVto8.jpg"}, {"title": "DomainNet", "short_description": "**DomainNet** is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game \u201cQuick Draw!\u201d.\r\n\r\nSource: [What is being transferred in transfer learning?](https://arxiv.org/abs/2008.11687)\r\nImage Source: [http://ai.bu.edu/M3SDA/](http://ai.bu.edu/M3SDA/)", "price": 574, "created_at": "2025-01-09 13:44:40.876319", "keyword": ["Domain Adaptation", "Unsupervised Domain Adaptation", "Domain Generalization", "Multi-Source Unsupervised Domain Adaptation", "Partial Domain Adaptation", "Universal Domain Adaptation", "Multi-target Domain Adaptation", "Blended-target Domain Adaptation", "Unsupervised Continual Domain Shift Learning", "Zero-Shot Learning + Domain Generalization"], "id": "31677b29-ad3f-43fa-863a-34464d374e1d", "image_url": "https://production-media.paperswithcode.com/datasets/DomainNet-0000002698-42f300d8_nfcNmI7.jpg"}, {"title": "Douban", "short_description": "We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. \r\n\r\n|      |Train|Val| Test         | \r\n| ------------- |:-------------:|:-------------:|:-------------:|\r\n| session-response pairs  | 1m|50k| 10k |\r\n| Avg. positive response per session     | 1|1| 1.18    | \r\n| Fless Kappa | N\\A|N\\A|0.41      | \r\n| Min turn per session | 3|3| 3      | \r\n| Max ture per session | 98|91|45    | \r\n| Average turn per session | 6.69|6.75|5.95    | \r\n| Average Word per utterance | 18.56|18.50|20.74   | \r\n\r\n\r\nThe test data contains 1000 dialogue context, and for each context we create 10 responses as candidates. We recruited three labelers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the context. Each pair received three labels and the majority of the labels was taken as the final decision.\r\n\r\n<br>\r\nAs far as we known, this is the first human-labeled test set for retrieval-based chatbots. The entire corpus link https://www.dropbox.com/s/90t0qtji9ow20ca/DoubanConversaionCorpus.zip?dl=0", "price": 555, "created_at": "2025-01-09 13:44:40.876417", "keyword": ["Link Prediction", "Recommendation Systems", "Conversational Response Selection"], "id": "76b72da4-3da2-41dd-bfde-77c4609ea892", "image_url": ""}, {"title": "DuReader", "short_description": "**DuReader** is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations \u2013 each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.\r\n\r\nSource: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)\r\nImage Source: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)", "price": 107, "created_at": "2025-01-09 13:44:40.876514", "keyword": ["Reading Comprehension", "Open-Domain Question Answering", "Reading Comprehension (Zero-Shot)", "Reading Comprehension (One-Shot)", "Reading Comprehension (Few-Shot)", "Reading Comprehension", "Open-Domain Question Answering", "Reading Comprehension (Zero-Shot)", "Reading Comprehension (One-Shot)", "Reading Comprehension (Few-Shot)"], "id": "a57dc970-118c-408b-afe2-9fd5e49daad5", "image_url": "https://production-media.paperswithcode.com/datasets/DuReader-0000000174-bc2ca15c_yhUWfPp.jpg"}, {"title": "DukeMTMC-reID", "short_description": "The **DukeMTMC-reID** (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.\r\n\r\n**NOTE**: This dataset [has been retracted](https://exposing.ai/duke_mtmc/).\r\n\r\nSource: [Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification](https://arxiv.org/abs/1804.11027)", "price": 378, "created_at": "2025-01-09 13:44:40.876616", "keyword": ["Person Re-Identification", "Unsupervised Person Re-Identification", "Style Transfer", "Person Re-Identification", "Unsupervised Person Re-Identification", "Style Transfer"], "id": "1e96c743-5bd8-4a14-857c-48400ab39f98", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.03.57_PM.png"}, {"title": "E2E", "short_description": "End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena.\r\n\r\nSource: [Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge](/paper/evaluating-the-state-of-the-art-of-end-to-end)", "price": 962, "created_at": "2025-01-09 13:44:40.876713", "keyword": ["Text Generation", "Language Modelling", "Data-to-Text Generation", "Table-to-Text Generation"], "id": "a27fff47-59c4-4553-a853-350200672ed2", "image_url": ""}, {"title": "ECSSD", "short_description": "The **Extended Complex Scene Saliency Dataset** (**ECSSD**) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants.\r\n\r\nSource: [SAD: Saliency-based Defenses Against Adversarial Examples](https://arxiv.org/abs/2003.04820)", "price": 724, "created_at": "2025-01-09 13:44:40.876811", "keyword": ["RGB Salient Object Detection", "Saliency Detection", "Unsupervised Object Segmentation", "Salient Object Detection", "Unsupervised Saliency Detection", "RGB Salient Object Detection", "Saliency Detection", "Unsupervised Object Segmentation", "Salient Object Detection", "Unsupervised Saliency Detection"], "id": "be456057-9266-4047-8bd8-506fe27e1fb7", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_12.40.09_PM.png"}, {"title": "ELI5", "short_description": "ELI5 is a dataset for long-form question answering. It contains 270K complex, diverse questions that require explanatory multi-sentence answers. Web search results are used as evidence documents to answer each question.\r\n\r\nELI5 is also a task in Dodecadialogue.\r\n\r\nSource: [ELI5](https://facebookresearch.github.io/ELI5/)\r\nImage Source: [https://arxiv.org/pdf/1907.09190v1.pdf](https://arxiv.org/pdf/1907.09190v1.pdf)", "price": 338, "created_at": "2025-01-09 13:44:40.876920", "keyword": ["Text Generation", "Question Answering", "Language Modelling", "Sequence-to-sequence Language Modeling", "Open-Domain Question Answering", "Long Form Question Answering"], "id": "5cb51afd-463d-4b28-af5f-f0717378f4c0", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_13.00.38.png"}, {"title": "EMBER", "short_description": "A labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign).\r\n\r\nSource: [EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models](https://arxiv.org/pdf/1804.04637v2.pdf)", "price": 518, "created_at": "2025-01-09 13:44:40.877014", "keyword": ["Feature Engineering", "Malware Classification", "Malware Detection"], "id": "1a9c9fee-eeab-40f8-b0b1-541d575bc008", "image_url": ""}, {"title": "EMOTIC", "short_description": "The EMOTIC dataset, named after EMOTions In Context, is a database of images with people in real environments, annotated with their apparent emotions. The images are annotated with an extended list of 26 emotion categories combined with the three common continuous dimensions Valence, Arousal and Dominance.\r\n\r\nSource: [Context Based Emotion Recognition using EMOTIC Dataset](https://arxiv.org/abs/2003.13401)", "price": 532, "created_at": "2025-01-09 13:44:40.877107", "keyword": ["Action Recognition", "Emotion Recognition", "Multimodal Emotion Recognition", "Emotion Recognition in Context"], "id": "78d6c733-05e7-4c6a-803c-044fbe6d8980", "image_url": "https://production-media.paperswithcode.com/datasets/emotic_logo.png"}, {"title": "ENZYMES", "short_description": "**ENZYMES** is a dataset of 600 protein tertiary structures obtained from the BRENDA enzyme database. The ENZYMES dataset contains 6 enzymes.\r\n\r\nSource: [When Work Matters: Transforming Classical Network Structures to Graph CNN](https://arxiv.org/abs/1807.02653)", "price": 728, "created_at": "2025-01-09 13:44:40.877202", "keyword": ["Graph Classification"], "id": "b39db483-d6ef-433f-b36d-c4fdf2b1203d", "image_url": ""}, {"title": "ESC-50", "short_description": "The **ESC-50** dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.\r\n\r\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\r\nImage Source: [https://github.com/karolpiczak/ESC-50](https://github.com/karolpiczak/ESC-50)", "price": 747, "created_at": "2025-01-09 13:44:40.877296", "keyword": ["Audio Classification", "Few-Shot Audio Classification", "Environmental Sound Classification", "Data Augmentation", "Self-Supervised Audio Classification", "Zero-Shot Environment Sound Classification", "Environment Sound Classification"], "id": "bf0ea921-341f-416b-a1a1-880be6745453", "image_url": "https://production-media.paperswithcode.com/datasets/ESC-50-0000000260-d413a34e_wdcCSmV.gif"}, {"title": "ETH", "short_description": "**ETH** is a dataset for pedestrian detection. The testing set contains 1,804 images in three video clips. The dataset is captured from a stereo rig mounted on car, with a resolution of 640 x 480 (bayered), and a framerate of 13--14 FPS.\r\n\r\nSource: [Scale-aware Fast R-CNN for Pedestrian Detection](https://arxiv.org/abs/1510.08160)\r\nImage Source: [https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3](https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3)", "price": 101, "created_at": "2025-01-09 13:44:40.877390", "keyword": ["Object Detection", "Person Re-Identification", "Trajectory Prediction", "Pedestrian Detection", "Multi-future Trajectory Prediction", "Multi Future Trajectory Prediction", "Object Detection", "Person Re-Identification", "Trajectory Prediction", "Pedestrian Detection", "Multi-future Trajectory Prediction", "Multi Future Trajectory Prediction"], "id": "f7bbef35-f1df-4ba6-babc-6d3558492391", "image_url": "https://production-media.paperswithcode.com/datasets/ETH-0000003253-bb3afa0a.jpg"}, {"title": "EVALution", "short_description": "**EVALution** dataset is evenly distributed among the three classes (hypernyms, co-hyponyms and random) and involves three types of parts of speech (noun, verb, adjective). The full dataset contains a total of 4,263 distinct terms consisting of 2,380 nouns, 958 verbs and 972 adjectives.\r\n\r\nSource: [Network Features Based Co-hyponymy Detection](https://arxiv.org/abs/1802.04609)\r\nImage Source: [https://www.aclweb.org/anthology/W15-4208.pdf](https://www.aclweb.org/anthology/W15-4208.pdf)", "price": 151, "created_at": "2025-01-09 13:44:40.877484", "keyword": ["Semantic Textual Similarity", "Lexical Entailment", "Word Embeddings"], "id": "746a460f-34b1-4c37-a018-e8af147e4ecd", "image_url": "https://production-media.paperswithcode.com/datasets/EVALution-0000003559-6350defa.jpg"}, {"title": "EYEDIAP", "short_description": "The **EYEDIAP** dataset is a dataset for gaze estimation from remote RGB, and RGB-D (standard vision and depth), cameras. The recording methodology was designed by systematically including, and isolating, most of the variables which affect the remote gaze estimation algorithms:\r\n\r\n* Head pose variations.\r\n* Person variation.\r\n* Changes in ambient and sensing condition.\r\n* Types of target: screen or 3D object.\r\n\r\nSource: [https://www.idiap.ch/dataset/eyediap](https://www.idiap.ch/dataset/eyediap)\r\nImage Source: [https://www.idiap.ch/dataset/eyediap](https://www.idiap.ch/dataset/eyediap)", "price": 626, "created_at": "2025-01-09 13:44:40.877578", "keyword": ["Gaze Estimation"], "id": "ff68f419-fbd6-408d-a922-987085f541ad", "image_url": "https://production-media.paperswithcode.com/datasets/EYEDIAP-0000003292-d441fff8.jpg"}, {"title": "EgoGesture", "short_description": "The **EgoGesture** dataset contains 2,081 RGB-D videos, 24,161 gesture samples and 2,953,224 frames from 50 distinct subjects.\r\n\r\nSource: [http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html)\r\nImage Source: [http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html)", "price": 294, "created_at": "2025-01-09 13:44:40.877672", "keyword": ["Action Recognition", "Hand Gesture Recognition"], "id": "140df821-eb81-49f5-99a0-8ede8b8a4910", "image_url": "https://production-media.paperswithcode.com/datasets/EgoGesture-0000001345-933b6bdc_BgeOGd5.jpg"}, {"title": "EgoHands", "short_description": "The EgoHands dataset contains 48 Google Glass videos of complex, first-person interactions between two people. The main intention of this dataset is to enable better, data-driven approaches to understanding hands in first-person computer vision. The dataset offers\r\n\r\n* high quality, pixel-level segmentations of hands\r\n* the possibility to semantically distinguish between the observer\u2019s hands and someone else\u2019s hands, as well as left and right hands\r\n* virtually unconstrained hand poses as actors freely engage in a set of joint activities\r\n* lots of data with 15,053 ground-truth labeled hands\r\n \r\nSource: [Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions](/paper/lending-a-hand-detecting-hands-and)", "price": 780, "created_at": "2025-01-09 13:44:40.877765", "keyword": ["Object Detection", "Action Recognition", "Hand Segmentation"], "id": "a4bda0f0-5317-42aa-b261-c41e7649efdf", "image_url": "https://production-media.paperswithcode.com/datasets/egohands.jpg"}, {"title": "EmoryNLP", "short_description": "EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)\u2019s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.", "price": 949, "created_at": "2025-01-09 13:44:40.877914", "keyword": ["Emotion Recognition in Conversation"], "id": "9f0e7f26-d14d-4c0d-a466-c50ef00c8c37", "image_url": "https://production-media.paperswithcode.com/datasets/2-Table1-1.png"}, {"title": "EmotionLines", "short_description": "**EmotionLines** contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman\u2019s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.\r\n\r\nSource: [Bridging Dialogue Generation and Facial Expression Synthesis](https://arxiv.org/abs/1905.11240)\r\nImage Source: [https://arxiv.org/pdf/1802.08379.pdf](https://arxiv.org/pdf/1802.08379.pdf)", "price": 428, "created_at": "2025-01-09 13:44:40.878012", "keyword": ["Language Modelling", "Emotion Recognition", "Emotion Recognition in Conversation", "Emotion Classification"], "id": "e8c94747-1dcc-4227-83f2-a2afa751ecad", "image_url": "https://production-media.paperswithcode.com/datasets/EmotionLines-0000003572-33b525d1.jpg"}, {"title": "English Web Treebank", "short_description": "**English Web Treebank** is a dataset containing 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level tokenization, part-of-speech, and syntactic structure. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks in the DARPA GALE project. Only text from the subject line and message body of posts, articles, messages and question-answers were collected and annotated.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC2012T13](https://catalog.ldc.upenn.edu/LDC2012T13)", "price": 30, "created_at": "2025-01-09 13:44:40.878108", "keyword": ["Dependency Parsing", "Coreference Resolution", "Part-Of-Speech Tagging"], "id": "5efaf680-a3ed-4db7-b2b2-919bb09b515d", "image_url": ""}, {"title": "Epinions", "short_description": "The **Epinions** dataset is built form a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user.\nIt contains 75,879 nodes and 50,8837 edges.\n\nSource: [https://snap.stanford.edu/data/soc-Epinions1.html](https://snap.stanford.edu/data/soc-Epinions1.html)", "price": 576, "created_at": "2025-01-09 13:44:40.878209", "keyword": ["Recommendation Systems", "Link Sign Prediction"], "id": "d3d029e4-d222-4d36-b763-5dd5e316bb1b", "image_url": ""}, {"title": "EuroSAT", "short_description": "**Eurosat** is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images.\r\n\r\nSource: [EuroSAT](https://github.com/phelber/eurosat)\r\nImage Source: [EuroSAT](https://github.com/phelber/eurosat)", "price": 705, "created_at": "2025-01-09 13:44:40.878307", "keyword": ["Image Classification", "Semantic Segmentation", "Few-Shot Learning", "Prompt Engineering", "Cross-Domain Few-Shot"], "id": "07dd14f2-4a0c-475e-976d-f318987a9aa8", "image_url": "https://production-media.paperswithcode.com/datasets/EUROSAT.jpg"}, {"title": "Event2Mind", "short_description": "Event2Mind is a corpus of 25,000 event phrases covering a diverse range of everyday events and situations.\r\n\r\nSource: [Event2Mind: Commonsense Inference on Events, Intents, and Reactions](/paper/event2mind-commonsense-inference-on-events)", "price": 955, "created_at": "2025-01-09 13:44:40.878411", "keyword": ["Common Sense Reasoning"], "id": "032aa990-8a26-410a-b6c5-45f79120302e", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_7.01.38_PM.png"}, {"title": "FB15k", "short_description": "The **FB15k** dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set.\r\n\r\nSource: [https://www.microsoft.com/en-us/download/details.aspx?id=52312](https://www.microsoft.com/en-us/download/details.aspx?id=52312)\r\nImage Source: [http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf](http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf)", "price": 477, "created_at": "2025-01-09 13:44:40.878510", "keyword": ["Link Prediction", "Knowledge Graph Completion", "Knowledge Graphs", "Complex Query Answering", "Knowledge Graph Embedding", "Link Prediction", "Knowledge Graph Completion", "Knowledge Graphs", "Complex Query Answering", "Knowledge Graph Embedding"], "id": "d560304c-9171-4616-882e-9aa900027c7f", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.26.00_PM.png"}, {"title": "FB15k-237", "short_description": "**FB15k-237** is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,116 triples with 14,541 entities and 237 relation types.", "price": 705, "created_at": "2025-01-09 13:44:40.878609", "keyword": ["Link Prediction", "Knowledge Graph Completion", "Complex Query Answering"], "id": "72254f1c-5509-4c27-8be6-a86c78ea01b5", "image_url": ""}, {"title": "FBMS", "short_description": "The **Freiburg-Berkeley Motion Segmentation** Dataset (**FBMS**-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set.\r\n\r\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/](https://lmb.informatik.uni-freiburg.de/resources/datasets/)\r\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/](https://lmb.informatik.uni-freiburg.de/resources/datasets/)", "price": 840, "created_at": "2025-01-09 13:44:40.878706", "keyword": ["Video Object Segmentation", "Video Salient Object Detection", "Unsupervised Object Segmentation", "Unsupervised Video Object Segmentation"], "id": "40ed830f-f475-4725-85ed-79817e386094", "image_url": "https://production-media.paperswithcode.com/datasets/FBMS-0000001974-ad2b8bf8_0c8m2Vo.jpg"}, {"title": "FC100", "short_description": "The **FC100** dataset (**Fewshot-CIFAR100**) is a newly split dataset based on CIFAR-100 for few-shot learning. It contains 20 high-level categories which are divided into 12, 4, 4 categories for training, validation and test. There are 60, 20, 20 low-level classes in the corresponding split containing 600 images of size 32 \u00d7 32 per class. Smaller image size makes it more challenging for few-shot learning.\r\n\r\nSource: [Prototype Rectification for Few-Shot Learning](https://arxiv.org/abs/1911.10713)", "price": 32, "created_at": "2025-01-09 13:44:40.878805", "keyword": ["Few-Shot Image Classification"], "id": "df8a46c9-613d-4c06-bb44-e462bd4ee88a", "image_url": "https://production-media.paperswithcode.com/datasets/cifar-100.jpg"}, {"title": "FCE", "short_description": "The Cambridge Learner Corpus **First Certificate in English** (CLC **FCE**) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively.\r\n\r\nSource: [Compositional Sequence Labeling Models for Error Detection in Learner Writing](https://arxiv.org/abs/1607.06153)", "price": 436, "created_at": "2025-01-09 13:44:40.878913", "keyword": ["Grammatical Error Detection"], "id": "ddd554ef-f140-4c8b-9ed3-055cad5be1e3", "image_url": ""}, {"title": "FDDB", "short_description": "The **Face Detection Dataset and Benchmark** (**FDDB**) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.\r\n\r\nSource: [A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications](https://arxiv.org/abs/1809.03336)", "price": 68, "created_at": "2025-01-09 13:44:40.879006", "keyword": ["Face Detection", "Face Detection"], "id": "6ee31684-3267-4f5b-a950-53c7016b1847", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_10.58.22_AM.png"}, {"title": "FER+", "short_description": "The **FER+** dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.\r\n\r\nSource: [https://github.com/Microsoft/FERPlus](https://github.com/Microsoft/FERPlus)\r\nImage Source: [https://github.com/Microsoft/FERPlus](https://github.com/Microsoft/FERPlus)", "price": 358, "created_at": "2025-01-09 13:44:40.879100", "keyword": ["Facial Expression Recognition (FER)", "Facial Expression Recognition"], "id": "dbda5317-4405-4a6f-afba-ebd300f32b48", "image_url": "https://production-media.paperswithcode.com/datasets/FER-0000001485-2b7e676f_4Vi1lAu.jpg"}, {"title": "FER2013", "short_description": "Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48\u00d748, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images \u2013 600, while other labels have nearly 5,000 samples each.\r\n\r\nSource: [Eavesdrop the Composition Proportion of Training Labels in Federated Learning](https://arxiv.org/abs/1910.06044)\r\nImage Source: [https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280](https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280)", "price": 985, "created_at": "2025-01-09 13:44:40.879199", "keyword": ["Emotion Recognition", "Facial Expression Recognition (FER)", "Image Compression"], "id": "792f5a5b-cc85-4556-bfaa-15853fea1caf", "image_url": "https://production-media.paperswithcode.com/datasets/FER2013-0000001434-01251bb8_415HDzL.jpg"}, {"title": "FERG", "short_description": "**FERG** is a database of cartoon characters with annotated facial expressions containing 55,769 annotated face images of six characters. The images for each character are grouped into 7 types of cardinal expressions, viz. anger, disgust, fear, joy, neutral, sadness and surprise.\n\nSource: [VGAN-Based Image Representation Learningfor Privacy-Preserving Facial Expression Recognition](https://arxiv.org/abs/1803.07100)\nImage Source: [http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html](http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html)", "price": 832, "created_at": "2025-01-09 13:44:40.879292", "keyword": ["Facial Expression Recognition (FER)"], "id": "93f2b4ba-d92d-48fb-a81c-fccaf3966ef0", "image_url": "https://production-media.paperswithcode.com/datasets/FERG-0000002220-049b651f_xoo2v7B.jpeg"}, {"title": "FEVER", "short_description": "FEVER is a publicly available dataset for fact extraction and verification against textual sources.\r\n\r\nIt consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.\r\n\r\nThe claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was\r\nextracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.\r\n\r\nSource: [FEVER: a large-scale dataset for Fact Extraction and VERification](https://arxiv.org/pdf/1803.05355v3.pdf)", "price": 963, "created_at": "2025-01-09 13:44:40.879386", "keyword": ["Zero-shot Text Search", "Fact Verification"], "id": "ede882a9-8bcd-4986-9ba8-6e2743bfac0c", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-08_at_17.34.31.png"}, {"title": "FFHQ", "short_description": "**Flickr-Faces-HQ (FFHQ)** consists of 70,000 high-quality PNG images at 1024\u00d71024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.\r\n\r\nSource: [Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset)\r\nImage Source: [https://github.com/NVlabs/ffhq-dataset](https://github.com/NVlabs/ffhq-dataset)", "price": 579, "created_at": "2025-01-09 13:44:40.879485", "keyword": ["Image Generation", "Image Super-Resolution", "Image Denoising", "Image Inpainting", "Facial Inpainting", "3D-Aware Image Synthesis", "Face Hallucination"], "id": "b530c673-c8bd-42f2-a16b-9a80fb9d9f6a", "image_url": "https://production-media.paperswithcode.com/datasets/FFHQ-0000000590-3f6c5915_WsoqczT.jpg"}, {"title": "FG-NET", "short_description": "FGNet is a dataset for age estimation and face recognition across ages. It is composed of a total of 1,002 images of 82 people with age range from 0 to 69 and an age gap up to 45 years\r\n\r\nSource: [Large age-gap face verification by feature injection in deep networks](https://arxiv.org/abs/1602.06149)\r\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621](https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621)", "price": 163, "created_at": "2025-01-09 13:44:40.879580", "keyword": ["Age Estimation", "Age-Invariant Face Recognition", "Age Estimation", "Age-Invariant Face Recognition"], "id": "dc594f51-4dd4-4b1a-870b-f288414c7201", "image_url": "https://production-media.paperswithcode.com/datasets/FG-NET-0000000529-30cca975_l1OKitP.jpg"}, {"title": "FGVC-Aircraft", "short_description": "FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label.\r\nAircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:\r\n\r\n* Model, e.g. Boeing 737-76J. Since certain models are nearly visually indistinguishable, this level is not used in the evaluation.\r\n* Variant, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 102 different variants.\r\n* Family, e.g. Boeing 737. The dataset comprises 70 different families.\r\n* Manufacturer, e.g. Boeing. The dataset comprises 41 different manufacturers.\r\nThe data is divided into three equally-sized training, validation and test subsets.\r\n\r\nSource: [https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/](https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/)\r\nImage Source: [Fine-Grained Visual Classification of Aircraft](https://arxiv.org/abs/1306.5151)", "price": 609, "created_at": "2025-01-09 13:44:40.879674", "keyword": ["Image Classification", "Few-Shot Learning", "Fine-Grained Image Classification", "Neural Architecture Search", "Prompt Engineering"], "id": "2421804c-b92d-4175-b51c-5fb57b85130d", "image_url": "https://production-media.paperswithcode.com/datasets/FGVC-Aircraft-0000003405-c35d29b7.jpg"}, {"title": "FLIC", "short_description": "The **FLIC** dataset contains 5003 images from popular Hollywood movies. The images were obtained by running a state-of-the-art person detector on every tenth frame of 30 movies. People detected with high confidence (roughly 20K candidates) were then sent to the crowdsourcing marketplace Amazon Mechanical Turk to obtain ground truth labelling. Each image was annotated by five Turkers to label 10 upper body joints. The median-of-five labelling was taken in each image to be robust to outlier annotation. Finally, images were rejected manually by if the person was occluded or severely non-frontal.\r\n\r\nSource: [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)\r\nImage Source: [https://www.tensorflow.org/datasets/catalog/flic](https://www.tensorflow.org/datasets/catalog/flic)", "price": 94, "created_at": "2025-01-09 13:44:40.879767", "keyword": ["Pose Estimation", "Pose Estimation"], "id": "ac0122ec-3043-48e0-b44f-f3a511292a04", "image_url": "https://production-media.paperswithcode.com/datasets/FLIC-0000003289-0acb57f0.jpg"}, {"title": "FMA", "short_description": "The **Free Music Archive** (**FMA**) is a large-scale dataset for evaluating several tasks in Music Information Retrieval. It consists of 343 days of audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.\r\n\r\nThere are four subsets defined by the authors:\r\n\r\n* Full: the complete dataset,\r\n* Large: the full dataset with audio limited to 30 seconds clips extracted from the middle of the tracks (or entire track if shorter than 30 seconds),\r\n* Medium: a selection of 25,000 30s clips having a single root genre,\r\n* Small: a balanced subset containing 8,000 30s clips with 1,000 clips per one of 8 root genres.\r\n\r\nThe official split into training, validation and test sets (80/10/10) uses stratified sampling to preserve the percentage of tracks per genre. Songs of the same artists are part of one set only.\r\n\r\nSource: [FMA: A Dataset For Music Analysis](https://arxiv.org/pdf/1612.01840.pdf)\r\nAudio Source: [https://github.com/mdeff/fma](https://github.com/mdeff/fma)", "price": 881, "created_at": "2025-01-09 13:44:40.879861", "keyword": ["Information Retrieval", "Genre classification", "Cadenza 1 - Task 2 - In Car", "Music Information Retrieval"], "id": "4bf962cf-e3f5-4249-b72e-3b076d1fecde", "image_url": ""}, {"title": "FRGC", "short_description": "The data for **FRGC** consists of 50,000 recordings divided into training and validation partitions. The training partition is designed for training algorithms and the validation partition is for assessing performance of an approach in a laboratory setting. The validation partition consists of data from 4,003 subject sessions. A subject session is the set of all images of a person taken each time a person's biometric data is collected and consists of four controlled still images, two uncontrolled still images, and one three-dimensional image. The controlled images were taken in a studio setting, are full frontal facial images taken under two lighting conditions and with two facial expressions (smiling and neutral). The uncontrolled images were taken in varying illumination conditions; e.g., hallways, atriums, or outside. Each set of uncontrolled images contains two expressions, smiling and neutral. The 3D image was taken under controlled illumination conditions. The 3D images consist of both a range and a texture image. The 3D images were acquired by a Minolta Vivid 900/910 series sensor.\r\n\r\nSource: [https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc](https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc)\r\nImage Source: [https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105](https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105)", "price": 305, "created_at": "2025-01-09 13:44:40.879954", "keyword": ["Image Clustering"], "id": "1985e127-2150-4cf3-9a5f-cda39b4ead8b", "image_url": "https://production-media.paperswithcode.com/datasets/FRGC-0000002121-7744ed87_TyV8Nbv.jpg"}, {"title": "FSS-1000", "short_description": "**FSS-1000** is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.\n\nSource: [https://github.com/HKUSTCV/FSS-1000](https://github.com/HKUSTCV/FSS-1000)\nImage Source: [https://github.com/HKUSTCV/FSS-1000](https://github.com/HKUSTCV/FSS-1000)", "price": 186, "created_at": "2025-01-09 13:44:40.880048", "keyword": ["Few-Shot Semantic Segmentation"], "id": "be853541-14a0-4b5a-81a8-63ae36c4b91d", "image_url": "https://production-media.paperswithcode.com/datasets/FSS-1000-0000001417-de465772.jpg"}, {"title": "FaceForensics", "short_description": "FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video forgeries. All videos are downloaded from Youtube and are cut down to short continuous clips that contain mostly frontal faces. This dataset has two versions:\r\n\r\n* Source-to-Target: where the authors reenact over 1000 videos with new facial expressions extracted from other videos, which e.g. can be used to train a classifier to detect fake images or videos.\r\n\r\n* Selfreenactment: where the authors use Face2Face to reenact the facial expressions of videos with their own facial expressions as input to get pairs of videos, which e.g. can be used to train supervised generative refinement models.", "price": 587, "created_at": "2025-01-09 13:44:40.880142", "keyword": ["DeepFake Detection"], "id": "e2cf01bc-b64b-4577-bd35-864e5d630455", "image_url": "https://production-media.paperswithcode.com/datasets/header.jpg"}, {"title": "FaceWarehouse", "short_description": "**FaceWarehouse** is a 3D facial expression database that provides the facial geometry of 150 subjects, covering a wide range of ages and ethnic backgrounds.\r\n\r\nSource: [3D Face Reconstruction with Geometry Details from a Single Image](https://arxiv.org/abs/1702.05619)", "price": 636, "created_at": "2025-01-09 13:44:40.880236", "keyword": ["3D Face Reconstruction", "Face Reconstruction", "Face Model"], "id": "bbee25e3-8a11-40c5-a4e2-f465e99a1cca", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.45.25_PM.png"}, {"title": "Fashion-MNIST", "short_description": "**Fashion-MNIST** is a dataset comprising of 28\u00d728 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.\r\n\r\nSource: [Generative Probabilistic Novelty Detection with Adversarial Autoencoders](https://arxiv.org/abs/1807.02588)\r\nImage Source: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)", "price": 103, "created_at": "2025-01-09 13:44:40.880332", "keyword": ["Image Classification", "Image Generation", "Anomaly Detection", "Out-of-Distribution Detection", "Image Clustering", "Domain Generalization", "Unsupervised Anomaly Detection", "Clustering Algorithms Evaluation", "Outlier Detection", "General Classification", "Multiview Clustering", "Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly", "Model Poisoning"], "id": "f6f5b2ca-8416-4f17-a5d0-75c3db746f93", "image_url": "https://production-media.paperswithcode.com/datasets/Fashion-MNIST-0000000040-4a13281a_m8bp4wm.jpg"}, {"title": "FewRel", "short_description": "The **FewRel** (**Few-Shot Relation Classification Dataset**) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations).\r\n\r\nSource: [Neural Snowball for Few-Shot Relation Learning](https://arxiv.org/abs/1908.11007)\r\nImage Source: [https://www.aclweb.org/anthology/D18-1514.pdf](https://www.aclweb.org/anthology/D18-1514.pdf)", "price": 427, "created_at": "2025-01-09 13:44:40.880430", "keyword": ["Relation Extraction", "Relation Classification", "Few-Shot Relation Classification", "Zero-shot Relation Triplet Extraction", "Zero-shot Relation Classification"], "id": "0703dc63-61bd-4c48-a334-6552bece45c8", "image_url": "https://production-media.paperswithcode.com/datasets/FewRel-0000000111-ce374e23_gqpgF4W.jpg"}, {"title": "FigureQA", "short_description": "FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. \r\n\r\nSource: [FigureQA: An Annotated Figure Dataset for Visual Reasoning](/paper/figureqa-an-annotated-figure-dataset-for)", "price": 462, "created_at": "2025-01-09 13:44:40.880525", "keyword": ["Question Answering", "Visual Question Answering (VQA)", "Visual Reasoning", "Chart Question Answering"], "id": "71944edb-0d5a-4840-807d-0364023d0ee6", "image_url": ""}, {"title": "Flickr-8k", "short_description": "Contains 8k flickr Images with captions. Visit [this](http://hockenmaier.cs.illinois.edu/8k-pictures.html) page to explore the data. \r\n\r\nCite this paper if you find it useful in your research: [Framing image description as a ranking task: data, models and evaluation metrics](http://hockenmaier.cs.illinois.edu/Framing_Image_Description/KCCA.html)", "price": 616, "created_at": "2025-01-09 13:44:40.880618", "keyword": ["Image Captioning", "Cross-Modal Retrieval"], "id": "081a0358-ec33-4165-8cbf-c180f6f6ee43", "image_url": ""}, {"title": "Flickr30k", "short_description": "The **Flickr30k** dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.\r\n\r\nSource: [Guiding Long-Short Term Memory for Image Caption Generation](https://arxiv.org/abs/1509.04942)\r\n\r\nImage Source: [Dual-Path Convolutional Image-Text Embedding with Instance Loss\r\n](https://arxiv.org/abs/1711.05535)", "price": 588, "created_at": "2025-01-09 13:44:40.880712", "keyword": ["Node Classification", "Image Retrieval", "Image Captioning", "Cross-Modal Retrieval", "Image-to-Text Retrieval", "Phrase Grounding", "Semi Supervised Learning for Image Captioning", "Zero-Shot Cross-Modal Retrieval", "Video Description", "Node Classification", "Image Retrieval", "Image Captioning", "Cross-Modal Retrieval", "Image-to-Text Retrieval", "Phrase Grounding", "Semi Supervised Learning for Image Captioning", "Zero-Shot Cross-Modal Retrieval", "Video Description"], "id": "d51437c6-1778-4f9d-9468-1cde01d2b652", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.11.08_PM.png"}, {"title": "Florence", "short_description": "The **Florence** 3D faces dataset consists of:\r\n\r\n* High-resolution 3D scans of human faces from many subjects.\r\n* Several video sequences of varying resolution, conditions and zoom level for each subject.\r\nEach subject is recorded in the following situations:\r\n* In a controlled setting in HD video.\r\n* In a less-constrained (but still indoor) setting using a standard, PTZ surveillance camera.\r\n* In an unconstrained, outdoor environment under challenging recording conditions.\r\n\r\nSource: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)\r\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)", "price": 678, "created_at": "2025-01-09 13:44:40.880806", "keyword": ["3D Face Reconstruction", "3D Face Reconstruction"], "id": "25331d89-43a4-44cf-9428-9594d686c10a", "image_url": "https://production-media.paperswithcode.com/datasets/Florence-0000000547-22c99005_mteKpWT.jpg"}, {"title": "Florentine", "short_description": "The **Florentine** dataset is a dataset of facial gestures which contains facial clips from 160 subjects (both male and female), where gestures were artificially generated according to a specific request, or genuinely given due to a shown stimulus. 1032 clips were captured for posed expressions and 1745 clips for induced facial expressions amounting to a total of 2777 video clips. Genuine facial expressions were induced in subjects using visual stimuli, i.e. videos selected randomly from a bank of Youtube videos to generate a specific emotion.\n\nSource: [Deep video gesture recognition using illumination invariants](https://arxiv.org/abs/1603.06531)\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)", "price": 520, "created_at": "2025-01-09 13:44:40.880899", "keyword": ["Gesture Recognition"], "id": "de0c0821-0629-4045-ac54-6a660c30e104", "image_url": "https://production-media.paperswithcode.com/datasets/Florentine-0000003272-9d3607c9.jpg"}, {"title": "FlyingChairs", "short_description": "The \"Flying Chairs\" are a synthetic dataset with optical flow ground truth. It consists of 22872 image pairs and corresponding flow fields. Images show renderings of 3D chair models moving in front of random backgrounds from Flickr. Motions of both the chairs and the background are purely planar.", "price": 118, "created_at": "2025-01-09 13:44:40.880994", "keyword": [], "id": "6d43bbb5-32e1-41fc-b3aa-a7403059893c", "image_url": ""}, {"title": "FlyingThings3D", "short_description": "**FlyingThings3D** is a synthetic dataset for optical flow, disparity and  scene flow estimation. It consists of everyday objects flying along randomized 3D trajectories. We generated about 25,000 stereo frames with ground truth data. Instead of focusing on a particular task (like KITTI) or enforcing strict naturalism (like Sintel), we rely on randomness and a large pool of rendering assets to generate orders of magnitude more data than any existing option, without running a risk of repetition or saturation.", "price": 990, "created_at": "2025-01-09 13:44:40.881088", "keyword": ["Optical Flow Estimation", "Disparity Estimation", "Scene Flow Estimation"], "id": "043353ca-0816-4779-b6be-d5ff2f469ab0", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-05-18_at_22.56.03.jpg"}, {"title": "Food-101", "short_description": "The  **Food-101** dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.\r\n\r\nSource: [Combining Weakly and Webly Supervised Learning for Classifying Food Images](https://arxiv.org/abs/1712.08730)\r\nImage Source: [https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)", "price": 972, "created_at": "2025-01-09 13:44:40.881182", "keyword": ["Image Classification", "Few-Shot Learning", "Fine-Grained Image Classification", "Neural Architecture Search", "Document Text Classification", "Prompt Engineering", "Zero-Shot Transfer Image Classification", "Multimodal Text and Image Classification", "Image Compression", "Multi-Modal Document Classification"], "id": "489a5ea4-b72c-46a5-b75a-99b78532bc02", "image_url": "https://production-media.paperswithcode.com/datasets/Food-101-0000000037-8c457091_ZXHhL3x.jpg"}, {"title": "Foursquare", "short_description": "The **Foursquare** dataset consists of check-in data for different cities. One subset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories).\nAnother subset contains long-term (about 18 months from April 2012 to September 2013) global-scale check-in data collected from Foursquare. It contains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415 cities in 77 countries). Those 415 cities are the most checked 415 cities by Foursquare users in the world, each of which contains at least 10K check-ins.\n\nSource: [https://sites.google.com/site/yangdingqi/home/foursquare-dataset](https://sites.google.com/site/yangdingqi/home/foursquare-dataset)", "price": 519, "created_at": "2025-01-09 13:44:40.881275", "keyword": ["Recommendation Systems", "Crime Prediction"], "id": "fb414b17-f92f-4b3b-ba44-ffd82044fb62", "image_url": ""}, {"title": "FreiHAND", "short_description": "**FreiHAND** is a 3D hand pose dataset which records different hand actions performed by 32 people. For each hand image, MANO-based 3D hand pose annotations are provided. It currently contains 32,560 unique training samples and 3960 unique samples for evaluation. The training samples are recorded with a green screen background allowing for background removal. In addition, it applies three different post processing strategies to training samples for data augmentation. However, these post processing strategies are not applied to evaluation samples.\r\n\r\nSource: [Knowledge as Priors: Cross-Modal Knowledge Generalizationfor Datasets without Superior Knowledge](https://arxiv.org/abs/2004.00176)\r\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html)", "price": 756, "created_at": "2025-01-09 13:44:40.881369", "keyword": ["3D Hand Pose Estimation"], "id": "538680c6-2935-41dc-bee9-97346127e9ec", "image_url": "https://production-media.paperswithcode.com/datasets/FreiHAND-0000002697-eec330f1_soCHIrC.jpg"}, {"title": "G3D", "short_description": "The Gaming 3D Dataset (**G3D**) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: \u201cpunch right\u201d, \u201cpunch left\u201d, \u201ckick right\u201d, \u201ckick left\u201d, \u201cdefend\u201d, \u201cgolf swing\u201d, \u201ctennis swing forehand\u201d, \u201ctennis swing backhand\u201d, \u201ctennis serve\u201d, \u201cthrow bowling ball\u201d, \u201caim and fire gun\u201d, \u201cwalk\u201d, \u201crun\u201d, \u201cjump\u201d, \u201cclimb\u201d, \u201ccrouch\u201d, \u201csteer a car\u201d, \u201cwave\u201d, \u201cflap\u201d and \u201cclap\u201d.\r\n\r\nSource: [Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN](https://arxiv.org/abs/1704.05645)\r\nImage Source: [G3D: A gaming action dataset and real time action recognition evaluation framework](https://doi.org/10.1109/CVPRW.2012.6239175)", "price": 100, "created_at": "2025-01-09 13:44:40.881463", "keyword": ["Skeleton Based Action Recognition", "Pose Prediction"], "id": "9668f687-bb10-4205-84a3-70355e4f7dd9", "image_url": "https://production-media.paperswithcode.com/datasets/G3D-0000003293-42543634.jpg"}, {"title": "GENIA", "short_description": "The **GENIA** corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.\r\n\r\nThe corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms \u201chuman\u201d, \u201cblood cells\u201d, and \u201ctranscription factors\u201d. The corpus has been annotated with various levels of linguistic and semantic information.\r\n\r\nThe primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:\r\n\r\n* Part-of-Speech annotation\r\n* Constituency (phrase structure) syntactic annotation\r\n* Term annotation\r\n* Event annotation\r\n* Relation annotation\r\n* Coreference annotation\r\n\r\nSource: [http://www.geniaproject.org/genia-corpus](http://www.geniaproject.org/genia-corpus)\r\nImage Source: [http://www.geniaproject.org/genia-corpus](http://www.geniaproject.org/genia-corpus)", "price": 143, "created_at": "2025-01-09 13:44:40.881556", "keyword": ["Named Entity Recognition (NER)", "Dependency Parsing", "Named Entity Recognition", "Event Extraction", "Nested Named Entity Recognition"], "id": "273306fb-0166-4d5a-ad94-f9c4317ab92b", "image_url": "https://production-media.paperswithcode.com/datasets/GENIA-0000000756-1e9eacb8_cjAPotB.gif"}, {"title": "GLUE", "short_description": "General Language Understanding Evaluation (**GLUE**) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.\r\n\r\nSource: [Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models](https://arxiv.org/abs/1908.06725)\r\nImage Source: [https://gluebenchmark.com/](https://gluebenchmark.com/)", "price": 553, "created_at": "2025-01-09 13:44:40.881655", "keyword": ["Text Generation", "Text Classification", "Few-Shot Learning", "Natural Language Inference", "Semantic Textual Similarity", "Stochastic Optimization", "Natural Language Understanding", "Linguistic Acceptability", "Model Compression", "Natural Language Inference (Zero-Shot)", "Semantic Textual Similarity within Bi-Encoder", "Headline style transfer (Il Giornale to Repubblica)", "Headline style transfer (Repubblica to Il Giornale)", "CoLA", "SST-2", "MRPC", "STS-B", "QQP", "MNLI-m", "MNLI-mm", "QNLI", "RTE", "WNLI", "Text Generation", "Text Classification", "Few-Shot Learning", "Natural Language Inference", "Semantic Textual Similarity", "Stochastic Optimization", "Natural Language Understanding", "Linguistic Acceptability", "Model Compression", "Natural Language Inference (Zero-Shot)", "Semantic Textual Similarity within Bi-Encoder", "Headline style transfer (Il Giornale to Repubblica)", "Headline style transfer (Repubblica to Il Giornale)", "CoLA", "SST-2", "MRPC", "STS-B", "QQP", "MNLI-m", "MNLI-mm", "QNLI", "RTE", "WNLI"], "id": "523138a2-f628-4b1f-a4c9-33971a7d9df7", "image_url": "https://production-media.paperswithcode.com/datasets/d4ec20f1-a6a0-44ff-8940-32a0bcc4710a.png"}, {"title": "GOT-10k", "short_description": "The **GOT-10k** dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.\r\n\r\nSource: [http://got-10k.aitestunion.com/](http://got-10k.aitestunion.com/)\r\nImage Source: [https://arxiv.org/pdf/1810.11981.pdf](https://arxiv.org/pdf/1810.11981.pdf)", "price": 54, "created_at": "2025-01-09 13:44:40.881750", "keyword": ["Visual Object Tracking", "Video Object Tracking"], "id": "e5b6d89e-fc1c-4224-81c7-008a2c5e5c00", "image_url": "https://production-media.paperswithcode.com/datasets/got10k.jpg"}, {"title": "GRID Dataset", "short_description": "The QMUL underGround Re-IDentification (**GRID**) dataset contains 250 pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views. All images are captured from 8 disjoint camera views installed in a busy underground station. The figures beside show a snapshot of each of the camera views of the station and sample images in the dataset. The dataset is challenging due to variations of pose, colours, lighting changes; as well as poor image quality caused by low spatial resolution.\r\n\r\nSource: [https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html](https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html)", "price": 882, "created_at": "2025-01-09 13:44:40.881845", "keyword": ["Speech Separation", "Speech Enhancement", "Lipreading", "Speaker-Specific Lip to Speech Synthesis", "Lip Reading"], "id": "c6b5e127-cff8-4ac2-b71e-45e44e7b27bf", "image_url": "https://production-media.paperswithcode.com/datasets/GRID-0000003402-e3bd50c7.jpg"}, {"title": "GTA5", "short_description": "The **GTA5** dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game **Grand Theft Auto 5** and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.\r\n\r\nSource: [Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/1909.00781)\r\nImage Source: [Richter et al](https://arxiv.org/pdf/1608.02192v1.pdf)", "price": 620, "created_at": "2025-01-09 13:44:40.881938", "keyword": ["Semantic Segmentation", "Domain Adaptation", "Image-to-Image Translation", "Unsupervised Domain Adaptation", "Synthetic-to-Real Translation", "One-shot Unsupervised Domain Adaptation", "Semantic Segmentation", "Domain Adaptation", "Image-to-Image Translation", "Unsupervised Domain Adaptation", "Synthetic-to-Real Translation", "One-shot Unsupervised Domain Adaptation"], "id": "a6444685-ca60-47ba-bbd0-d7706d87f2d2", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_12.42.27_PM.png"}, {"title": "GTEA", "short_description": "The Georgia Tech Egocentric Activities (**GTEA**) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute.\r\n\r\nSource: [TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation](https://arxiv.org/abs/1705.07818)\r\nImage Source: [http://cbs.ic.gatech.edu/fpv/](http://cbs.ic.gatech.edu/fpv/)", "price": 485, "created_at": "2025-01-09 13:44:40.882032", "keyword": ["Action Segmentation", "Weakly Supervised Action Localization", "Fine-Grained Action Detection"], "id": "a0dbd893-eb43-43a5-a8bb-4846c0dd42dd", "image_url": "https://production-media.paperswithcode.com/datasets/GTEA-0000000385-8332f75f_Ag8P8Qi.jpg"}, {"title": "GTSRB", "short_description": "The **German Traffic Sign Recognition Benchmark** (**GTSRB**) contains 43 classes of traffic signs, split into 39,209 training images and 12,630 test images. The images have varying light conditions and rich backgrounds.\r\n\r\nSource: [Invisible Backdoor Attacks Against Deep Neural Networks](https://arxiv.org/abs/1909.02742)\r\nImage Source: [https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388](https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388)", "price": 64, "created_at": "2025-01-09 13:44:40.882125", "keyword": ["Domain Adaptation", "Traffic Sign Recognition"], "id": "6ef27bbd-4490-4e47-b74d-09de31bcced9", "image_url": "https://production-media.paperswithcode.com/datasets/GTSRB-0000000633-9ce3c5f6_Dki5Rsf.jpg"}, {"title": "GYAFC", "short_description": "Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.\r\n\r\nYahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. \r\n\r\nThe Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly\r\nacross different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus\r\nwhich consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.\r\n\r\nSource: [Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer](https://arxiv.org/pdf/1803.06535v2.pdf)", "price": 922, "created_at": "2025-01-09 13:44:40.882220", "keyword": ["Unsupervised Text Style Transfer", "Style Transfer", "Formality Style Transfer"], "id": "82500b1e-9694-4d61-b785-4973e2a921ed", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_19.22.51.png"}, {"title": "General-100", "short_description": "The **General-100** dataset is a dataset for image super-resolution. It contains 100 bmp format images with no compression) The size of the 100 images ranges from 710 x 704 (large) to 131 x 112 (small).", "price": 186, "created_at": "2025-01-09 13:44:40.882314", "keyword": ["Image Super-Resolution", "Image Reconstruction"], "id": "4c2cae65-6a8a-4000-84b7-939c3aab227e", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_4.28.24_PM.png"}, {"title": "GoPro", "short_description": "The **GoPro** dataset for deblurring consists of 3,214 blurred images with the size of 1,280\u00d7720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth shapr image that are obtained by a high-speed camera.\r\n\r\nSource: [Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networksfor Non-Uniform Single Image Deblurring](https://arxiv.org/abs/1903.10157)\r\nImage Source: [Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring](https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf)", "price": 568, "created_at": "2025-01-09 13:44:40.882408", "keyword": ["Video Frame Interpolation", "Deblurring", "Image Deblurring"], "id": "178481ed-4da0-472d-8113-8fc6082c6b06", "image_url": "https://production-media.paperswithcode.com/datasets/GoPro-0000002577-bfd1865a_hxxgksi.jpg"}, {"title": "Gowalla", "short_description": "Gowalla is a location-based social networking website where users share their locations by checking-in. The friendship network is undirected and was collected using their public API, and consists of 196,591 nodes and 950,327 edges. We have collected a total of 6,442,890 check-ins of these users over the period of Feb. 2009 - Oct. 2010.", "price": 918, "created_at": "2025-01-09 13:44:40.882502", "keyword": ["Recommendation Systems", "Session-Based Recommendations", "Collaborative Filtering", "point of interests"], "id": "32d75dd8-59ff-44b0-8eb8-ae1723716073", "image_url": ""}, {"title": "HIC", "short_description": "The Hands in action dataset (**HIC**) dataset has RGB-D sequences of hands interacting with objects.\n\nSource: [Learning joint reconstruction of hands and manipulated objects](https://arxiv.org/abs/1904.05767)\nImage Source: [http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/](http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/)", "price": 413, "created_at": "2025-01-09 13:44:40.882596", "keyword": ["Hand Pose Estimation", "Hand Joint Reconstruction"], "id": "33276c7f-45a4-4cef-834a-f8e2d02ab735", "image_url": "https://production-media.paperswithcode.com/datasets/HIC-0000003522-02107325.jpeg"}, {"title": "HICO", "short_description": "**HICO** is a benchmark for recognizing human-object interactions (HOI). \r\n\r\nKey features:\r\n\r\n- A diverse set of interactions with common object categories\r\n- A list of well-defined, sense-based HOI categories\r\n- An exhaustive labeling of co-occurring interactions with an object category in each image\r\n- The annotation of each HOI instance (i.e. a human and an object bounding box with an interaction class label) in all images\r\n\r\nSource: [HICO: A Benchmark for Recognizing Human-Object Interactions in Images](http://openaccess.thecvf.com/content_iccv_2015/papers/Chao_HICO_A_Benchmark_ICCV_2015_paper.pdf)", "price": 924, "created_at": "2025-01-09 13:44:40.882690", "keyword": ["Human-Object Interaction Detection", "Zero-Shot Human-Object Interaction Detection"], "id": "338e4fec-e08e-4278-8ce3-dda577f92bde", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_2.26.15_PM.png"}, {"title": "HKU-IS", "short_description": "**HKU-IS** is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects.\r\n\r\nSource: [Deep Contrast Learning for Salient Object Detection](https://arxiv.org/abs/1603.01976)\r\nImage Source: [https://sites.google.com/site/ligb86/mdfsaliency/](https://sites.google.com/site/ligb86/mdfsaliency/)", "price": 788, "created_at": "2025-01-09 13:44:40.882784", "keyword": ["RGB Salient Object Detection", "Saliency Detection", "Salient Object Detection"], "id": "4cb6dbd3-20bc-4785-bbf2-d2812587f556", "image_url": "https://production-media.paperswithcode.com/datasets/HKU-IS-0000001036-74ca5021_5r1GCzj.jpg"}, {"title": "HMDB51", "short_description": "The **HMDB51** dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \u201cjump\u201d, \u201ckiss\u201d and \u201claugh\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\r\n\r\nSource: [Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors](https://arxiv.org/abs/1505.04868)\r\nImage Source: [https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database)", "price": 753, "created_at": "2025-01-09 13:44:40.882895", "keyword": ["Action Recognition", "Temporal Action Localization", "Skeleton Based Action Recognition", "Action Classification", "Action Recognition In Videos", "Zero-Shot Action Recognition", "Self-Supervised Action Recognition", "Few Shot Action Recognition", "Self-supervised Video Retrieval", "Self-Supervised Action Recognition Linear"], "id": "6c96f748-6601-4f78-9651-536b7bd9f248", "image_url": "https://production-media.paperswithcode.com/datasets/HMDB51-0000000067-acd6a5a5_xUuoXLd.jpg"}, {"title": "HOList", "short_description": "The official **HOList** benchmark for automated theorem proving consists of all theorem statements in the core, complex, and flyspeck corpora. The goal of the benchmark is to prove as many theorems as possible in the HOList environment in the order they appear in the database. That is, only theorems that occur before the current theorem are supposed to be used as premises (lemmata) in its proof.\r\n\r\nSource: [HoList](https://sites.google.com/view/holist/home)\r\nImage Source: [https://sites.google.com/view/holist/home](https://sites.google.com/view/holist/home)", "price": 641, "created_at": "2025-01-09 13:44:40.882986", "keyword": ["Automated Theorem Proving"], "id": "2518c961-057b-4abc-ad39-94cd5e2abde4", "image_url": "https://production-media.paperswithcode.com/datasets/HOList-0000003627-c2ad3ede.jpg"}, {"title": "HRF", "short_description": "The **HRF** dataset is a dataset for retinal vessel segmentation which comprises 45 images and is organized as 15 subsets. Each subset contains one healthy fundus image, one image of patient with diabetic retinopathy and one glaucoma image. The image sizes are 3,304 x 2,336, with a training/testing image split of 22/23.\r\n\r\nSource: [Connection Sensitive Attention U-NET for Accurate Retinal Vessel Segmentation](https://arxiv.org/abs/1903.05558)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531](https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531)", "price": 639, "created_at": "2025-01-09 13:44:40.883077", "keyword": ["Retinal Vessel Segmentation"], "id": "28c9d812-cd59-4934-b175-a9e351f22200", "image_url": "https://production-media.paperswithcode.com/datasets/HRF-0000001444-d6ad3438_uGBINBF.jpg"}, {"title": "HandNet", "short_description": "The HandNet dataset contains depth images of 10 participants' hands non-rigidly deforming in front of a RealSense RGB-D camera. The annotations are generated by a magnetic annotation technique. 6D pose is available for the center of the hand as well as the five fingertips (i.e. position and orientation of each).\r\n\r\nSource: [Rule Of Thumb: Deep derotation for improved fingertip detection](/paper/rule-of-thumb-deep-derotation-for-improved)", "price": 780, "created_at": "2025-01-09 13:44:40.883176", "keyword": ["Pose Estimation", "Hand Pose Estimation", "Hand Segmentation"], "id": "4108ada9-651e-4a01-9e5c-d6d01c5adda1", "image_url": "https://production-media.paperswithcode.com/datasets/HandNetData.gif"}, {"title": "HappyDB", "short_description": "**HappyDB** is a corpus of 100,000 crowdsourced happy moments.\r\n\r\nSource: [HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments](/paper/happydb-a-corpus-of-100000-crowdsourced-happy)", "price": 768, "created_at": "2025-01-09 13:44:40.883269", "keyword": ["Sentiment Analysis", "Art Analysis"], "id": "7c3986bd-1745-489f-a384-af74450656de", "image_url": ""}, {"title": "Helen", "short_description": "The HELEN dataset is composed of 2330 face images of 400\u00d7400 pixels with labeled facial components generated through manually-annotated contours along eyes, eyebrows, nose, lips and jawline.\r\n\r\nSource: [Face Parsing via a Fully-Convolutional Continuous CRF Neural Network](https://arxiv.org/abs/1708.03736)\r\nImage Source: [http://www.ifp.illinois.edu/~vuongle2/helen/](http://www.ifp.illinois.edu/~vuongle2/helen/)", "price": 173, "created_at": "2025-01-09 13:44:40.883361", "keyword": ["Semantic Segmentation", "Face Alignment", "Facial Landmark Detection", "Face Parsing", "Semantic Segmentation", "Face Alignment", "Facial Landmark Detection", "Face Parsing"], "id": "639d5ec8-168d-4a74-afe6-0c52720393b0", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.49.56_PM.png"}, {"title": "HindEnCorp", "short_description": "A parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences.\r\n\r\nSource: [HindEnCorp - Hindi-English and Hindi-only Corpus for Machine Translation](/paper/hindencorp-hindi-english-and-hindi-only)", "price": 886, "created_at": "2025-01-09 13:44:40.883457", "keyword": ["Machine Translation", "Language Identification", "Chunking"], "id": "eadd71c9-eb1d-4a69-bbc9-c2138974587f", "image_url": ""}, {"title": "HoME", "short_description": "HoME (Household Multimodal Environment) is a multimodal environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more.", "price": 898, "created_at": "2025-01-09 13:44:40.883549", "keyword": [], "id": "934f45d2-b51a-4c2f-ad40-89f5a1b653b6", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_16.24.28.png"}, {"title": "HolStep", "short_description": "HolStep is a dataset based on higher-order logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. \r\n\r\nSource: [HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving](/paper/holstep-a-machine-learning-dataset-for-higher)", "price": 647, "created_at": "2025-01-09 13:44:40.883643", "keyword": ["Automated Theorem Proving", "Mathematical Proofs", "Dimensionality Reduction"], "id": "e1ae2207-f734-4f26-8600-b1966adf8fe6", "image_url": ""}, {"title": "Hopkins155", "short_description": "The Hopkins 155 dataset consists of 156 video sequences of two or three motions. Each video sequence motion corresponds to a low-dimensional subspace. There are 39\u2212550 data vectors drawn from two or three motions for each video sequence.\r\n\r\nSource: [Symmetric low-rank representation for subspace clustering](https://arxiv.org/abs/1410.8618)\r\nImage Source: [http://www.vision.jhu.edu/data/hopkins155/](http://www.vision.jhu.edu/data/hopkins155/)", "price": 231, "created_at": "2025-01-09 13:44:40.883735", "keyword": ["Motion Segmentation"], "id": "03a6aba7-9396-4bad-9caf-3c50ac0a8f11", "image_url": "https://production-media.paperswithcode.com/datasets/Hopkins155-0000000947-6716dbe2_TwNjCjg.jpg"}, {"title": "HotpotQA", "short_description": "**HotpotQA** is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. \r\n\r\nA diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.\r\n\r\nSource: [Answering Complex Open-domain Questions Through Iterative Query Generation](https://arxiv.org/abs/1910.07000)\r\nImage Source: [Yang et al](https://arxiv.org/pdf/1809.09600v1.pdf)", "price": 128, "created_at": "2025-01-09 13:44:40.883826", "keyword": ["Question Answering", "Sequence-to-sequence Language Modeling", "Reading Comprehension"], "id": "1b8c59af-8008-4a59-895c-725e9a5995b7", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.08.56_PM.png"}, {"title": "How2", "short_description": "The **How2** dataset contains 13,500 videos, or 300 hours of speech, and is split into 185,187 training, 2022 development (dev), and 2361 test utterances. It has subtitles in English and crowdsourced Portuguese translations.\r\n\r\nSource: [exploring multiview correlations in open-domain videos](https://arxiv.org/abs/1811.08890)", "price": 894, "created_at": "2025-01-09 13:44:40.883918", "keyword": ["Text Summarization", "Audio-Visual Speech Recognition", "Multimodal Abstractive Text Summarization"], "id": "874e1abc-f386-4c35-acd5-06535aaf43a8", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_6.42.23_PM.png"}, {"title": "Human3.6M", "short_description": "The **Human3.6M** dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos.\r\n\r\nSource: [Space-Time Representation of People Based on 3D Skeletal Data: A Review](https://arxiv.org/abs/1601.01006)\r\n\r\nImage Source: [Yu et al](https://www.researchgate.net/publication/320271480_Coupled_Multiview_Auto-Encoders_with_Locality-Sensitivity_for_3D_Human_Pose_Estimation)", "price": 645, "created_at": "2025-01-09 13:44:40.884010", "keyword": ["3D Human Pose Estimation", "3D Absolute Human Pose Estimation", "Video Prediction", "Human action generation", "2D Pose Estimation", "3D Pose Estimation", "Human Part Segmentation", "Human Pose Forecasting", "Unsupervised Human Pose Estimation", "Pose Retrieval", "Weakly-supervised 3D Human Pose Estimation", "Multi-Hypotheses 3D Human Pose Estimation", "Unsupervised 3D Human Pose Estimation", "Root Joint Localization", "Monocular 3D Human Pose Estimation", "3D Human Pose Estimation in Limited Data", "3D Human Pose Estimation", "3D Absolute Human Pose Estimation", "Video Prediction", "Human action generation", "2D Pose Estimation", "3D Pose Estimation", "Human Part Segmentation", "Human Pose Forecasting", "Unsupervised Human Pose Estimation", "Pose Retrieval", "Weakly-supervised 3D Human Pose Estimation", "Multi-Hypotheses 3D Human Pose Estimation", "Unsupervised 3D Human Pose Estimation", "Root Joint Localization", "Monocular 3D Human Pose Estimation", "3D Human Pose Estimation in Limited Data"], "id": "b3e98a59-87bd-4eaf-a0a8-de5d1c12484f", "image_url": "https://production-media.paperswithcode.com/datasets/HUMANPOSE.png"}, {"title": "Hutter Prize", "short_description": "The Hutter Prize Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens.\r\n\r\nSource: [NLP Progress](http://nlpprogress.com/english/language_modeling.html)", "price": 159, "created_at": "2025-01-09 13:44:40.884109", "keyword": ["Language Modelling"], "id": "973bdeb6-f43c-4440-824d-10ab24e8f39f", "image_url": ""}, {"title": "HyperLex", "short_description": "A dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. \r\n\r\nSource: [HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment](/paper/hyperlex-a-large-scale-evaluation-of-graded)", "price": 287, "created_at": "2025-01-09 13:44:40.884203", "keyword": ["Lexical Entailment"], "id": "2269a2b2-7f9c-49c0-89e5-229737f60492", "image_url": ""}, {"title": "IAM", "short_description": "The **IAM** database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.\r\n\r\nSource: [Measuring Human Perception to Improve Handwritten Document Transcription](https://arxiv.org/abs/1904.03734)\r\nImage Source: [https://fki.tic.heia-fr.ch/databases/iam-handwriting-database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database)", "price": 419, "created_at": "2025-01-09 13:44:40.884296", "keyword": ["Optical Character Recognition (OCR)", "Handwritten Text Recognition", "Handwriting Recognition", "Data Augmentation", "Optical Character Recognition (OCR)", "Handwritten Text Recognition", "Handwriting Recognition", "Data Augmentation"], "id": "85e7a8b0-e2de-4840-b145-dac5c3ed1ab6", "image_url": "https://production-media.paperswithcode.com/datasets/IAM-0000003255-109cf78d.jpg"}, {"title": "ICDAR 2013", "short_description": "The **ICDAR 2013** dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.\r\n\r\nSource: [Single Shot Text Detector with Regional Attention](https://arxiv.org/abs/1709.00138)\r\nImage Source: [https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856](https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856)", "price": 584, "created_at": "2025-01-09 13:44:40.884390", "keyword": ["Scene Text Recognition", "Scene Text Detection", "Table Detection", "Handwritten Chinese Text Recognition", "Scene Text Recognition", "Scene Text Detection", "Table Detection", "Handwritten Chinese Text Recognition"], "id": "44791de4-c517-4ad1-a206-008f64218723", "image_url": "https://production-media.paperswithcode.com/datasets/ICDAR_2013-0000000183-2bd434f3_L1zMfSY.jpeg"}, {"title": "ICDAR 2015", "short_description": "**ICDAR 2015** was a scene text detection used for the ICDAR 2015 conference.", "price": 514, "created_at": "2025-01-09 13:44:40.884487", "keyword": ["Scene Text Detection", "Text Spotting"], "id": "fb9bdbe9-92da-45b9-b93c-6fe32a5bb496", "image_url": ""}, {"title": "ICDAR 2017", "short_description": "ICDAR2017 is a dataset for scene text detection.\n\nSource: [Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images](https://arxiv.org/abs/2002.06423)\nImage Source: [https://rrc.cvc.uab.es/?ch=7](https://rrc.cvc.uab.es/?ch=7)", "price": 852, "created_at": "2025-01-09 13:44:40.884581", "keyword": ["Scene Text Detection"], "id": "74add219-a81a-4160-8cf4-0eb3bb7f5e13", "image_url": "https://production-media.paperswithcode.com/datasets/ICDAR_2017-0000003610-acd6bcd0.jpg"}, {"title": "ICL-NUIM", "short_description": "The **ICL-NUIM** dataset aims at benchmarking RGB-D, Visual Odometry and SLAM algorithms. Two different scenes (the living room and the office room scene) are provided with ground truth. Living room has 3D surface ground truth together with the depth-maps as well as camera poses and as a result perfectly suits not just for benchmarking camera trajectory but also reconstruction. Office room scene comes with only trajectory data and does not have any explicit 3D model with it.\n\nAll data is compatible with the evaluation tools available for the TUM RGB-D dataset, and if your system can take TUM RGB-D format PNGs as input, the authors\u2019 TUM RGB-D Compatible data will also work (given the correct camera parameters).\n\nSource: [https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)\nImage Source: [https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)", "price": 332, "created_at": "2025-01-09 13:44:40.884674", "keyword": [], "id": "068b5d34-807e-48cd-a8aa-cb0ed7061491", "image_url": "https://production-media.paperswithcode.com/datasets/ICL-NUIM-0000003643-03bf063d.jpg"}, {"title": "ICVL Hand Posture", "short_description": "The ICVL dataset is a hand pose estimation dataset that consists of 330K training frames and 2 testing sequences with each 800 frames. The dataset is collected from 10 different subjects with 16 hand joint annotations for each frame.\n\nSource: [AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation](https://arxiv.org/abs/2007.09590)\nImage Source: [Tang et al.; Latent Regression Forest: Structured Estimation of 3D Hand Poses](https://alykhantejani.github.io/pdfs/LRF_PAMI_DRAFT.pdf)", "price": 611, "created_at": "2025-01-09 13:44:40.884765", "keyword": ["Pose Estimation", "Hand Pose Estimation", "3D Hand Pose Estimation"], "id": "9d01929a-b6aa-4a5a-a943-5ba0f90cdf75", "image_url": "https://production-media.paperswithcode.com/datasets/ICVL_Hand_Posture-0000003412-444da56f.jpg"}, {"title": "IEMOCAP", "short_description": "Multimodal Emotion Recognition **IEMOCAP** The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.\r\n\r\nSource: [Multi-attention Recurrent Network for Human Communication Comprehension](https://arxiv.org/abs/1802.00923)\r\nImage Source: [https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf](https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf)", "price": 917, "created_at": "2025-01-09 13:44:40.884857", "keyword": ["Speech Emotion Recognition", "Emotion Recognition in Conversation", "Multimodal Emotion Recognition", "Speech Emotion Recognition", "Emotion Recognition in Conversation", "Multimodal Emotion Recognition"], "id": "e0fe07c5-8992-4adb-9918-8f0f4c67ab36", "image_url": "https://production-media.paperswithcode.com/datasets/IEMOCAP-0000000251-ca93d821_Jxi5lIZ.jpg"}, {"title": "IJB-A", "short_description": "The **IARPA Janus Benchmark A** (**IJB-A**) database is developed with the aim to augment more challenges to the face recognition task by collecting facial images with a wide variations in pose, illumination, expression, resolution and occlusion. IJB-A is constructed by collecting 5,712 images and 2,085 videos from 500 identities, with an average of 11.4 images and 4.2 videos per identity.\r\n\r\nSource: [von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification](https://arxiv.org/abs/1706.04264)\r\n\r\nImage Source: [Ruan et al](https://www.researchgate.net/figure/The-IARPA-Janus-Benchmark-A-IJB-A-dataset-face-verification-11-test-protocol-a_fig12_342756996)", "price": 538, "created_at": "2025-01-09 13:44:40.884948", "keyword": ["Face Verification", "Face Identification"], "id": "326806c6-04f9-47c8-92b7-aaff13f21d8d", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_10.54.39_AM.png"}, {"title": "IJB-B", "short_description": "The **IJB-B** dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification.\r\n\r\nSource: [An Automatic System for Unconstrained Video-Based Face Recognition](https://arxiv.org/abs/1812.04058)\r\nImage Source: [https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf](https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf)", "price": 644, "created_at": "2025-01-09 13:44:40.885040", "keyword": ["Face Recognition", "Face Verification", "Quantization", "Face Identification"], "id": "f7487ee3-3ea5-439f-a14d-98516fa7210a", "image_url": "https://production-media.paperswithcode.com/datasets/IJB-B-0000000524-84574b63_6cR1dhC.jpg"}, {"title": "IMDB-BINARY", "short_description": "**IMDB-BINARY** is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.\r\n\r\nSource: [A simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508)", "price": 213, "created_at": "2025-01-09 13:44:40.885131", "keyword": ["Graph Classification", "Graph Representation Learning"], "id": "4e619e67-e0de-4c4e-ae39-68dd0bf43d6b", "image_url": ""}, {"title": "IMDB-MULTI", "short_description": "**IMDB-MULTI** is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi.\r\n\r\nSource: [Learning metrics for persistence-based summaries and applications for graph classification](https://arxiv.org/abs/1904.12189)", "price": 848, "created_at": "2025-01-09 13:44:40.885222", "keyword": ["Graph Classification", "Document Classification", "Graph Similarity"], "id": "9872d7a8-5a00-4749-865c-1106d49215e7", "image_url": ""}, {"title": "IMDb Movie Reviews", "short_description": "The **IMDb Movie Reviews** dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.\r\n\r\nSource: [http://nlpprogress.com/english/sentiment_analysis.html](http://nlpprogress.com/english/sentiment_analysis.html)\r\nImage Source: [Maas et al](https://www.aclweb.org/anthology/P11-1015/)", "price": 417, "created_at": "2025-01-09 13:44:40.885318", "keyword": ["Text Classification", "Sentiment Analysis", "Link Prediction", "Language Modelling", "Node Clustering", "Paraphrase Identification", "SQL Parsing", "Opinion Mining", "Graph Similarity"], "id": "0cb7b1bc-d669-41dc-8e21-6b9d5e023b4d", "image_url": "https://production-media.paperswithcode.com/datasets/review.png"}, {"title": "INRIA Holidays Dataset", "short_description": "The Holidays dataset is a set of images which mainly contains some of the authors' personal holidays photos. The remaining ones were taken on purpose to test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images are in high resolution. The dataset contains 500 image groups, each of which represents a distinct scene or object. The first image of each group is the query image and the correct retrieval results are the other images of the group.\r\n\r\nSource: [INRIA Holidays Dataset](http://lear.inrialpes.fr/~jegou/data.php#holidays)", "price": 838, "created_at": "2025-01-09 13:44:40.885409", "keyword": ["Content-Based Image Retrieval"], "id": "d13e8476-0ba6-46a8-9b7d-9d435e088bab", "image_url": ""}, {"title": "INRIA Person", "short_description": "The **INRIA Person** dataset is a dataset of images of persons used for pedestrian detection. It consists of 614 person detections for training and 288 for testing.\r\n\r\nSource: [http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/)\r\nImage Source: [https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181](https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181)", "price": 695, "created_at": "2025-01-09 13:44:40.885501", "keyword": ["Object Detection", "Pose Estimation", "Pedestrian Detection"], "id": "e3c6532b-e19b-4340-9833-d3400d8ac7d6", "image_url": "https://production-media.paperswithcode.com/datasets/INRIA_Person-0000003261-ae8b6c09.jpg"}, {"title": "IQUAD", "short_description": "IQUAD is a dataset for Visual Question Answering in interactive environments. It is built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive object. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration.\r\n\r\nSource: [IQA: Visual Question Answering in Interactive Environments](https://arxiv.org/abs/1712.03316)", "price": 987, "created_at": "2025-01-09 13:44:40.885592", "keyword": ["Question Answering", "Visual Question Answering (VQA)", "Visual Navigation"], "id": "04a32100-5a99-4691-aaa3-a8da2bc2275e", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-26_at_18.16.29.png"}, {"title": "ISTD", "short_description": "The Image Shadow Triplets dataset (**ISTD**) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image.\r\n\r\nSource: [ST-CGAN: \"Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal\"](https://arxiv.org/pdf/1712.02478.pdf)\r\nImage Source: [Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal](https://paperswithcode.com/paper/stacked-conditional-generative-adversarial/)", "price": 80, "created_at": "2025-01-09 13:44:40.885684", "keyword": ["RGB Salient Object Detection", "Shadow Removal"], "id": "1c8c0e89-4c12-47d3-946f-1b6a77dd1223", "image_url": "https://production-media.paperswithcode.com/datasets/ISTD-0000001614-924a0ee6_DBzVIbn.jpg"}, {"title": "ITOP", "short_description": "The **ITOP** dataset consists of 40K training and 10K testing depth images for each of the front-view and top-view tracks. This dataset contains depth images with 20 actors who perform 15 sequences each and is recorded by two Asus Xtion Pro cameras. The ground-truth of this dataset is the 3D coordinates of 15 body joints.\n\nSource: [V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map](https://arxiv.org/abs/1711.07399)\nImage Source: [https://www.youtube.com/watch?v=4gPI-GOf9wg](https://www.youtube.com/watch?v=4gPI-GOf9wg)", "price": 62, "created_at": "2025-01-09 13:44:40.885775", "keyword": ["Pose Estimation", "Pose Estimation"], "id": "a2852e4a-0c3c-46fa-8ae8-f2e3fa32e096", "image_url": "https://production-media.paperswithcode.com/datasets/ITOP-0000003396-ff75c08c.jpeg"}, {"title": "IWSLT 2019", "short_description": "The **IWSLT 2019** dataset contains source, Machine Translated, reference and Post-Edited text, which can be used to quantify and evaluate Post-editing effort after automatic MT.\n\nSource: [https://arxiv.org/abs/1910.06204](https://arxiv.org/abs/1910.06204)", "price": 528, "created_at": "2025-01-09 13:44:40.885866", "keyword": [], "id": "3ed8c680-70fb-410c-82c2-adbcd0474a79", "image_url": ""}, {"title": "IWSLT2015", "short_description": "The IWSLT 2015 Evaluation Campaign featured three tracks: automatic speech recognition (ASR), spoken language translation (SLT), and machine translation (MT). For ASR we offered two tasks, on English and German, while\r\nfor SLT and MT a number of tasks were proposed, involving English, German, French, Chinese, Czech, Thai, and Vietnamese. All tracks involved the transcription or translation of TED talks, either made available by the official TED website or by other TEDx events. A notable change with respect to previous evaluations was the use of unsegmented speech in the SLT track in order to better fit a real application scenario.", "price": 235, "created_at": "2025-01-09 13:44:40.885957", "keyword": ["Machine Translation", "Document Translation"], "id": "2e601d0a-7c4a-4175-8073-37c55f375f69", "image_url": ""}, {"title": "ImageNet", "short_description": "The **ImageNet** dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.\r\nThe publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.\r\nILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., \u201cthere are cars in this image\u201d but \u201cthere are no tigers,\u201d and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., \u201cthere is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels\u201d.\r\nThe ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.\r\n\r\n* Total number of non-empty WordNet synsets: 21841\r\n* Total number of images: 14197122\r\n* Number of images with bounding box annotations: 1,034,908\r\n* Number of synsets with SIFT features: 1000\r\n* Number of images with SIFT features: 1.2 million\r\n\r\nSource: [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)\r\nImage Source: [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)", "price": 446, "created_at": "2025-01-09 13:44:40.886049", "keyword": ["Image Classification", "Image Generation", "Few-Shot Image Classification", "Image Super-Resolution", "Few-Shot Learning", "Semi-Supervised Image Classification", "Image Clustering", "Neural Architecture Search", "Weakly Supervised Object Detection", "Image Inpainting", "Prompt Engineering", "Binarization", "Zero-Shot Transfer Image Classification", "Adversarial Defense", "Quantization", "Object Recognition", "Weakly-Supervised Object Localization", "Unsupervised Image Classification", "Adversarial Robustness", "Zero-Shot Composed Image Retrieval (ZS-CIR)", "Image Deblurring", "Medical Image Classification", "Network Pruning", "Knowledge Distillation", "Image Compressed Sensing", "Model Compression", "Data Augmentation", "Sparse Learning", "Classification with Binary Neural Network", "Self-Supervised Image Classification", "Image Colorization", "JPEG Decompression", "Image Classification with Differential Privacy", "Image Classification", "Image Generation", "Few-Shot Image Classification", "Image Super-Resolution", "Few-Shot Learning", "Semi-Supervised Image Classification", "Image Clustering", "Neural Architecture Search", "Weakly Supervised Object Detection", "Image Inpainting", "Prompt Engineering", "Binarization", "Zero-Shot Transfer Image Classification", "Adversarial Defense", "Quantization", "Object Recognition", "Weakly-Supervised Object Localization", "Unsupervised Image Classification", "Adversarial Robustness", "Zero-Shot Composed Image Retrieval (ZS-CIR)", "Image Deblurring", "Medical Image Classification", "Network Pruning", "Knowledge Distillation", "Image Compressed Sensing", "Model Compression", "Data Augmentation", "Sparse Learning", "Classification with Binary Neural Network", "Self-Supervised Image Classification", "Image Colorization", "JPEG Decompression", "Image Classification with Differential Privacy"], "id": "4084ffec-b3ea-4a58-9d93-3f6e5b8582f1", "image_url": "https://production-media.paperswithcode.com/datasets/ImageNet-0000000008-f2e87edd_Y0fT5zg.jpg"}, {"title": "Indian Pines", "short_description": "**Indian Pines** is a Hyperspectral image segmentation dataset. The input data consists of hyperspectral bands over a single landscape in Indiana, US, (Indian Pines data set) with 145\u00d7145 pixels. For each pixel, the data set contains 220 spectral reflectance bands which represent different portions of the electromagnetic spectrum in the wavelength range 0.4\u22122.5\u22c510\u22126.\r\n\r\nSource: [Layer-Parallel Training of Deep Residual Neural Networks](https://arxiv.org/abs/1812.04352)\r\nImage Source: [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines)", "price": 30, "created_at": "2025-01-09 13:44:40.886140", "keyword": ["Hyperspectral Image Classification", "Hyperspectral Image Inpainting"], "id": "564a5530-6f5b-4342-9bfd-0c776fbc8de2", "image_url": "https://production-media.paperswithcode.com/datasets/Indian_Pines-0000000500-cc013448_8g6nORw.jpg"}, {"title": "JAFFE", "short_description": "The **JAFFE** dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.\r\n\r\nSource: [Balanced k-Means and Min-Cut Clustering](https://arxiv.org/abs/1411.6235)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190](https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190)", "price": 550, "created_at": "2025-01-09 13:44:40.886230", "keyword": ["Facial Expression Recognition (FER)", "Clustering Algorithms Evaluation", "Image/Document Clustering"], "id": "2df46781-1e5b-4e51-b680-e90aa588aa45", "image_url": "https://production-media.paperswithcode.com/datasets/JAFFE-0000000269-3ccc1831_E7er8Bu.jpg"}, {"title": "JAMUL", "short_description": "A large-scale evaluation dataset for headlines of three different lengths composed by professional editors.\r\n\r\nSource: [A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation](/paper/a-large-scale-multi-length-headline-corpus)", "price": 43, "created_at": "2025-01-09 13:44:40.886322", "keyword": [], "id": "befbb7bf-4303-464b-9bea-b97febaceef5", "image_url": ""}, {"title": "JFLEG", "short_description": "JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. \r\n\r\nSource: [JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction](https://arxiv.org/pdf/1702.04066v1.pdf)", "price": 328, "created_at": "2025-01-09 13:44:40.886414", "keyword": ["Grammatical Error Correction", "Grammatical Error Detection", "Grammatical Error Correction", "Grammatical Error Detection"], "id": "98520123-1798-4a30-b896-94fd42b97a84", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_6.48.34_PM.png"}, {"title": "JFT-300M", "short_description": "**JFT-300M** is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images.", "price": 438, "created_at": "2025-01-09 13:44:40.886509", "keyword": ["Image Classification", "Image Classification"], "id": "2e5dc94f-4977-416c-8627-e6a861beb008", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-14_at_5.06.38_PM.png"}, {"title": "JHMDB", "short_description": "**JHMDB** is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality).\r\n\r\nSource: [Unsupervised Deep Metric Learning via Orthogonality based Probabilistic Loss](https://arxiv.org/abs/2008.09880)\r\nImage Source: [https://arxiv.org/pdf/1712.06316.pdf](https://arxiv.org/pdf/1712.06316.pdf)", "price": 520, "created_at": "2025-01-09 13:44:40.886600", "keyword": ["Pose Estimation", "Action Detection", "Skeleton Based Action Recognition", "Referring Expression Segmentation", "2D Human Pose Estimation"], "id": "e593da76-091c-4cba-b902-247ad42b80e9", "image_url": "https://production-media.paperswithcode.com/datasets/JHMDB-0000003601-47f7c300.jpg"}, {"title": "JNC", "short_description": "The JNC data provides common supervision data for headline generation.\r\n\r\nSource: [A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation](/paper/a-large-scale-multi-length-headline-corpus)", "price": 202, "created_at": "2025-01-09 13:44:40.886691", "keyword": [], "id": "2723de9a-64ce-4364-aa52-df1c9abd345c", "image_url": ""}, {"title": "JNLPBA", "short_description": "**JNLPBA** is a biomedical dataset that comes from the GENIA version 3.02 corpus (Kim et al., 2003). It was created with a controlled search on MEDLINE. From this search 2,000 abstracts were selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification. 36 terminal classes were used to annotate the GENIA corpus.", "price": 680, "created_at": "2025-01-09 13:44:40.886782", "keyword": ["Token Classification", "Named Entity Recognition (NER)", "Relation Extraction", "Medical Named Entity Recognition"], "id": "1cca71d2-04f0-461a-912f-f537e274a042", "image_url": ""}, {"title": "JW300", "short_description": "A parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average.\r\n\r\nSource: [JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages](/paper/jw300-a-wide-coverage-parallel-corpus-for-low)", "price": 223, "created_at": "2025-01-09 13:44:40.886887", "keyword": [], "id": "51e8e393-1351-463b-93dc-13ec3ed66900", "image_url": ""}, {"title": "K2HPD", "short_description": "Includes 100K depth images under challenging scenarios.\r\n\r\nSource: [Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning](/paper/human-pose-estimation-from-depth-images-via)", "price": 315, "created_at": "2025-01-09 13:44:40.886976", "keyword": ["Hand Pose Estimation", "3D Pose Estimation"], "id": "7421fc97-e4ad-4c42-8ea3-c7bf1f0b906d", "image_url": ""}, {"title": "KAIST Multispectral Pedestrian Detection Benchmark", "short_description": "KAIST Multispectral Pedestrian Dataset\r\n\r\nThe KAIST Multispectral Pedestrian Dataset is imaging hardware consisting of a color camera, a thermal camera and a beam splitter to capture the aligned multispectral (RGB color + Thermal) images. With this hardware, we captured various regular traffic scenes at day and night time to consider changes in light conditions. and, consists of 95k color-thermal pairs (640x480, 20Hz) taken from a vehicle. All the pairs are manually annotated (person, people, cyclist) for the total of 103,128 dense annotations and 1,182 unique pedestrians. The annotation includes temporal correspondence between bounding boxes like Caltech Pedestrian Dataset.\r\n\r\nFor more information, read [Multispectral Pedestrian Detection: Benchmark Dataset and Baseline (CVPR 2015)](https://openaccess.thecvf.com/content_cvpr_2015/papers/Hwang_Multispectral_Pedestrian_Detection_2015_CVPR_paper.pdf) or visit [this website](https://soonminhwang.github.io/rgbt-ped-detection/)", "price": 508, "created_at": "2025-01-09 13:44:40.887064", "keyword": ["Multispectral Object Detection"], "id": "9262db79-dfa1-4b2d-9a71-b0f0c6a58c3b", "image_url": "https://production-media.paperswithcode.com/datasets/d9eb255e-8afd-4af7-88cf-8661f82dcc61.png"}, {"title": "KITTI", "short_description": "**KITTI** (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. [\u00c1lvarez et al.](http://yann.lecun.com/exdb/publis/pdf/alvarez-eccv-12.pdf) generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. [Zhang et al.](http://www-video.eecs.berkeley.edu/papers/rzhang/zhang-icra-submission.pdf) annotated 252 (140 for training and 112 for testing) acquisitions \u2013 RGB and Velodyne scans \u2013 from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. [Ros et al.](http://refbase.cvc.uab.es/files/rrg2015.pdf) labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.\r\n\r\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)", "price": 597, "created_at": "2025-01-09 13:44:40.887153", "keyword": ["Object Detection", "Semantic Segmentation", "Pose Estimation", "Object Tracking", "3D Object Detection", "Image Super-Resolution", "Image-to-Image Translation", "Depth Estimation", "Object Localization", "Visual Place Recognition", "Multi-Object Tracking", "Birds Eye View Object Detection", "Video Prediction", "Panoptic Segmentation", "Transfer Learning", "Monocular Depth Estimation", "Novel View Synthesis", "Monocular 3D Object Detection", "Point Cloud Registration", "Image Dehazing", "Multiple Object Tracking", "Optical Flow Estimation", "Unsupervised Object Detection", "Depth Completion", "Stereo Image Super-Resolution", "Real-time Instance Segmentation", "Visual Odometry", "Stereo Depth Estimation", "3D Multi-Object Tracking", "Class-agnostic Object Detection", "Unsupervised Monocular Depth Estimation", "Object Proposal Generation", "Scene Flow Estimation", "Dense Pixel Correspondence Estimation", "Horizon Line Estimation", "Vehicle Pose Estimation", "3D Object Detection From Stereo Images", "Stereo Disparity Estimation", "Monocular Cross-View Road Scene Parsing(Road)", "3D Object Tracking", "Monocular Cross-View Road Scene Parsing(Vehicle)", "Prediction Of Occupancy Grid Maps", "Egocentric Pose Estimation", "Image to Point Cloud Registration", "3D Single Object Tracking", "Object Detection", "Semantic Segmentation", "Pose Estimation", "Object Tracking", "3D Object Detection", "Image Super-Resolution", "Image-to-Image Translation", "Depth Estimation", "Object Localization", "Visual Place Recognition", "Multi-Object Tracking", "Birds Eye View Object Detection", "Video Prediction", "Panoptic Segmentation", "Transfer Learning", "Monocular Depth Estimation", "Novel View Synthesis", "Monocular 3D Object Detection", "Point Cloud Registration", "Image Dehazing", "Multiple Object Tracking", "Optical Flow Estimation", "Unsupervised Object Detection", "Depth Completion", "Stereo Image Super-Resolution", "Real-time Instance Segmentation", "Visual Odometry", "Stereo Depth Estimation", "3D Multi-Object Tracking", "Class-agnostic Object Detection", "Unsupervised Monocular Depth Estimation", "Object Proposal Generation", "Scene Flow Estimation", "Dense Pixel Correspondence Estimation", "Horizon Line Estimation", "Vehicle Pose Estimation", "3D Object Detection From Stereo Images", "Stereo Disparity Estimation", "Monocular Cross-View Road Scene Parsing(Road)", "3D Object Tracking", "Monocular Cross-View Road Scene Parsing(Vehicle)", "Prediction Of Occupancy Grid Maps", "Egocentric Pose Estimation", "Image to Point Cloud Registration", "3D Single Object Tracking"], "id": "ff7dfeca-da32-45cd-bba1-5dacdd5e2262", "image_url": "https://production-media.paperswithcode.com/datasets/KITTI-0000000061-82e8e2fe_XTTqZ4N.jpg"}, {"title": "KP20k", "short_description": "**KP20k** is a large-scale scholarly articles dataset with 528K articles for training, 20K articles for validation and 20K articles for testing.\r\n\r\nSource: [Keyphrase Prediction With Pre-trained Language Model](https://arxiv.org/abs/2004.10462)\r\nImage Source: [https://arxiv.org/pdf/1704.06879.pdf](https://arxiv.org/pdf/1704.06879.pdf)", "price": 110, "created_at": "2025-01-09 13:44:40.887241", "keyword": ["Text Summarization", "Language Modelling", "Multi-Task Learning", "Keyphrase Extraction", "Phrase Ranking", "Phrase Tagging"], "id": "f458a614-778a-41c7-bc2c-b231e17a3a65", "image_url": "https://production-media.paperswithcode.com/datasets/KP20k-0000003557-d16e04c1.jpg"}, {"title": "KTH", "short_description": "The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the **KTH** Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors.\r\n\r\nSource: [Review of Action Recognition and Detection Methods](https://arxiv.org/abs/1610.06906)", "price": 766, "created_at": "2025-01-09 13:44:40.887334", "keyword": ["Action Recognition", "Temporal Action Localization", "Video Prediction"], "id": "0c1887ed-5089-47e0-ab8d-be95e5fdf6d0", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.37.36_PM.png"}, {"title": "Kvasir", "short_description": "The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class.\r\n\r\nSource: [Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis](https://arxiv.org/abs/2007.05914)\r\nImage Source: [https://datasets.simula.no/kvasir/](https://datasets.simula.no/kvasir/)", "price": 80, "created_at": "2025-01-09 13:44:40.887424", "keyword": ["Object Detection", "Semantic Segmentation", "Medical Image Segmentation", "Real-Time Object Detection", "Real-Time Semantic Segmentation", "Colorectal Polyps Characterization", "Polyp Segmentation", "Colorectal Gland Segmentation:"], "id": "d8fef9d8-c598-4aef-aeb6-572fd74a4b40", "image_url": "https://production-media.paperswithcode.com/datasets/Kvasir-0000003434-53159afb.jpg"}, {"title": "LAMA", "short_description": "LAnguage Model Analysis (**LAMA**) consists of a set of knowledge sources, each comprised of a set of facts. LAMA is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models.\r\n\r\nSource: [Language Models as Knowledge Bases?](https://arxiv.org/pdf/1909.01066v2.pdf)", "price": 808, "created_at": "2025-01-09 13:44:40.887512", "keyword": ["Question Answering", "Language Modelling", "Open-Domain Question Answering"], "id": "76bce256-27b2-4ff2-8112-ea37a274372f", "image_url": ""}, {"title": "LAMBADA", "short_description": "The **LAMBADA** (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.\r\n\r\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\r\nImage Source: [https://arxiv.org/pdf/1606.06031.pdf](https://arxiv.org/pdf/1606.06031.pdf)", "price": 871, "created_at": "2025-01-09 13:44:40.887601", "keyword": ["Language Modelling"], "id": "78f83130-ea3b-4d75-abf0-1c06f9b4deaa", "image_url": "https://production-media.paperswithcode.com/datasets/LAMBADA-0000002422-52650e4e_B4dJstl.jpg"}, {"title": "LFPW", "short_description": "The **Labeled Face Parts in-the-Wild** (**LFPW**) consists of 1,432 faces from images downloaded from the web using simple text queries on sites such as google.com, flickr.com, and yahoo.com.   Each image was labeled by three MTurk workers, and 29 fiducial points, shown below, are included in dataset.\r\n\r\nSource: [https://neerajkumar.org/databases/lfpw/](https://neerajkumar.org/databases/lfpw/)\r\nImage Source: [https://neerajkumar.org/databases/lfpw/](https://neerajkumar.org/databases/lfpw/)", "price": 33, "created_at": "2025-01-09 13:44:40.887693", "keyword": ["Pose Estimation", "Face Alignment", "Facial Landmark Detection", "Pose Estimation", "Face Alignment", "Facial Landmark Detection"], "id": "49608e0e-5ca7-40cb-a04b-8c03783899e8", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.02.37_PM.png"}, {"title": "LFSD", "short_description": "The **Light Field Saliency Database** (**LFSD**) contains 100 light fields with 360\u00d7360 spatial resolution. A rough focal stack and an all-focus image are provided for each light field. The images in this dataset usually have one salient foreground object and a background with good color contrast.\r\n\r\nSource: [Light Field Saliency Detection with Deep Convolutional Networks](https://arxiv.org/abs/1906.08331)\r\nImage Source: [https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/](https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/)", "price": 808, "created_at": "2025-01-09 13:44:40.887784", "keyword": ["RGB-D Salient Object Detection"], "id": "3d2d24fc-b9ad-4adf-926c-e9dd145446b1", "image_url": "https://production-media.paperswithcode.com/datasets/LFSD-0000002785-a646d105_jxj6l4j.jpg"}, {"title": "LFW", "short_description": "The **LFW** dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.\r\n\r\nSource: [A Performance Evaluation of Loss Functions for Deep Face Recognition](https://arxiv.org/abs/1901.05903)\r\nImage Source: [http://vis-www.cs.umass.edu/lfw](http://vis-www.cs.umass.edu/lfw)", "price": 489, "created_at": "2025-01-09 13:44:40.887874", "keyword": ["3D Face Modelling", "Face Recognition", "Face Verification", "Quantization", "Blind Face Restoration", "Synthetic Face Recognition", "Face Quality Assessement", "Face Anonymization", "Unsupervised face recognition", "Face Quality sAsessement"], "id": "cf844232-8a79-4d52-930e-67f461da3d04", "image_url": "https://production-media.paperswithcode.com/datasets/LFW-0000000022-7647ef6f_M2DdqYg.jpg"}, {"title": "LIAR", "short_description": "LIAR is a publicly available dataset for fake news detection. A decade-long of 12.8K manually labeled short statements were collected in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. The LIAR dataset4 includes 12.8K human labeled short statements from POLITIFACT.COM\u2019s API, and each statement is evaluated by a POLITIFACT.COM editor for its truthfulness. \r\n\r\nSource: [\u201cLiar, Liar Pants on Fire\u201d: A New Benchmark Dataset for Fake News Detection](https://www.aclweb.org/anthology/P17-2067.pdf)", "price": 735, "created_at": "2025-01-09 13:44:40.887966", "keyword": ["Stance Detection", "Fake News Detection", "Misinformation"], "id": "0ed42d42-cacd-4cd1-a806-a97692b8cd71", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_20.40.33.png"}, {"title": "LIP", "short_description": "The **LIP** (**Look into Person**) dataset is a large-scale dataset focusing on semantic understanding of a person. It contains 50,000 images with elaborated pixel-wise annotations of 19 semantic human part labels and 2D human poses with 16 key points. The images are collected from real-world scenarios and the subjects appear with challenging poses and view, heavy occlusions, various appearances and low resolution.\r\n\r\nSource: [http://sysu-hcp.net/lip/](http://sysu-hcp.net/lip/)\r\nImage Source: [http://sysu-hcp.net/lip/](http://sysu-hcp.net/lip/)", "price": 972, "created_at": "2025-01-09 13:44:40.888056", "keyword": ["Semantic Segmentation"], "id": "6da0129d-0a93-4d23-89db-6d574f00a34d", "image_url": "https://production-media.paperswithcode.com/datasets/LIP-0000003408-d2c0c9e1.jpg"}, {"title": "LOCATA", "short_description": "The **LOCATA** dataset is a dataset for acoustic source localization. It consists of real-world ambisonic speech recordings with optically tracked azimuth-elevation labels.\n\nSource: [Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks](https://arxiv.org/abs/1904.08452)\nImage Source: [https://www.locata.lms.tf.fau.de/files/2018/05/LOCATA_Paper_SAM_Workshop_2018.pdf](https://www.locata.lms.tf.fau.de/files/2018/05/LOCATA_Paper_SAM_Workshop_2018.pdf)", "price": 24, "created_at": "2025-01-09 13:44:40.888144", "keyword": ["Direction of Arrival Estimation"], "id": "e01e3c70-ed00-4690-b9ed-d6e208e07855", "image_url": "https://production-media.paperswithcode.com/datasets/LOCATA-0000003728-48dbca35.jpg"}, {"title": "LOL", "short_description": "The **LOL** dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400\u00d7600.\r\n\r\nSource: [Unsupervised Real-world Low-light Image Enhancement with Decoupled Networks](https://arxiv.org/abs/2005.02818)\r\nImage Source: [https://daooshee.github.io/BMVC2018website/](https://daooshee.github.io/BMVC2018website/)", "price": 178, "created_at": "2025-01-09 13:44:40.888232", "keyword": ["Low-Light Image Enhancement"], "id": "7b71c593-1298-4702-a7c9-7ce4d17308b4", "image_url": "https://production-media.paperswithcode.com/datasets/LOL-0000001442-ceaf989c_SxokoDT.jpg"}, {"title": "LRS2", "short_description": "The Oxford-BBC **Lip Reading Sentences 2** (**LRS2**) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.\r\n\r\nSource: [Audio-visual Recognition of Overlapped speech for the LRS2 dataset](https://arxiv.org/abs/2001.01656)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)", "price": 35, "created_at": "2025-01-09 13:44:40.888321", "keyword": ["Speech Recognition", "Speech Separation", "Visual Speech Recognition", "Automatic Speech Recognition (ASR)", "Lipreading", "Audio-Visual Speech Recognition", "Unconstrained Lip-synchronization", "Visual Keyword Spotting", "Image Manipulation"], "id": "cd0a4020-93fa-4225-b3d4-59a8c91d0c02", "image_url": "https://production-media.paperswithcode.com/datasets/LRS2-0000002639-120a5de5_wjTB8MZ.jpg"}, {"title": "LRS3-TED", "short_description": "LRS3-TED is a multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research. \r\n\r\nSource: [LRS3-TED: a large-scale dataset for visual speech recognition](https://arxiv.org/pdf/1809.00496v2.pdf)", "price": 17, "created_at": "2025-01-09 13:44:40.888410", "keyword": ["Speech Recognition", "Visual Speech Recognition", "Automatic Speech Recognition (ASR)", "Lipreading", "Audio-Visual Speech Recognition", "Visual Keyword Spotting"], "id": "24afce45-9cf0-4b3e-9a8d-73d075eb6cca", "image_url": ""}, {"title": "LRW", "short_description": "The **Lip Reading in the Wild** (**LRW**) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances.\r\n\r\nSource: [Towards Pose-invariant Lip-Reading](https://arxiv.org/abs/1911.06095)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)", "price": 953, "created_at": "2025-01-09 13:44:40.888517", "keyword": ["Lip to Speech Synthesis", "Lipreading", "Lip Reading", "Audio-Visual Speech Recognition", "Unconstrained Lip-synchronization", "Visual Keyword Spotting", "Talking Face Generation"], "id": "8e122954-ff4f-43da-9597-86c6ce0d8a00", "image_url": "https://production-media.paperswithcode.com/datasets/LRW-0000000054-ea37a69c_n7viTIh.jpg"}, {"title": "LSHTC", "short_description": "LSHTC is a dataset for large-scale text classification. The data used in the LSHTC challenges originates from two popular sources: the DBpedia and the ODP (Open Directory Project) directory, also known as DMOZ. DBpedia instances were selected from the english, non-regional Extended Abstracts provided by the DBpedia site. The DMOZ instances consist\r\nof either Content vectors, Description vectors or both. A Content vectors is obtained by directly indexing the web page using standard indexing chain (preprocessing, stemming/lemmatization, stop-word removal). \r\n\r\nSource: [LSHTC: A Benchmark for Large-Scale Text Classification](/paper/lshtc-a-benchmark-for-large-scale-text)", "price": 794, "created_at": "2025-01-09 13:44:40.888610", "keyword": ["Text Classification", "Multi-Label Classification", "Extreme Multi-Label Classification"], "id": "77ab912f-6996-40da-98a1-aed56631e314", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_6.11.37_PM.png"}, {"title": "LSMDC", "short_description": "This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. The validation set contains 7408 clips and evaluation is performed on a test set of 1000 videos from movies disjoint from the training and val sets.\r\n\r\nSource: [Use What You Have: Video Retrieval Using Representations From Collaborative Experts](https://arxiv.org/abs/1907.13487)\r\nImage Source: [https://sites.google.com/site/describingmovies/](https://sites.google.com/site/describingmovies/)", "price": 688, "created_at": "2025-01-09 13:44:40.888699", "keyword": ["Zero-Shot Learning", "Video Retrieval", "Zero-Shot Video Retrieval", "Fill Mask"], "id": "51223511-a312-4edc-a73b-2c614234871a", "image_url": "https://production-media.paperswithcode.com/datasets/LSMDC-0000001107-3ba9fe41_gFo8VOG.jpg"}, {"title": "LSP", "short_description": "The **Leeds Sports Pose** (**LSP**) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training.\r\n\r\nSource: [Deep Deformation Network for Object Landmark Localization](https://arxiv.org/abs/1605.01014)\r\n\r\nImage: [Sumer et al](https://www.researchgate.net/figure/Pose-estimation-results-in-Leeds-Sports-Pose-dataset-First-images-are-from-test-set-with_fig3_322058596)", "price": 572, "created_at": "2025-01-09 13:44:40.888787", "keyword": ["Pose Estimation", "3D Human Pose Estimation", "3D Pose Estimation", "Pose Estimation", "3D Human Pose Estimation", "3D Pose Estimation"], "id": "2925af5e-d8ff-4397-8020-baf2fb11b864", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_10.56.54_AM.png"}, {"title": "LSUN", "short_description": "The Large-scale Scene Understanding (**LSUN**) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category.\r\n\r\nSource: [Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs](https://arxiv.org/abs/1610.01119)\r\nImage Source: [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)", "price": 967, "created_at": "2025-01-09 13:44:40.888880", "keyword": ["Image Generation", "Image Generation"], "id": "8d281ce1-60bc-4a78-b05e-46c4ad9618ca", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_1.11.50_PM.png"}, {"title": "LUNA", "short_description": "The **LUNA** challenges provide datasets for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In [LUNA16](https://paperswithcode.com/dataset/luna16), participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified.\r\n\r\nSource: [Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge](/paper/validation-comparison-and-combination-of)", "price": 250, "created_at": "2025-01-09 13:44:40.888969", "keyword": ["Lung Nodule Segmentation", "Lung Nodule Detection", "Computed Tomography (CT)", "Lung Nodule Segmentation", "Lung Nodule Detection", "Computed Tomography (CT)"], "id": "9e860cf9-52ea-4019-8234-80e9fbae28c4", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_2.06.21_PM.png"}, {"title": "LVIS", "short_description": "LVIS is a dataset for long tail instance segmentation. It has annotations for over 1000 object categories in 164k images.\r\n\r\nSource: [LVIS](https://arxiv.org/pdf/1908.03195.pdf)", "price": 696, "created_at": "2025-01-09 13:44:40.889058", "keyword": ["Object Detection", "Instance Segmentation", "Unsupervised Object Detection", "Few-Shot Object Detection", "Zero-Shot Object Detection", "Open Vocabulary Object Detection", "Long-tailed Object Detection", "Novel Object Detection", "Zero-Shot Instance Segmentation"], "id": "ddad5cd4-79e6-4ce5-8b72-338178267fb9", "image_url": "https://production-media.paperswithcode.com/datasets/lvis.jpg"}, {"title": "LaMem", "short_description": "An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources).\r\n\r\nSource: [Understanding and Predicting Image Memorability at a Large Scale](/paper/understanding-and-predicting-image)", "price": 657, "created_at": "2025-01-09 13:44:40.889147", "keyword": ["Image Classification", "Image-to-Image Translation", "Style Transfer"], "id": "ba0a4f5c-0549-4a6b-9c17-94b9172a5a88", "image_url": ""}, {"title": "LaSOT", "short_description": "LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated\r\ntracking benchmark. The average video length of LaSOT\r\nis more than 2,500 frames, and each sequence comprises\r\nvarious challenges deriving from the wild where target objects may disappear and re-appear again in the view.", "price": 284, "created_at": "2025-01-09 13:44:40.889240", "keyword": ["Object Tracking", "Visual Object Tracking", "Visual Tracking"], "id": "76f7aec6-18d1-4142-9c03-cdee5dfca3ad", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_16.27.06.png"}, {"title": "Letter", "short_description": "Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\r\n\r\nSource: [UCL Machine Learning Repository Letter Recognition](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition)\r\nImage Source: [http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf](http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf)", "price": 543, "created_at": "2025-01-09 13:44:40.889329", "keyword": ["Image Clustering", "Core set discovery", "Image Clustering", "Core set discovery"], "id": "d49e8d64-2a29-4516-b6aa-fa2803059239", "image_url": "https://production-media.paperswithcode.com/datasets/Letter-0000002496-c0cafd99_Shc4uHm.jpg"}, {"title": "LibriSpeech", "short_description": "The **LibriSpeech** corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the \u2019clean\u2019 and \u2019other\u2019 categories, respectively, depending upon how well or challenging Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.\r\n\r\nSource: [State-of-the-art Speech Recognition using Multi-stream Self-attention with Dilated 1D Convolutions](https://arxiv.org/abs/1910.00716)", "price": 152, "created_at": "2025-01-09 13:44:40.889417", "keyword": ["Speech Recognition", "Automatic Speech Recognition", "Voice Conversion", "Resynthesis", "Automatic Phoneme Recognition", "Speech Recognition", "Automatic Speech Recognition", "Voice Conversion", "Resynthesis", "Automatic Phoneme Recognition"], "id": "7dfca587-90ae-40ca-9820-73c48057dfa5", "image_url": "https://production-media.paperswithcode.com/datasets/librispeech.png"}, {"title": "ListOps", "short_description": "The ListOps examples are comprised of summary operations on lists of single digit integers, written in prefix notation. The full sequence has a corresponding solution which is\r\nalso a single-digit integer, thus making it a ten-way balanced classification problem. For example, [MAX 2 9 [MIN 4 7 ] 0 ] has the solution 9. Each operation has a corresponding closing square bracket that defines the list of numbers for the operation. In this example, MIN operates on {4, 7}, while MAX operates on {2, 9, 4, 0}. \r\n\r\nSource: [ListOps: A Diagnostic Dataset for Latent Tree Learning](https://arxiv.org/pdf/1804.06028v1.pdf)", "price": 439, "created_at": "2025-01-09 13:44:40.889506", "keyword": ["Natural Language Inference", "Structured Prediction"], "id": "027ccf0c-ffdc-42c2-a915-ae1bbcfe9506", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_19.32.55.png"}, {"title": "M4", "short_description": "The **M4** dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones.\r\n\r\nThe M4 dataset was created by selecting a random sample of 100,000 time series from the ForeDeCk database. The selected series were then scaled to prevent negative observations and values lower than 10, thus avoiding possible problems when calculating various error measures. The scaling was performed by simply adding a constant to the series so that their minimum value was equal to 10 (29 occurrences across the whole dataset). In addition, any information that could possibly lead to the identification of the original series was removed so as to ensure the objectivity of the results. This included the starting dates of the series, which did not become available to the participants until the M4 had ended.\r\n\r\nSource: [The M4 competition: Results, findings, conclusion and way forward](https://doi.org/10.1016/j.ijforecast.2018.06.001)\r\nImage Source: [Are forecasting competitions data representative of the reality?](https://www.sciencedirect.com/science/article/abs/pii/S0169207019300159)", "price": 589, "created_at": "2025-01-09 13:44:40.889595", "keyword": ["Time Series Forecasting", "Meta-Learning"], "id": "6b989cc6-b10e-41d2-8856-4445c3d7e774", "image_url": "https://production-media.paperswithcode.com/datasets/M4-0000003351-6dd03d9b.jpg"}, {"title": "MAESTRO", "short_description": "The **MAESTRO** dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with \u223c3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1\u201348 kHz 16-bit PCM stereo).\r\n\r\nSource: [https://magenta.tensorflow.org/datasets/maestro](https://magenta.tensorflow.org/datasets/maestro)\r\nImage Source: [https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458](https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458)", "price": 793, "created_at": "2025-01-09 13:44:40.889683", "keyword": ["Audio Generation", "Music Transcription", "Music Generation"], "id": "7bd353fe-6aab-4fbf-a01c-04fef6ee5554", "image_url": "https://production-media.paperswithcode.com/datasets/MAESTRO-0000000390-bef70864_2MhcRKf.jpg"}, {"title": "MAFL", "short_description": "The **MAFL** dataset contains manually annotated facial landmark locations for 19,000 training and 1,000 test images.\r\n\r\nSource: [Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance](https://arxiv.org/abs/1806.06503)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html](http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html)", "price": 453, "created_at": "2025-01-09 13:44:40.889772", "keyword": ["Unsupervised Facial Landmark Detection"], "id": "04be7551-98df-41a4-8837-c1fada43e11c", "image_url": "https://production-media.paperswithcode.com/datasets/MAFL-0000001132-0319c46f_EQwm3M5.jpg"}, {"title": "MALF", "short_description": "The **MALF** dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.\n\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\nImage Source: [http://www.cbsr.ia.ac.cn/faceevaluation/](http://www.cbsr.ia.ac.cn/faceevaluation/)", "price": 138, "created_at": "2025-01-09 13:44:40.889871", "keyword": ["Object Detection", "Face Detection", "Robust Face Recognition"], "id": "bdc00fef-cba3-47f2-8c3c-8fb84ca02512", "image_url": "https://production-media.paperswithcode.com/datasets/MALF-0000003486-d8e1da4f.jpg"}, {"title": "MARS", "short_description": "**MARS** (**Motion Analysis and Re-identification Set**) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\r\n\r\nSource: [Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets](https://arxiv.org/abs/1706.06196)", "price": 199, "created_at": "2025-01-09 13:44:40.889961", "keyword": ["Person Re-Identification", "Unsupervised Person Re-Identification", "Video-Based Person Re-Identification", "Person Re-Identification", "Unsupervised Person Re-Identification", "Video-Based Person Re-Identification"], "id": "799ed9d8-5c87-418a-846a-0fac57e96ac2", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.46.57_PM.png"}, {"title": "MCScript", "short_description": "**MCScript** is used as the official dataset of SemEval2018 Task11. This dataset constructs a collection of text passages about daily life activities and a series of questions referring to each passage, and each question is equipped with two answer choices. The MCScript comprises 9731, 1411, and 2797 questions in training, development, and test set respectively.\r\n\r\nSource: [Multi-Perspective Fusion Network for Commonsense Reading Comprehension](https://arxiv.org/abs/1901.02257)\r\nImage Source: [https://arxiv.org/pdf/1803.05223.pdf](https://arxiv.org/pdf/1803.05223.pdf)", "price": 694, "created_at": "2025-01-09 13:44:40.890055", "keyword": ["Common Sense Reasoning", "Reading Comprehension", "Natural Language Understanding"], "id": "ca61388d-ed97-464a-8bbb-c7219a471fd8", "image_url": "https://production-media.paperswithcode.com/datasets/MCScript-0000003556-0a6f41ae.jpg"}, {"title": "MCTest", "short_description": "MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. \r\n\r\nMCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.\r\n\r\nSource: [MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text](https://www.aclweb.org/anthology/D13-1020.pdf)\r\nImage Source: [Richardson et al](https://www.aclweb.org/anthology/D13-1020)", "price": 495, "created_at": "2025-01-09 13:44:40.890146", "keyword": ["Question Answering", "Reading Comprehension", "Machine Reading Comprehension"], "id": "ca89138e-0070-4bfd-8774-31da3f916fbb", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_14.24.26.png"}, {"title": "MELD", "short_description": "**Multimodal EmotionLines Dataset** (**MELD**) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance.\r\n\r\nSource: [https://affective-meld.github.io/](https://affective-meld.github.io/)\r\nImage Source: [https://affective-meld.github.io/](https://affective-meld.github.io/)", "price": 388, "created_at": "2025-01-09 13:44:40.890236", "keyword": ["Speech Emotion Recognition", "Emotion Recognition in Conversation", "Multimodal Emotion Recognition"], "id": "c349e163-f62c-4801-99bf-7d070e463bdd", "image_url": "https://production-media.paperswithcode.com/datasets/MELD-0000002446-92eabb33_pYXU2aT.jpg"}, {"title": "MHP", "short_description": "The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting.\r\n\r\nSource: [Multiple-Human Parsing in the Wild](https://arxiv.org/pdf/1705.07206)\r\nImage Source: [Li et al](https://arxiv.org/pdf/1705.07206.pdf)", "price": 828, "created_at": "2025-01-09 13:44:40.890325", "keyword": ["Semantic Segmentation", "Human Part Segmentation", "Human Parsing", "Multi-Human Parsing"], "id": "dd9a7e27-99cd-4798-9361-5ad95d24b172", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_13.14.04.png"}, {"title": "MINC", "short_description": "MINC is a large-scale, open dataset of materials in the wild.\r\n\r\nSource: [Material Recognition in the Wild with the Materials in Context Database](https://arxiv.org/pdf/1412.0623v2.pdf)\r\nImage Source: [Bell et al](https://arxiv.org/pdf/1412.0623v2.pdf)", "price": 231, "created_at": "2025-01-09 13:44:40.890414", "keyword": ["Image Classification", "Material Classification", "Material Recognition"], "id": "a414499b-7a16-41d7-84e1-db7d988bd8a4", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_15.44.17.png"}, {"title": "MLDoc", "short_description": "**Multilingual Document Classification Corpus** (**MLDoc**) is a cross-lingual document classification dataset covering English, German, French, Spanish, Italian, Russian, Japanese and Chinese. It is a subset of the Reuters Corpus Volume 2 selected according to the following design choices:\r\n\r\n* uniform class coverage: same number of examples for each class and language,\r\n* official train / development / test split: for each language a training data of different sizes (1K, 2K, 5K and 10K stories), a development (1K) and a test corpus (4K) are provided (with exception of Spanish and Russian with 9458 and 5216 training documents respectively.\r\n\r\nSource: [A Corpus for Multilingual Document Classification in Eight Languages](https://paperswithcode.com/paper/a-corpus-for-multilingual-document/)", "price": 410, "created_at": "2025-01-09 13:44:40.890502", "keyword": ["Cross-Lingual Document Classification", "Cross-Lingual Sentiment Classification", "Cross-Lingual Document Classification", "Cross-Lingual Sentiment Classification"], "id": "9c3fa461-e7d3-41b6-afd1-b7e87f068d7e", "image_url": ""}, {"title": "MMI", "short_description": "The **MMI** Facial Expression Database consists of over 2900 videos and high-resolution still images of 75 subjects. It is fully annotated for the presence of AUs in videos (event coding), and partially coded on frame-level, indicating for each frame whether an AU is in either the neutral, onset, apex or offset phase. A small part was annotated for audio-visual laughters.\r\n\r\nSource: [https://mmifacedb.eu/](https://mmifacedb.eu/)\r\nImage Source: [https://mmifacedb.eu/](https://mmifacedb.eu/)", "price": 637, "created_at": "2025-01-09 13:44:40.890591", "keyword": ["Facial Expression Recognition (FER)", "Facial Expression Recognition", "Facial Expression Recognition (FER)", "Facial Expression Recognition"], "id": "b007f899-4255-49a9-8844-9f731353a638", "image_url": "https://production-media.paperswithcode.com/datasets/MMI-0000000267-88013726_NDinLAR.jpg"}, {"title": "MNIST", "short_description": "The **MNIST** database (**Modified National Institute of Standards and Technology** database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\r\n\r\nSource: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\r\nImage Source: [https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png)", "price": 105, "created_at": "2025-01-09 13:44:40.890681", "keyword": ["Image Classification", "Image Generation", "Speech Recognition", "Question Answering", "Text Classification", "Token Classification", "Automatic Speech Recognition", "Domain Adaptation", "Anomaly Detection", "Graph Classification", "Continual Learning", "Sequence-to-sequence Language Modeling", "Image Clustering", "Fine-Grained Image Classification", "Summarization", "Neural Architecture Search", "Video Prediction", "Unsupervised Anomaly Detection", "Density Estimation", "Core set discovery", "Clustering Algorithms Evaluation", "Adversarial Defense", "Stochastic Optimization", "General Classification", "Unsupervised Image Classification", "Personalized Federated Learning", "Network Pruning", "Deep Clustering", "Multiview Clustering", "Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly", "Intent Classification", "NER", "Sequential Image Classification", "Continuously Indexed Domain Adaptation", "Classification with Binary Weight Network", "Model Poisoning", "Sparse Learning and binarization", "Unsupervised Image-To-Image Translation", "Hard-label Attack", "POS", "Fill Mask", "AbbreviationDetection", "Nature-Inspired Optimization Algorithm", "Structured Prediction", "One-Shot Learning", "Handwritten Digit Recognition", "Unsupervised MNIST", "Rotated MNIST", "Superpixel Image Classification", "TAG", "Multi Label Text Classification", "text2text-generation", "Optical Charater Recogntion", "SENTS", "Malicious Detection", "Adversarial Defense against FGSM Attack", "Iloko Speech Recognition", "SpaceInvadersNoFrameskip", "CEFR proficiency prediction", "Analogical questions", "STS", "Question-Generation", "NMT", "Animal-10 Classification", "WOLOF Speech Recognition"], "id": "f5f587dc-378f-409f-9268-99f36b9f9164", "image_url": "https://production-media.paperswithcode.com/datasets/MNIST-0000000001-2e09631a_09liOmx.jpg"}, {"title": "MNIST-M", "short_description": "**MNIST-M** is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images.\r\n\r\nSource: [A Review of Single-Source Deep Unsupervised Visual Domain Adaptation](https://arxiv.org/abs/2009.00155)\r\nImage Source: [https://arxiv.org/pdf/1505.07818v4.pdf](https://arxiv.org/pdf/1505.07818v4.pdf)", "price": 717, "created_at": "2025-01-09 13:44:40.890769", "keyword": ["Domain Adaptation"], "id": "99aa8133-08f0-48af-abfd-35c082bcdfed", "image_url": "https://production-media.paperswithcode.com/datasets/MNIST-M-0000003425-911fa8c2.jpg"}, {"title": "MORPH", "short_description": "**MORPH** is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old.\r\n\r\nSource: [Deep Ordinal Regression Forests](https://arxiv.org/abs/2008.03077)\r\nImage Source: [https://uncw.edu/oic/tech/morph.html](https://uncw.edu/oic/tech/morph.html)", "price": 854, "created_at": "2025-01-09 13:44:40.890857", "keyword": ["Face Recognition", "Age Estimation", "Facial Attribute Classification", "Age-Invariant Face Recognition", "Fairness", "Few-shot Age Estimation", "Face Recognition", "Age Estimation", "Facial Attribute Classification", "Age-Invariant Face Recognition", "Fairness", "Few-shot Age Estimation"], "id": "4948f9fd-1a5e-412a-9ccf-e5daff6f1ef4", "image_url": "https://production-media.paperswithcode.com/datasets/MORPH-0000000550-3e9dcead_4sImAxL.jpg"}, {"title": "MOT15", "short_description": "MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector.\r\n\r\nSource: [FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking](https://arxiv.org/abs/1904.04989)\r\nImage Source: [https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377](https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377)", "price": 503, "created_at": "2025-01-09 13:44:40.890945", "keyword": ["Multi-Object Tracking", "Online Multi-Object Tracking"], "id": "ed2c3d67-3e6a-43c9-aa14-aba97dfe6fdb", "image_url": "https://production-media.paperswithcode.com/datasets/MOT15-0000002169-86c96c2c_bJaRwo4.jpg"}, {"title": "MOT16", "short_description": "The **MOT16** dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful.\r\n\r\nSource: [SOT for MOT](https://arxiv.org/abs/1712.01059)\r\nImage Source: [https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746](https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746)", "price": 57, "created_at": "2025-01-09 13:44:40.891033", "keyword": ["Multi-Object Tracking", "Online Multi-Object Tracking"], "id": "a8780f1a-97ec-47de-93a0-25d460fe6741", "image_url": "https://production-media.paperswithcode.com/datasets/MOT16-0000000376-93154a4f_YOWw0Ds.jpg"}, {"title": "MOT17", "short_description": "The **Multiple Object Tracking 17** (**MOT17**) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks.\r\n\r\nSource: [Deep Affinity Network for Multiple Object Tracking](https://arxiv.org/abs/1810.11780)\r\nImage Source: [https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502](https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502)", "price": 246, "created_at": "2025-01-09 13:44:40.891127", "keyword": ["Multi-Object Tracking", "Online Multi-Object Tracking"], "id": "8c5e0d5c-e98b-4395-bab5-41b6b3c182f2", "image_url": "https://production-media.paperswithcode.com/datasets/MOT17-0000001559-7f15ee55_GU5qqj9.jpg"}, {"title": "MOTChallenge", "short_description": "The **MOTChallenge** datasets are designed for the task of multiple object tracking. There are several variants of the dataset released each year, such as MOT15, MOT17, MOT20.\r\n\r\nSource: [MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking](https://arxiv.org/pdf/1504.01942v1.pdf)", "price": 461, "created_at": "2025-01-09 13:44:40.891219", "keyword": ["Object Tracking", "Multi-Object Tracking", "Multiple Object Tracking", "Online Multi-Object Tracking", "Multiple Object Tracking with Transformer"], "id": "8205bbea-ad3a-4fc2-b49a-7c011589075a", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_12.33.32_PM.png"}, {"title": "MPI Sintel", "short_description": "MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024\u00d7436 pixels and 8-bit per channel.\r\n\r\nSource: [Fast Disparity Estimation using Dense Networks*](https://arxiv.org/abs/1805.07499)", "price": 802, "created_at": "2025-01-09 13:44:40.891307", "keyword": ["Video Prediction", "Optical Flow Estimation", "Temporal View Synthesis", "Style Transfer", "Intrinsic Image Decomposition"], "id": "2c2391e7-9283-4ab9-a291-1835f5061f18", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_12.28.38_PM.png"}, {"title": "MPII", "short_description": "The **MPII** Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints.\r\n\r\nSource: [2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning](https://arxiv.org/abs/1802.09232)\r\nImage Source: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)", "price": 894, "created_at": "2025-01-09 13:44:40.891395", "keyword": ["Pose Estimation", "Temporal Action Localization", "Multi-Person Pose Estimation", "Keypoint Detection"], "id": "d1923509-3337-4a36-b435-2125f4ca52aa", "image_url": "https://production-media.paperswithcode.com/datasets/MPII-0000000344-d375bad7_ef0jlvO.jpg"}, {"title": "MPII Human Pose", "short_description": "**MPII Human Pose** Dataset is a dataset for human pose estimation. It consists of around 25k images extracted from online videos. Each image contains one or more people, with over 40k people annotated in total. Among the 40k samples, \u223c28k samples are for training and the remainder are for testing. Overall the dataset covers 410 human activities and each image is provided with an activity label. Images were extracted from a YouTube video and provided with preceding and following un-annotated frames.\r\n\r\nSource: [Accelerating Large-Kernel Convolution Using Summed-Area Tables](https://arxiv.org/abs/1906.11367)\r\nImage Source: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)", "price": 959, "created_at": "2025-01-09 13:44:40.891483", "keyword": ["Pose Estimation"], "id": "e507fad2-cc30-4dfa-b338-745aac9a965e", "image_url": "https://production-media.paperswithcode.com/datasets/MPII_Human_Pose-0000000466-3aef0279_icIDzP9.jpg"}, {"title": "MPQA Opinion Corpus", "short_description": "The **MPQA Opinion Corpus** contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).\r\n\r\nSource: [http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf](http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf)\r\nImage Source: [https://mpqa.cs.pitt.edu/](https://mpqa.cs.pitt.edu/)", "price": 529, "created_at": "2025-01-09 13:44:40.891571", "keyword": ["Sentiment Analysis", "Document Classification", "Keyword Extraction", "Opinion Mining", "Fine-Grained Opinion Analysis", "Sentiment Analysis", "Document Classification", "Keyword Extraction", "Opinion Mining", "Fine-Grained Opinion Analysis"], "id": "7d5a8175-7348-4cb5-be86-fd414bd97d2c", "image_url": "https://production-media.paperswithcode.com/datasets/MPQA_Opinion_Corpus-0000003365-9cca4006.jpg"}, {"title": "MR", "short_description": "**MR** Movie Reviews is a dataset for use in sentiment-analysis experiments. Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.\n\nSource: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\nImage Source: [https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png](https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png)", "price": 502, "created_at": "2025-01-09 13:44:40.891659", "keyword": ["Text Classification", "Sentiment Analysis", "Few-Shot Learning"], "id": "f824fa42-7b3b-4e80-a9fb-7f646edb7161", "image_url": "https://production-media.paperswithcode.com/datasets/MR-0000000765-3949451a_VlP0NNT.jpg"}, {"title": "MRPC", "short_description": "Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).\r\n\r\nSource: [Exploiting Semantic Annotations and Q-Learning for Constructing an Efficient Hierarchy/Graph Texts Organization](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313059/)\r\nImage Source: [https://www.aclweb.org/anthology/I05-5002.pdf](https://www.aclweb.org/anthology/I05-5002.pdf)", "price": 440, "created_at": "2025-01-09 13:44:40.891749", "keyword": ["Text Generation", "Few-Shot Learning", "Natural Language Inference", "Semantic Textual Similarity", "Semantic Textual Similarity within Bi-Encoder", "MRPC", "Text Generation", "Few-Shot Learning", "Natural Language Inference", "Semantic Textual Similarity", "Semantic Textual Similarity within Bi-Encoder", "MRPC"], "id": "da12485b-66e5-4e73-bb5f-9491149235c2", "image_url": "https://production-media.paperswithcode.com/datasets/MRPC-0000001009-dc06a901_IWCKdJb.jpg"}, {"title": "MS MARCO", "short_description": "The **MS MARCO** (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.\r\nThe first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.\r\n\r\nSource: [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/)\r\nImage Source: [https://arxiv.org/pdf/1809.08267.pdf](https://arxiv.org/pdf/1809.08267.pdf)", "price": 687, "created_at": "2025-01-09 13:44:40.891837", "keyword": ["Question Answering", "Reading Comprehension", "Information Retrieval", "Passage Retrieval", "Passage Re-Ranking", "TREC 2019 Passage Ranking", "Passage Ranking"], "id": "c3858d23-8d55-4a64-9cd4-0f5a4b95f1bc", "image_url": "https://production-media.paperswithcode.com/datasets/MS_MARCO-0000000205-06b435b2.jpg"}, {"title": "MSD", "short_description": "The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.\r\n\r\nThe core of the dataset is the feature analysis and metadata for one million songs, provided by The Echo Nest. The dataset does not include any audio, only the derived features. Note, however, that sample audio can be fetched from services like 7digital, using [code]( https://github.com/tbertinmahieux/MSongsDB/tree/master/Tasks_Demos/Preview7digital) provided by the authors.\r\n\r\n\r\nSource: [http://millionsongdataset.com/](http://millionsongdataset.com/)\r\nPaper: [The Million Song Dataset](https://doi.org/10.7916/D8NZ8J07)", "price": 548, "created_at": "2025-01-09 13:44:40.891937", "keyword": ["Recommendation Systems", "Music Auto-Tagging"], "id": "427b3543-790c-4443-bfe0-64bf51779fe1", "image_url": ""}, {"title": "MSRA Hand", "short_description": "**MSRA Hand**s is a dataset for hand tracking. In total 6 subjects' right hands are captured using Intel's Creative Interactive Gesture Camera. Each subject is asked to make various rapid gestures in a 400-frame video sequence. To account for different hand sizes, a global hand model scale is specified for each subject: 1.1, 1.0, 0.9, 0.95, 1.1, 1.0 for subject 1~6, respectively.\nThe camera intrinsic parameters are: principle point = image center(160, 120), focal length = 241.42. The depth image is 320x240, each *.bin file stores the depth pixel values in row scanning order, which are 320*240 floats. The unit is millimeters. The bin file is binary and needs to be opened with std::ios::binary flag.\njoint.txt file stores 400 frames x 21 hand joints per frame. Each line has 3 * 21 = 63 floats for 21 3D points in (x, y, z) coordinates. The 21 hand joints are: wrist, index_mcp, index_pip, index_dip, index_tip, middle_mcp, middle_pip, middle_dip, middle_tip, ring_mcp, ring_pip, ring_dip, ring_tip, little_mcp, little_pip, little_dip, little_tip, thumb_mcp, thumb_pip, thumb_dip, thumb_tip.\nThe corresponding *.jpg file is just for visualization of depth and ground truth joints.\n\nSource: [https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt](https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt)\nImage Source: [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf)", "price": 469, "created_at": "2025-01-09 13:44:40.892022", "keyword": ["Pose Estimation", "Hand Pose Estimation", "Stochastic Optimization"], "id": "c3a50480-bd27-440c-bca7-0aa607b4430b", "image_url": "https://production-media.paperswithcode.com/datasets/MSRA_Hand-0000003598-46e72593.jpg"}, {"title": "MSRA-B", "short_description": "The MSRA-B dataset is a dataset for salient object detection. It contains 5,000 images with a variety of image contents. Most of the images have a single salient object. There is a large variation among images including natural scenes, animals, indoor, outdoor, etc.\r\n\r\nSource: [Deep Contrast Learning for Salient Object Detection](https://arxiv.org/pdf/1603.01976.pdf)", "price": 982, "created_at": "2025-01-09 13:44:40.892118", "keyword": [], "id": "96592091-5d1e-45c7-bc81-1b2ec6dd0ac3", "image_url": ""}, {"title": "MSRA-TD500", "short_description": "The **MSRA-TD500** dataset is a text detection dataset that contains 300 training images and 200 test images. Text regions are arbitrarily orientated and annotated at sentence level. Different from the other datasets, it contains both English and Chinese text.\r\n\r\nSource: [Detecting Text in the Wild with Deep Character Embedding Network](https://arxiv.org/abs/1901.00363)\r\nImage Source: [http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg](http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg)", "price": 761, "created_at": "2025-01-09 13:44:40.892206", "keyword": ["Scene Text Detection"], "id": "ded5db85-5e72-4a97-a493-194ccc37aaca", "image_url": "https://production-media.paperswithcode.com/datasets/MSRA-TD500-0000000283-33cfea78_C88ZZW7.jpg"}, {"title": "MSRC-12", "short_description": "The Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system. The data set includes 594 sequences and 719,359 frames\u2014approximately six hours and 40 minutes\u2014collected from 30 people performing 12 gestures. In total, there are 6,244 gesture instances. The motion files contain tracks of 20 joints estimated using the Kinect Pose Estimation pipeline. The body poses are captured at a sample rate of 30Hz with an accuracy of about two centimeters in joint positions.\r\n\r\nSource: [https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/](https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/)", "price": 819, "created_at": "2025-01-09 13:44:40.892296", "keyword": ["Skeleton Based Action Recognition", "Gesture Recognition"], "id": "35ff3502-af9f-49f3-8588-8cf77fc98fd9", "image_url": ""}, {"title": "MSRDailyActivity3D", "short_description": "**DailyActivity3D** dataset is a daily activity dataset captured by a Kinect device. There are 16 activity types: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up, sit down. If possible, each subject performs an activity in two different poses: \u201csitting on sofa\u201d and \u201cstanding\u201d. The total number of the activity samples is 320.\r\nThis dataset is designed to cover human\u2019s daily activities in the living room. When the performer stands close to the sofa or sits on the sofa, the 3D joint positions extracted by the skeleton tracker are very noisy. Moreover, most of the activities involve the humans-object interactions. Thus this dataset is more challenging.\r\n\r\nSource: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf)\r\nImage Source: [https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition](https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition)", "price": 333, "created_at": "2025-01-09 13:44:40.892384", "keyword": ["Multimodal Activity Recognition"], "id": "ad0c9b2f-3200-4fd5-81ba-b0a224424141", "image_url": "https://production-media.paperswithcode.com/datasets/MSRDailyActivity3D-0000003421-6ab4cf4a.jpg"}, {"title": "MSVD", "short_description": "The **Microsoft Research Video Description Corpus** (**MSVD**) dataset consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data.\r\n\r\nSource: [https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F](https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F)\r\nImage Source: [https://arxiv.org/pdf/1609.06782.pdf](https://arxiv.org/pdf/1609.06782.pdf)", "price": 32, "created_at": "2025-01-09 13:44:40.892473", "keyword": ["Video Retrieval", "Video Captioning", "Zero-Shot Video Retrieval"], "id": "ab1fe61c-bd70-4ddc-9635-5724f4bf5dae", "image_url": "https://production-media.paperswithcode.com/datasets/MSVD-0000001109-dbac1b61_zOlqUSU.jpg"}, {"title": "MTNT", "short_description": "The Machine Translation of Noisy Text (**MTNT**) dataset is a Machine Translation dataset that consists of noisy comments on Reddit and professionally sourced translation. The translation are between French, Japanese and French, with between 7k and 37k sentence per language pair.\r\n\r\nSource: [https://arxiv.org/abs/1809.00388](https://arxiv.org/abs/1809.00388)\r\nImage Source: [https://github.com/pmichel31415/mtnt](https://github.com/pmichel31415/mtnt)", "price": 504, "created_at": "2025-01-09 13:44:40.892560", "keyword": ["Text Generation", "Domain Adaptation", "Machine Translation"], "id": "3b7b5ab9-451e-4b66-ab37-df5ff8fe7a66", "image_url": "https://production-media.paperswithcode.com/datasets/MTNT-0000004931-3a593559.gif"}, {"title": "MURA", "short_description": "A large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. \r\n\r\nSource: [MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs](/paper/mura-large-dataset-for-abnormality-detection)", "price": 635, "created_at": "2025-01-09 13:44:40.892645", "keyword": ["Anomaly Detection", "Decision Making"], "id": "faf2ad0d-cdde-451d-8c8e-5c1c6884bfca", "image_url": ""}, {"title": "MUSE", "short_description": "The **MUSE** dataset contains bilingual dictionaries for 110 pairs of languages. For each language pair, the training seed dictionaries contain approximately 5000 word pairs while the evaluation sets contain 1500 word pairs.\r\n\r\nSource: [Filtered Inner Product Projection for Multilingual Embedding Alignment](https://arxiv.org/abs/2006.03652)\r\nImage Source: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE)", "price": 737, "created_at": "2025-01-09 13:44:40.892730", "keyword": ["Machine Translation", "Word Alignment", "Word Embeddings"], "id": "139bc1e3-3c18-4f0c-b2aa-992d10e5c8fd", "image_url": "https://production-media.paperswithcode.com/datasets/MUSE-0000003621-dcee2a66.jpg"}, {"title": "Mall", "short_description": "The **Mall** is a dataset for crowd counting and profiling research. Its images are collected from publicly accessible webcam. It mainly includes 2,000 video frames, and the head position of every pedestrian in all frames is annotated. A total of more than 60,000 pedestrians are annotated in this dataset.\r\n\r\nSource: [Drone Based RGBT Vehicle Detection and Counting: A Challenge](https://arxiv.org/abs/2003.02437)\r\nImage Source: [http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf](http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf)", "price": 296, "created_at": "2025-01-09 13:44:40.892815", "keyword": ["Object Localization"], "id": "f9365dfc-9006-4854-974c-c88c7172fe14", "image_url": "https://production-media.paperswithcode.com/datasets/Mall-0000002115-bbc80123_o0RWU8r.jpg"}, {"title": "Manga109", "short_description": "**Manga109** has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library \u201cManga Library Z\u201d (formerly the \u201cZeppan Manga Toshokan\u201d library of out-of-print manga).\r\n\r\nSource: [Manga109](http://www.manga109.org/en/)\r\nImage Source: [https://arxiv.org/pdf/1510.04389v1.pdf](https://arxiv.org/pdf/1510.04389v1.pdf)", "price": 693, "created_at": "2025-01-09 13:44:40.892911", "keyword": ["Object Detection", "Image Super-Resolution", "Blind Super-Resolution", "Face Detection", "Body Detection", "Object Detection", "Image Super-Resolution", "Blind Super-Resolution", "Face Detection", "Body Detection"], "id": "cc79b969-926e-496e-b0fe-0f641c8304e6", "image_url": "https://production-media.paperswithcode.com/datasets/Manga109-0000003619-8b50d4ee.jpg"}, {"title": "Mapillary Vistas Dataset", "short_description": "Mapillary Vistas Dataset is a diverse street-level imagery dataset with pixel\u2011accurate and instance\u2011specific human annotations for understanding street scenes around the world.\r\n\r\nSource: [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas?lat=20&lng=0&z=1.5&pKey=pBBmjuJ8yU1r2ROYRzWmFg)\r\n\r\nImage Source: [Neuhold et al](https://openaccess.thecvf.com/content_ICCV_2017/papers/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.pdf)", "price": 424, "created_at": "2025-01-09 13:44:40.892994", "keyword": ["Semantic Segmentation", "Visual Place Recognition", "Panoptic Segmentation"], "id": "faf308f9-5ba1-4f56-83df-6b2a58fbf9bd", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-26_at_17.27.40.png"}, {"title": "Market-1501", "short_description": "**Market-1501** is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.\r\n\r\nSource: [A Survey of Pruning Methods for Efficient Person Re-identification Across Domains](https://arxiv.org/abs/1907.02547)\r\nImage Source: [https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification](https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification)", "price": 90, "created_at": "2025-01-09 13:44:40.893076", "keyword": ["Person Re-Identification", "Unsupervised Domain Adaptation", "Unsupervised Person Re-Identification", "Pose Transfer", "Generalizable Person Re-identification", "Person Re-Identification", "Unsupervised Domain Adaptation", "Unsupervised Person Re-Identification", "Pose Transfer", "Generalizable Person Re-identification"], "id": "f91dfec0-01bd-43f2-b091-f113d0eb9ae6", "image_url": "https://production-media.paperswithcode.com/datasets/Market-1501-0000000097-a728ab2d_gyNBlrI.jpg"}, {"title": "Matterport3D", "short_description": "The **Matterport3D** dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation.\r\n\r\nSource: [Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention](https://arxiv.org/abs/1812.04155)", "price": 620, "created_at": "2025-01-09 13:44:40.893159", "keyword": ["Semantic Segmentation", "Depth Estimation", "Monocular Depth Estimation", "Depth Completion", "Depth Prediction"], "id": "573b56c6-20ee-46cb-8dc9-7af7da1abaa1", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.21.59_PM.png"}, {"title": "McMaster", "short_description": "The **McMaster** dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500\u00d7500.\r\n\r\nSource: [FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising](https://arxiv.org/abs/1710.04026)\r\nImage Source: [https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf](https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf)", "price": 777, "created_at": "2025-01-09 13:44:40.893241", "keyword": ["Color Image Denoising", "Joint Demosaicing and Denoising", "Color Image Denoising", "Joint Demosaicing and Denoising"], "id": "db4854b7-7e7b-48ae-b9d7-bbb22b93d50a", "image_url": "https://production-media.paperswithcode.com/datasets/McMaster-0000003422-2c6d95c8.jpg"}, {"title": "MegaDepth", "short_description": "The MegaDepth dataset is a dataset for single-view depth prediction that includes 196 different locations reconstructed from COLMAP SfM/MVS.\r\n\r\nSource: [MegaDepth: Learning Single-View Depth Prediction from Internet Photos](/paper/megadepth-learning-single-view-depth)", "price": 892, "created_at": "2025-01-09 13:44:40.893323", "keyword": ["Depth Estimation", "3D Reconstruction", "Optical Flow Estimation"], "id": "dc46c031-556c-4f1e-8df7-9e893fb646aa", "image_url": "https://production-media.paperswithcode.com/datasets/demo1.png"}, {"title": "MegaFace", "short_description": "**MegaFace** was a publicly available dataset which is used for evaluating the performance of face recognition algorithms with up to a million distractors (i.e., up to a million people who are not in the test set). MegaFace contains 1M images from 690K individuals with unconstrained pose, expression, lighting, and exposure. MegaFace captures many different subjects rather than many images of a small number of subjects. The gallery set of MegaFace is collected from a subset of Flickr. The probe set of MegaFace used in the challenge consists of two databases; Facescrub and FGNet. FGNet contains 975 images of 82 individuals, each with several images spanning ages from 0 to 69. Facescrub dataset contains more than 100K face images of 530 people. The MegaFace challenge evaluates performance of face recognition algorithms by increasing the numbers of \u201cdistractors\u201d (going from 10 to 1M) in the gallery set. In order to evaluate the face recognition algorithms fairly, MegaFace challenge has two protocols including large or small training sets. If a training set has more than 0.5M images and 20K subjects, it is considered as large. Otherwise, it is considered as small.\r\n\r\n**NOTE**: This dataset [has been retired](https://exposing.ai/megaface/). \r\n\r\nSource: [A Deep Face Identification Network Enhanced by Facial Attributes Prediction](https://arxiv.org/abs/1805.00324)", "price": 37, "created_at": "2025-01-09 13:44:40.893405", "keyword": ["Face Verification", "Face Identification", "Disguised Face Verification"], "id": "23082432-30ba-4a63-8e4a-f79c6f4b7a5a", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.39.15_PM.png"}, {"title": "MemeTracker", "short_description": "The Memetracker corpus contains articles from mainstream media and blogs from August 1 to October 31, 2008 with about 1 million documents per day. It has 10,967 hyperlink cascades among 600 media sites.\n\nSource: [Marked Temporal Dynamics Modeling based on Recurrent Neural Network](https://arxiv.org/abs/1701.03918)\nImage Source: [http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html](http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html)", "price": 500, "created_at": "2025-01-09 13:44:40.893493", "keyword": ["Recommendation Systems", "Combinatorial Optimization", "Point Processes"], "id": "6e891614-3868-40c1-a834-36c6e0ab8183", "image_url": "https://production-media.paperswithcode.com/datasets/MemeTracker-0000003257-58fd1faa.jpg"}, {"title": "MetaQA", "short_description": "The **MetaQA** dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries.\r\n\r\nSource: [https://arxiv.org/abs/1907.08176](https://arxiv.org/abs/1907.08176)\r\nImage Source: [https://github.com/yuyuz/MetaQA](https://github.com/yuyuz/MetaQA)", "price": 695, "created_at": "2025-01-09 13:44:40.893576", "keyword": ["Question Answering", "Language Modelling", "Knowledge Graphs"], "id": "0559a24b-7e97-452b-a2c3-696ce8457ac1", "image_url": "https://production-media.paperswithcode.com/datasets/MetaQA-0000003577-228ef27a.jpg"}, {"title": "Middlebury", "short_description": "The **Middlebury** Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors.\r\n\r\nSource: [https://vision.middlebury.edu/stereo/data/](https://vision.middlebury.edu/stereo/data/)\r\nImage Source: [https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625](https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625)", "price": 708, "created_at": "2025-01-09 13:44:40.893662", "keyword": ["Image Super-Resolution", "Depth Estimation", "Video Frame Interpolation", "Stereo Image Super-Resolution", "Image Super-Resolution", "Depth Estimation", "Video Frame Interpolation", "Stereo Image Super-Resolution"], "id": "613492c6-d554-4b12-b568-50080e5d143f", "image_url": "https://production-media.paperswithcode.com/datasets/Middlebury-0000000336-e7d59d8f_bRDceq5.jpg"}, {"title": "Mindboggle", "short_description": "**Mindboggle** is a large publicly available dataset of manually labeled brain MRI. It consists of 101 subjects collected from different sites, with cortical meshes varying from 102K to 185K vertices. Each brain surface contains 25 or 31 manually labeled parcels.\r\n\r\nSource: [Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface Data](https://arxiv.org/abs/1803.10336)\r\nImage Source: [https://mindboggle.info/data.html](https://mindboggle.info/data.html)", "price": 575, "created_at": "2025-01-09 13:44:40.893745", "keyword": ["Domain Adaptation", "Graph Matching", "Graph Learning"], "id": "b6aa6440-35d4-496b-a694-a14e3a8c0173", "image_url": "https://production-media.paperswithcode.com/datasets/Mindboggle-0000003313-a169a0d5.jpg"}, {"title": "MineRL", "short_description": "**MineRL**is an imitation learning dataset with over 60 million frames of recorded human player data. The dataset includes a set of tasks which highlights many of the hardest problems in modern-day Reinforcement Learning: sparse rewards and hierarchical policies.\r\n\r\nSource: [MineRL](https://minerl.io/)", "price": 518, "created_at": "2025-01-09 13:44:40.893828", "keyword": ["Visual Navigation", "Imitation Learning"], "id": "2a98cbd2-5a46-4d7b-befa-04fd5e3e4dfb", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-11_at_19.39.38.png"}, {"title": "ModelNet", "short_description": "The **ModelNet**40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.\r\n\r\nSource: [Geometric Feedback Network for Point Cloud Classification](https://arxiv.org/abs/1911.12885)", "price": 99, "created_at": "2025-01-09 13:44:40.893922", "keyword": ["3D Point Cloud Classification", "3D Object Recognition", "Few-Shot 3D Point Cloud Classification", "3D Object Classification", "3D Point Cloud Data Augmentation", "Zero-Shot Transfer 3D Point Cloud Classification", "Few-Shot Point Cloud Classification", "3D Object Retrieval", "Training-free 3D Point Cloud Classification", "3D Point Cloud Linear Classification", "Zero-shot 3D Point Cloud Classification", "3D Point Cloud Classification", "3D Object Recognition", "Few-Shot 3D Point Cloud Classification", "3D Object Classification", "3D Point Cloud Data Augmentation", "Zero-Shot Transfer 3D Point Cloud Classification", "Few-Shot Point Cloud Classification", "3D Object Retrieval", "Training-free 3D Point Cloud Classification", "3D Point Cloud Linear Classification", "Zero-shot 3D Point Cloud Classification"], "id": "4c1e2d44-4f8c-4bfa-807b-5246908c52a6", "image_url": "https://production-media.paperswithcode.com/datasets/modelnet.jpeg"}, {"title": "MovieLens", "short_description": "The **MovieLens** datasets, first released in 1998, describe people\u2019s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 \u2014 a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations.\r\n\r\nSource: [The MovieLens Datasets: History and Context](http://files.grouplens.org/papers/harper-tiis2015.pdf)\r\nImage Source: [http://files.grouplens.org/papers/harper-tiis2015.pdf](http://files.grouplens.org/papers/harper-tiis2015.pdf)", "price": 810, "created_at": "2025-01-09 13:44:40.894002", "keyword": ["Link Prediction", "Recommendation Systems", "Knowledge Graph Completion", "Click-Through Rate Prediction", "Explainable Recommendation", "Recommendation Systems (Item cold-start)", "Movie Recommendation", "Link Prediction", "Recommendation Systems", "Knowledge Graph Completion", "Click-Through Rate Prediction", "Explainable Recommendation", "Recommendation Systems (Item cold-start)", "Movie Recommendation"], "id": "2a798183-b232-4dd2-8f0d-6584b589adfe", "image_url": "https://production-media.paperswithcode.com/datasets/MovieLens-0000000331-d5657d5d_KgRuquW.jpg"}, {"title": "MovieQA", "short_description": "The **MovieQA** dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\r\n\r\nSource: [Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents](https://arxiv.org/abs/1804.09412)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716](https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716)", "price": 60, "created_at": "2025-01-09 13:44:40.894082", "keyword": ["Video Question Answering", "Video Story QA"], "id": "27c42d4a-86b5-40d1-9865-71c2897ae1b2", "image_url": "https://production-media.paperswithcode.com/datasets/MovieQA-0000000179-f4df8b4e_xUcYFTa.jpg"}, {"title": "MuJoCo", "short_description": "**MuJoCo** (multi-joint dynamics with contact) is a physics engine used to implement environments to benchmark Reinforcement Learning methods.", "price": 490, "created_at": "2025-01-09 13:44:40.894162", "keyword": ["Multivariate Time Series Forecasting", "Multivariate Time Series Imputation"], "id": "0e9baaae-34f8-4f10-b512-f526110312c2", "image_url": "https://production-media.paperswithcode.com/datasets/mujoco_dBuTUMT.jpeg"}, {"title": "MuPoTS-3D", "short_description": "MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model.\r\n\r\nSource: [DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild](https://arxiv.org/abs/2008.09457)\r\nImage Source: [http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/](http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/)", "price": 378, "created_at": "2025-01-09 13:44:40.894243", "keyword": ["3D Multi-Person Pose Estimation", "3D Multi-Person Pose Estimation (root-relative)", "3D Multi-Person Pose Estimation (absolute)", "Unsupervised 3D Multi-Person Pose Estimation"], "id": "37929ff2-ebde-46c7-8c75-9169ad0429a7", "image_url": "https://production-media.paperswithcode.com/datasets/MuPoTS-3D-0000001113-a52b4629_9omc3hu.jpg"}, {"title": "Multi-Domain Sentiment Dataset v2.0", "short_description": "The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.\r\n\r\nSource: [Multi-Domain Sentiment Dataset (version 2.0)](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/)", "price": 320, "created_at": "2025-01-09 13:44:40.894323", "keyword": ["Sentiment Analysis"], "id": "2cf40e0a-74df-4ab2-8059-924dcce5b09d", "image_url": ""}, {"title": "MultiMNIST", "short_description": "The **MultiMNIST** dataset is generated from MNIST. The training and tests are generated by overlaying a digit on top of another digit from the same set (training or test) but different class. Each digit is shifted up to 4 pixels in each direction resulting in a 36\u00d736 image. Considering a digit in a 28\u00d728 image is bounded in a 20\u00d720 box, two digits bounding boxes on average have 80% overlap. For each digit in the MNIST dataset 1,000 MultiMNIST examples are generated, so the training set size is 60M and the test set size is 10M.\r\n\r\nSource: [https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)\r\nImage Source: [Sabour et al](https://arxiv.org/pdf/1710.09829v2.pdf)", "price": 252, "created_at": "2025-01-09 13:44:40.894403", "keyword": ["Image Classification"], "id": "ab6c5a69-32ee-4782-9e55-4eab9ae1c4ac", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_15.46.56.png"}, {"title": "MultiNLI", "short_description": "The **Multi-Genre Natural Language Inference** (**MultiNLI**) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like [SNLI](/dataset/snli). MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.\r\n\r\nSource: [Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information](https://arxiv.org/abs/1805.11360)", "price": 974, "created_at": "2025-01-09 13:44:40.894483", "keyword": ["Text Generation", "Natural Language Inference", "Faithfulness Critic", "MNLI-m", "MNLI-mm", "Text Generation", "Natural Language Inference", "Faithfulness Critic", "MNLI-m", "MNLI-mm"], "id": "0fe38346-8f93-4bad-8a38-4c11cc4bb1d1", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.17.20_PM.png"}, {"title": "MultiRC", "short_description": "**MultiRC** (**Multi-Sentence Reading Comprehension**) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph.\r\nThe dataset was designed with three key challenges in mind:\r\n* The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.\r\n* The correct answer(s) is not required to be a span in the text.\r\n* The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.\r\nThe entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time.\r\n\r\nSource: [https://cogcomp.seas.upenn.edu/multirc/](https://cogcomp.seas.upenn.edu/multirc/)\r\nImage Source: [https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/](https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/)", "price": 760, "created_at": "2025-01-09 13:44:40.894567", "keyword": ["Text Generation", "Question Answering", "Reading Comprehension"], "id": "caaac673-76bf-46f3-b078-147a2bf2bdf9", "image_url": "https://production-media.paperswithcode.com/datasets/MultiRC-0000000206-876f3342_Bjz1Stv.jpg"}, {"title": "MultiTHUMOS", "short_description": "The **MultiTHUMOS** dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\r\n\r\nSource: [http://ai.stanford.edu/~syyeung/everymoment.html](http://ai.stanford.edu/~syyeung/everymoment.html)\r\nImage Source: [http://ai.stanford.edu/~syyeung/everymoment.html](http://ai.stanford.edu/~syyeung/everymoment.html)", "price": 527, "created_at": "2025-01-09 13:44:40.894651", "keyword": ["Action Recognition", "Temporal Action Localization", "Action Detection"], "id": "fbde9ff0-aff9-44fa-89d5-2f3fa0b3d92a", "image_url": "https://production-media.paperswithcode.com/datasets/MultiTHUMOS-0000003419-395f7b01.jpg"}, {"title": "MultiWOZ", "short_description": "The **Multi-domain Wizard-of-Oz** (**MultiWOZ**) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police.\r\n\r\nSource: [Contents](https://arxiv.org/abs/1905.07687)\r\n\r\nImage Source: [Zhang et al](https://www.researchgate.net/figure/Example-of-the-difference-between-dialogue-state-annotation-in-MultiWOZ-21-and-MultiWOZ_fig2_343022084)", "price": 976, "created_at": "2025-01-09 13:44:40.894733", "keyword": ["Text Generation", "Data-to-Text Generation", "Dialogue Generation", "Natural Language Understanding", "Dialogue State Tracking", "Task-Oriented Dialogue Systems", "Multi-domain Dialogue State Tracking", "dialog state tracking", "End-To-End Dialogue Modelling"], "id": "0e751f30-ce54-4af2-879a-1ee4569b3879", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.10.26_PM.png"}, {"title": "Multimodal Opinionlevel Sentiment Intensity", "short_description": "Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features.\r\n\r\nSource: [Zadeh et al](https://arxiv.org/pdf/1606.06259.pdf)\r\n\r\nImage source: [Zadeh et al](https://arxiv.org/pdf/1606.06259.pdf)", "price": 69, "created_at": "2025-01-09 13:44:40.894815", "keyword": ["Multimodal Sentiment Analysis"], "id": "c9de6b1c-6496-41c5-bae9-fc7922657e98", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-18_at_09.52.26.png"}, {"title": "MusicNet", "short_description": "MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.\r\n\r\nSource: [MusicNet](https://homes.cs.washington.edu/~thickstn/musicnet.html)", "price": 251, "created_at": "2025-01-09 13:44:40.894908", "keyword": ["Music Transcription"], "id": "e46c2977-8e9a-4f8d-ae88-9cb49f1f770f", "image_url": "https://production-media.paperswithcode.com/datasets/musicnet.png"}, {"title": "NAB", "short_description": "**The First Temporal Benchmark Designed to Evaluate  Real-time Anomaly Detectors Benchmark**\r\n\r\nThe growth of the Internet of Things has created an abundance of streaming data. Finding anomalies in this data can provide valuable insights into opportunities or failures. Yet it\u2019s difficult to achieve, due to the need to process data in real time, continuously learn and make predictions. How do we evaluate and compare various real-time anomaly detection techniques? \r\n\r\nThe Numenta Anomaly Benchmark (NAB) provides a standard, open source framework for evaluating real-time anomaly detection algorithms on streaming data. Through a controlled, repeatable environment of open-source tools, NAB rewards detectors that find anomalies as soon as possible, trigger no false alarms, and automatically adapt to any changing statistics. \r\n\r\nNAB comprises two main components: a scoring system designed for streaming data and a dataset with labeled, real-world time-series data.\r\n\r\nSource: [Evaluating Real-time Anomaly Detection Algorithms \u2013 the Numenta Anomaly Benchmark](http://arxiv.org/abs/1510.03336)\r\nImage Source: [https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/](https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/)", "price": 475, "created_at": "2025-01-09 13:44:40.894985", "keyword": ["Anomaly Detection"], "id": "93df4553-7e46-4bbc-9a98-ea9ae94ef3ac", "image_url": "https://production-media.paperswithcode.com/datasets/nab.png"}, {"title": "NABirds", "short_description": "**NABirds** V1 is a collection of 48,000 annotated photographs of the 400 species of birds that are commonly observed in North America. More than 100 photographs are available for each species, including separate annotations for males, females and juveniles that comprise 700 visual categories. This dataset is to be used for fine-grained visual categorization experiments.\r\n\r\nSource: [https://dl.allaboutbirds.org/nabirds](https://dl.allaboutbirds.org/nabirds)\r\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf)", "price": 125, "created_at": "2025-01-09 13:44:40.895063", "keyword": ["Fine-Grained Image Classification", "Fine-Grained Image Classification"], "id": "472d196e-f443-40cd-826e-3825eb34a182", "image_url": "https://production-media.paperswithcode.com/datasets/NABirds-0000000575-b689a48f_WHYxXHx.jpg"}, {"title": "NCBI Disease", "short_description": "The **NCBI Disease** corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.\r\n\r\nSource: [A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization](https://arxiv.org/abs/1812.06081)", "price": 133, "created_at": "2025-01-09 13:44:40.895140", "keyword": ["Token Classification", "Named Entity Recognition (NER)", "Named Entity Recognition"], "id": "3db58a6f-472a-4f1d-974e-a4a67bbf0bb8", "image_url": ""}, {"title": "NCI1", "short_description": "The **NCI1** dataset comes from the cheminformatics domain, where each input graph is used as representation of a chemical compound: each vertex stands for an atom of the molecule, and edges between vertices represent bonds between atoms. This dataset is relative to anti-cancer screens where the chemicals are assessed as positive or negative to cell lung cancer. Each vertex has an input label representing the corresponding atom type, encoded by a one-hot-encoding scheme into a vector of 0/1 elements.\r\n\r\nSource: [Ring Reservoir Neural Networks for Graphs](https://arxiv.org/abs/2005.05294)", "price": 113, "created_at": "2025-01-09 13:44:40.895222", "keyword": ["Graph Classification", "Graph Classification"], "id": "d0f6e161-0fc9-46dc-b6af-09a427e3be12", "image_url": ""}, {"title": "NCLT", "short_description": "The **NCLT** dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan\u2019s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects.\r\n\r\nSource: [http://robots.engin.umich.edu/nclt/nclt.pdf](http://robots.engin.umich.edu/nclt/nclt.pdf)\r\nImage Source: [http://robots.engin.umich.edu/nclt/](http://robots.engin.umich.edu/nclt/)", "price": 657, "created_at": "2025-01-09 13:44:40.895301", "keyword": ["Pose Estimation", "Visual Place Recognition", "Visual Localization"], "id": "a33f2230-f166-4402-b713-0b37ecbe2a14", "image_url": "https://production-media.paperswithcode.com/datasets/NCLT-0000003614-ec11889d.jpg"}, {"title": "NELL", "short_description": "**NELL** is a dataset built from the Web via an intelligent agent called Never-Ending Language Learner. This agent attempts to learn over time to read the web. NELL has accumulated over 50 million candidate beliefs by reading the web, and it is considering these at different levels of confidence. NELL has high confidence in 2,810,379 of these beliefs.\r\n\r\nSource: [A Survey on Knowledge Graphs: Representation, Acquisition and Applications](https://arxiv.org/abs/2002.00388)\r\nImage Source: [http://rtw.ml.cmu.edu/rtw/](http://rtw.ml.cmu.edu/rtw/)", "price": 994, "created_at": "2025-01-09 13:44:40.895380", "keyword": ["Node Classification"], "id": "a748dd2d-b288-40fb-b771-2a47d74858f1", "image_url": "https://production-media.paperswithcode.com/datasets/NELL-0000000702-dfdbf2be_KSH5wbQ.jpg"}, {"title": "NEWSROOM", "short_description": "CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.\r\n\r\nSource: [CORNELL NEWSROOM](http://lil.nlp.cornell.edu/newsroom/)", "price": 378, "created_at": "2025-01-09 13:44:40.895459", "keyword": ["Text Summarization", "Abstractive Text Summarization", "Document Summarization"], "id": "815f5f03-9f65-40d8-a42f-fa49ef80ee6d", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_19.16.38.png"}, {"title": "NIST SD 19", "short_description": "NIST Special Database 19 contains NIST's entire corpus of training materials for handprinted document and character recognition. It publishes Handprinted Sample Forms from 3600 writers, 810,000 character images isolated from their forms, ground truth classifications for those images, reference forms for further data collection, and software utilities for image management and handling.\n\nSource: [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19)\nImage Source: [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19)", "price": 720, "created_at": "2025-01-09 13:44:40.895538", "keyword": [], "id": "08a6b6a8-2a81-4d6e-902a-dbaba8d45db2", "image_url": "https://production-media.paperswithcode.com/datasets/NIST_SD_19-0000003634-5454e102.jpg"}, {"title": "NLPR", "short_description": "The **NLPR** dataset for salient object detection consists of 1,000 image pairs captured by a standard Microsoft Kinect with a resolution of 640\u00d7480. The images include indoor and outdoor scenes (e.g., offices, campuses, streets and supermarkets).\r\n\r\nSource: [Bifurcated Backbone Strategy for RGB-D Salient Object Detection](https://arxiv.org/abs/2007.02713)\r\nImage Source: [https://sites.google.com/site/rgbdsaliency/dataset](https://sites.google.com/site/rgbdsaliency/dataset)", "price": 456, "created_at": "2025-01-09 13:44:40.895620", "keyword": ["RGB-D Salient Object Detection"], "id": "cfeb9c5a-1197-46c7-8309-fc8dd5e4691d", "image_url": "https://production-media.paperswithcode.com/datasets/NLPR-0000002781-5cb4478d_2P8Remp.jpg"}, {"title": "NSynth", "short_description": "**NSynth** is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.\r\n\r\nSource: [Data Augmentation for Instrument Classification Robust to Audio Effects](https://arxiv.org/abs/1907.08520)\r\nImage Source: [https://magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth)", "price": 496, "created_at": "2025-01-09 13:44:40.895698", "keyword": ["Few-Shot Audio Classification", "Audio Generation", "Self-Supervised Learning", "Instrument Recognition", "Pitch Classification", "Music Generation"], "id": "07db6582-ea3d-4edd-87ad-40b1e30abdac", "image_url": "https://production-media.paperswithcode.com/datasets/NSynth-0000003672-74d8c0c7.jpg"}, {"title": "NYU Hand", "short_description": "The **NYU Hand** pose dataset contains 8252 test-set and 72757 training-set frames of captured RGBD data with ground-truth hand-pose information. For each frame, the RGBD data from 3 Kinects is provided: a frontal view and 2 side views. The training set contains samples from a single user only (Jonathan Tompson), while the test set contains samples from two users (Murphy Stein and Jonathan Tompson). A synthetic re-creation (rendering) of the hand pose is also provided for each view.\n\nSource: [https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm](https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm)\nImage Source: [https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm](https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm)", "price": 843, "created_at": "2025-01-09 13:44:40.895776", "keyword": ["Hand Pose Estimation"], "id": "8718bc3e-9f39-4135-9446-226876159106", "image_url": "https://production-media.paperswithcode.com/datasets/NYU_Hand-0000003665-9080c163.jpg"}, {"title": "NYUv2", "short_description": "The **NYU-Depth V2** data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:\r\n\r\n* 1449 densely labeled pairs of aligned RGB and depth images\r\n* 464 new scenes taken from 3 cities\r\n* 407,024 new unlabeled frames\r\n* Each object is labeled with a class and an instance number.\r\nThe dataset has several components:\r\n* Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.\r\n* Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.\r\n* Toolbox: Useful functions for manipulating the data and labels.\r\n\r\nSource: [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)\r\nImage Source: [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)", "price": 392, "created_at": "2025-01-09 13:44:40.895867", "keyword": ["Semantic Segmentation", "Instance Segmentation", "3D Object Detection", "Depth Estimation", "Panoptic Segmentation", "Monocular Depth Estimation", "Scene Segmentation", "Multi-Task Learning", "Depth Completion", "Real-Time Semantic Segmentation", "Surface Normals Estimation", "3D Semantic Scene Completion", "Boundary Detection", "3D Semantic Scene Completion from a single RGB image", "Surface Normal Estimation", "Scene Classification (unified classes)", "Plane Instance Segmentation", "Zero-shot Scene Classification (unified classes)", "Semantic Segmentation", "Instance Segmentation", "3D Object Detection", "Depth Estimation", "Panoptic Segmentation", "Monocular Depth Estimation", "Scene Segmentation", "Multi-Task Learning", "Depth Completion", "Real-Time Semantic Segmentation", "Surface Normals Estimation", "3D Semantic Scene Completion", "Boundary Detection", "3D Semantic Scene Completion from a single RGB image", "Surface Normal Estimation", "Scene Classification (unified classes)", "Plane Instance Segmentation", "Zero-shot Scene Classification (unified classes)"], "id": "c758de25-a60a-4b74-9d93-a6dc8de505b6", "image_url": "https://production-media.paperswithcode.com/datasets/NYUv2-0000003446-63769b25.jpg"}, {"title": "NarrativeQA", "short_description": "The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers.\r\n\r\nSource: [DeepMind](https://deepmind.com/research/open-source/narrativeqa)\r\nImage Source: [Ko\u010disk\u00fd et al ](https://arxiv.org/pdf/1712.07040v1.pdf)", "price": 44, "created_at": "2025-01-09 13:44:40.895943", "keyword": ["Question Answering", "Reading Comprehension", "Question Answering", "Reading Comprehension"], "id": "cfb0c152-e6ed-4807-b7ce-cea9291dee9a", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-12_at_09.40.53.png"}, {"title": "Netflix Prize", "short_description": "**Netflix Prize** consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5.\r\n\r\nSource: [The Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf)\r\nImage Source: [https://www.netflixprize.com/](https://www.netflixprize.com/)", "price": 751, "created_at": "2025-01-09 13:44:40.896019", "keyword": ["Recommendation Systems", "Fairness", "Matrix Completion"], "id": "4815ea57-1868-4302-b9ea-8467e2cff65d", "image_url": "https://production-media.paperswithcode.com/datasets/Netflix_Prize-0000003624-afc1adc7.jpg"}, {"title": "NewsQA", "short_description": "The **NewsQA** dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs.\r\n\r\n* Documents are CNN news articles.\r\n* Questions are written by human users in natural language.\r\n* Answers may be multiword passages of the source text.\r\n* Questions may be unanswerable.\r\n* NewsQA is collected using a 3-stage, siloed process.\r\n* Questioners see only an article\u2019s headline and highlights.\r\n* Answerers see the question and the full article, then select an answer passage.\r\n* Validators see the article, the question, and a set of answers that they rank.\r\n* NewsQA is more natural and more challenging than previous datasets.\r\n\r\nSource: [https://www.microsoft.com/en-us/research/project/newsqa-dataset/](https://www.microsoft.com/en-us/research/project/newsqa-dataset/)\r\nImage Source: [Trischler et al](https://arxiv.org/pdf/1611.09830v3.pdf)", "price": 608, "created_at": "2025-01-09 13:44:40.896095", "keyword": ["Question Answering", "Reading Comprehension"], "id": "018b8ccb-52a1-42f0-b6a3-8231e21b2bfb", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.12.56_PM.png"}, {"title": "Nottingham", "short_description": "The **Nottingham** Dataset is a collection of 1200 American and British folk songs.\n\nSource: [Rethinking Recurrent Latent Variable Model for Music Composition](https://arxiv.org/abs/1810.03226)\nImage Source: [https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/](https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/)", "price": 188, "created_at": "2025-01-09 13:44:40.896171", "keyword": ["Music Modeling"], "id": "bff02430-3612-4642-b0bf-56178237d7c5", "image_url": "https://production-media.paperswithcode.com/datasets/Nottingham-0000001071-1a926d96_S7ayxFp.jpg"}, {"title": "OCHuman", "short_description": "This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, **OCHuman** is the most complex and challenging dataset related to human.\n\nSource: [https://github.com/liruilong940607/OCHumanApi](https://github.com/liruilong940607/OCHumanApi)\nImage Source: [https://github.com/liruilong940607/OCHumanApi](https://github.com/liruilong940607/OCHumanApi)", "price": 207, "created_at": "2025-01-09 13:44:40.896247", "keyword": ["Pose Estimation", "2D Human Pose Estimation", "Multi-Person Pose Estimation", "Keypoint Detection", "Human Instance Segmentation"], "id": "d991a20b-c5da-48bf-833d-63042eeeb0c5", "image_url": "https://production-media.paperswithcode.com/datasets/OCHuman-0000000841-8de92bc9.jpeg"}, {"title": "OLID", "short_description": "The **OLID** is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and publicly available. There are 14,100 tweets in total, in which 13,240 are in the training set, and 860 are in the test set. For each tweet, there are three levels of labels: (A) Offensive/Not-Offensive, (B) Targeted-Insult/Untargeted, (C) Individual/Group/Other. The relationship between them is hierarchical. If a tweet is offensive, it can have a target or no target. If it is offensive to a specific target, the target can be an individual, a group, or some other objects. This dataset is used in the OffensEval-2019 competition in SemEval-2019.\r\n\r\nSource: [Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection](https://arxiv.org/abs/2004.13432)\r\nImage Source: [https://arxiv.org/pdf/1902.09666.pdf](https://arxiv.org/pdf/1902.09666.pdf)", "price": 120, "created_at": "2025-01-09 13:44:40.896322", "keyword": ["Language Modelling", "Hate Speech Detection", "Language Identification"], "id": "10a48d01-4f54-45c6-9101-02a4f57e947e", "image_url": "https://production-media.paperswithcode.com/datasets/OLID-0000003564-5ec985c4.jpg"}, {"title": "ORL", "short_description": "The **ORL** Database of Faces contains 400 images from 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel.\r\n\r\nSource: [https://cam-orl.co.uk/facedatabase.html](https://cam-orl.co.uk/facedatabase.html)\r\nDownload dataset from Kaggle: [https://www.kaggle.com/datasets/kasikrit/att-database-of-faces](https://www.kaggle.com/datasets/kasikrit/att-database-of-faces)\r\nImage Source: [https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition](https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition)", "price": 218, "created_at": "2025-01-09 13:44:40.896401", "keyword": ["Multi-view Subspace Clustering"], "id": "9cb91f49-61f7-4e0e-bb44-7c6450b75ab4", "image_url": "https://production-media.paperswithcode.com/datasets/ORL-0000001343-301c377a_olRLQ6o.jpg"}, {"title": "OTB", "short_description": "Object Tracking Benchmark (**OTB**) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. [OTB-2013](otb-2013) dataset contains 51 sequences and the [OTB-2015](otb-2015) dataset contains all 100 sequences of the OTB dataset.\r\n\r\nSource: [Deep Meta Learning for Real-Time Target-Aware Visual Tracking](https://arxiv.org/abs/1712.09153)", "price": 430, "created_at": "2025-01-09 13:44:40.896477", "keyword": ["Visual Object Tracking", "Visual Tracking"], "id": "f7daf05d-121a-4411-b38b-c14da2cbf436", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.20.30_PM.png"}, {"title": "ObjectNet", "short_description": "**ObjectNet** is a test set of images collected directly using crowd-sourcing. ObjectNet is unique as the objects are captured at unusual poses in cluttered, natural scenes, which can severely degrade recognition performance. There are 50,000 images in the test set which controls for rotation, background and viewpoint. There are 313 object classes with 113 overlapping ImageNet.\r\n\r\nSource: [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558)\r\nImage Source: [https://objectnet.dev/](https://objectnet.dev/)", "price": 152, "created_at": "2025-01-09 13:44:40.896553", "keyword": ["Image Classification", "Zero-Shot Transfer Image Classification", "Unsupervised Image Classification"], "id": "1c533aa6-89cf-4205-ae19-33664c2fbdbf", "image_url": "https://production-media.paperswithcode.com/datasets/ObjectNet-0000002301-79a702bc_h5WhB4b.jpg"}, {"title": "Office-31", "short_description": "The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288\u00d72848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640\u00d7480) exhibit significant noise and color as well as white balance artifacts.\r\n\r\nSource: [Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors](https://arxiv.org/abs/1611.08195)\r\nImage Source: [https://www.researchgate.net/publication/310953258](https://www.researchgate.net/publication/310953258)", "price": 408, "created_at": "2025-01-09 13:44:40.896632", "keyword": ["Domain Adaptation", "Unsupervised Domain Adaptation", "Multi-Source Unsupervised Domain Adaptation", "Partial Domain Adaptation", "Universal Domain Adaptation", "Multi-target Domain Adaptation", "Blended-target Domain Adaptation"], "id": "f1f8ed9f-3645-4a84-9447-9931ee532b59", "image_url": "https://production-media.paperswithcode.com/datasets/Office-31-0000000855-395ad845_FUUZpN7.jpg"}, {"title": "OmniArt", "short_description": "Presents half a million samples and structured meta-data to encourage further research and societal engagement.\r\n\r\nSource: [OmniArt: Multi-task Deep Learning for Artistic Data Analysis](/paper/omniart-multi-task-deep-learning-for-artistic)", "price": 367, "created_at": "2025-01-09 13:44:40.896708", "keyword": ["Multi-Task Learning"], "id": "5ec866d3-4c37-4fef-8224-230d793a77c9", "image_url": ""}, {"title": "Omniglot", "short_description": "The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.", "price": 952, "created_at": "2025-01-09 13:44:40.896784", "keyword": ["Few-Shot Image Classification", "Meta-Learning", "Density Estimation", "Multi-Task Learning", "Personalized Federated Learning", "One-Shot Segmentation"], "id": "fc5ed4bd-160a-4f99-80c2-740100a0b5ff", "image_url": ""}, {"title": "OntoNotes 5.0", "short_description": "**OntoNotes 5.0** is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).\r\n\r\nOntoNotes Release 5.0 contains the content of earlier releases - and adds source data from and/or additional annotations for, newswire, broadcast news, broadcast conversation, telephone conversation and web data in English and Chinese and newswire data in Arabic.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)", "price": 799, "created_at": "2025-01-09 13:44:40.896873", "keyword": ["Named Entity Recognition (NER)", "Coreference Resolution", "Named Entity Recognition", "Generalized Zero-Shot Learning", "Semantic Role Labeling", "Weakly-Supervised Named Entity Recognition", "Chinese Named Entity Recognition", "Entity Typing", "FG-1-PG-1", "coreference-resolution"], "id": "90fbf2d8-0e98-48bf-b499-bd8592702ccd", "image_url": ""}, {"title": "OpenBookQA", "short_description": "**OpenBookQA** is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small \u201cbook\u201d of 1,326 core science facts and the application of these facts to novel situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions, by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\r\nAdditionally, the dataset includes a collection of 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker ID.\r\n\r\nSource: [https://allenai.org/data/open-book-qa](https://allenai.org/data/open-book-qa)\r\nImage Source: [https://arxiv.org/pdf/1809.02789.pdf](https://arxiv.org/pdf/1809.02789.pdf)", "price": 906, "created_at": "2025-01-09 13:44:40.896947", "keyword": ["Text Generation", "Question Answering"], "id": "32da95de-d138-4611-bd62-9ee2c7346acb", "image_url": "https://production-media.paperswithcode.com/datasets/OpenBookQA-0000002426-0fb76b92_C0D71QL.jpg"}, {"title": "OpenEDS", "short_description": "OpenEDS (Open Eye Dataset) is a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eyefacing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. \r\n\r\nSource: [OpenEDS: Open Eye Dataset](/paper/190503702)\r\nImage Source: [https://research.fb.com/programs/openeds-challenge](https://research.fb.com/programs/openeds-challenge)", "price": 964, "created_at": "2025-01-09 13:44:40.897021", "keyword": ["Semantic Segmentation", "Gaze Estimation"], "id": "f01c4c3c-e4f3-4d08-b361-d6bca130e867", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_14.54.41.png"}, {"title": "OpenSubtitles", "short_description": "OpenSubtitles is collection of multilingual parallel corpora. The dataset is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages.", "price": 975, "created_at": "2025-01-09 13:44:40.897095", "keyword": ["Domain Adaptation", "Machine Translation", "Dialogue Generation", "Language Identification"], "id": "593a5061-a419-424a-9ed9-e011f3c35b71", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_12.26.15_PM.png"}, {"title": "Oxford 102 Flower", "short_description": "**Oxford 102 Flower** is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.\r\n\r\nThe images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories.", "price": 46, "created_at": "2025-01-09 13:44:40.897169", "keyword": ["Image Classification", "Image Generation", "Text-to-Image Generation", "Zero-Shot Learning", "Few-Shot Image Classification", "Few-Shot Learning", "Continual Learning", "Fine-Grained Image Classification", "Neural Architecture Search", "Prompt Engineering", "Generalized Zero-Shot Learning", "Point-interactive Image Colorization", "Unsupervised Image Segmentation"], "id": "f613a376-9429-487b-9bcf-ec70e1987e2b", "image_url": "https://production-media.paperswithcode.com/datasets/flowers.jpg"}, {"title": "Oxford105k", "short_description": "**Oxford105k** is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale.\r\n\r\nSource: [Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors Extended Version](https://arxiv.org/abs/1504.03285)", "price": 328, "created_at": "2025-01-09 13:44:40.897242", "keyword": ["Image Retrieval", "Instance Search", "Dimensionality Reduction"], "id": "5f1f0edd-6b39-4a52-919b-038490d68462", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_6.54.57_PM.png"}, {"title": "Oxford5k", "short_description": "Oxford5K is the **Oxford Buildings** Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark.\r\n\r\nSource: [Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN 1 Corresponding Author](https://arxiv.org/abs/1907.05793)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/](https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/)", "price": 293, "created_at": "2025-01-09 13:44:40.897317", "keyword": ["Image Classification", "Image Retrieval", "Content-Based Image Retrieval", "Image Classification", "Image Retrieval", "Content-Based Image Retrieval"], "id": "56972db2-6a84-43b1-ab8e-f2a2693ba294", "image_url": "https://production-media.paperswithcode.com/datasets/Oxford5k-0000003481-020ba221.jpg"}, {"title": "PACS", "short_description": "**PACS** is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories.\r\n\r\nSource: [Deep Domain-Adversarial Image Generation for Domain Generalisation](https://arxiv.org/abs/2003.06054)\r\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033](https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033)", "price": 319, "created_at": "2025-01-09 13:44:40.897390", "keyword": ["Domain Adaptation", "Unsupervised Domain Adaptation", "Domain Generalization", "Annotated Code Search", "Unsupervised Continual Domain Shift Learning"], "id": "d520b4cf-987d-4d29-a34a-5497551e3ce5", "image_url": "https://production-media.paperswithcode.com/datasets/PACS-0000001060-38db8f1d_ucFoTXn.jpg"}, {"title": "PASCAL Context", "short_description": "The **PASCAL Context** dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use.\r\n\r\nSource: [Image Segmentation Using Deep Learning:A Survey](https://arxiv.org/abs/2001.05566)\r\nImage Source: [https://cs.stanford.edu/~roozbeh/pascal-context/](https://cs.stanford.edu/~roozbeh/pascal-context/)", "price": 265, "created_at": "2025-01-09 13:44:40.897463", "keyword": ["Semantic Segmentation", "Zero-Shot Learning", "Saliency Detection", "Surface Normals Estimation", "Human Parsing", "Boundary Detection"], "id": "2e47b8d0-2401-4a21-bc2d-561f85491c73", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.16.24_PM.png"}, {"title": "PASCAL Face", "short_description": "The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.\r\n\r\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\r\nImage Source: [https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926](https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926)", "price": 967, "created_at": "2025-01-09 13:44:40.897536", "keyword": ["Face Detection"], "id": "5a749c8d-15a8-467e-9410-5deeed35fe03", "image_url": "https://production-media.paperswithcode.com/datasets/PASCAL_Face-0000000535-2677a168_6Q0sX7Q.jpg"}, {"title": "PASCAL VOC", "short_description": "The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The **PASCAL VOC** dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.\r\n\r\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)\r\nImage Source: [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg)", "price": 17, "created_at": "2025-01-09 13:44:40.897620", "keyword": ["Object Detection", "Semantic Segmentation", "Node Classification", "Interactive Segmentation", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Open Vocabulary Semantic Segmentation", "Graph Matching", "Single-object discovery", "Knowledge Distillation", "Class-agnostic Object Detection", "Single-object colocalization", "Zero-Shot Semantic Segmentation", "Multi-object discovery", "Talking Face Generation", "3D Face Animation", "Multi-object colocalization"], "id": "578b525e-3a0f-445f-abc8-5c6ee6641ee9", "image_url": "https://production-media.paperswithcode.com/datasets/PASCAL_VOC-0000003438-3c401c50.jpg"}, {"title": "PASCAL VOC 2007", "short_description": "**PASCAL VOC 2007** is a dataset for image recognition. The twenty object classes that have been selected are:\r\n\r\nPerson: person\r\nAnimal: bird, cat, cow, dog, horse, sheep\r\nVehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\r\nIndoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\r\n\r\nThe dataset can be used for image classification and object detection tasks.\r\n\r\nImage Source: [Object Detection and Recognition in Images](https://arxiv.org/abs/1708.01241)", "price": 324, "created_at": "2025-01-09 13:44:40.897696", "keyword": ["Image Classification", "Object Detection", "Semantic Segmentation", "Multi-Label Classification", "Object Localization", "Weakly Supervised Object Detection", "Cross-Modal Retrieval", "Unsupervised Semantic Segmentation with Language-image Pre-training", "Unsupervised Object Detection", "Object Counting", "Real-Time Object Detection", "Zero-Shot Object Detection", "Open World Object Detection", "Robust Object Detection", "Unsupervised Object Localization", "Multi-label Image Recognition with Partial Labels"], "id": "28b793ff-2d88-492e-8cf3-ca1b6248ec90", "image_url": "https://production-media.paperswithcode.com/datasets/pascalvoc.png"}, {"title": "PASCAL VOC 2011", "short_description": "**PASCAL VOC 2011** is an image segmentation dataset. It contains around 2,223 images for training, consisting of 5,034 objects. Testing consists of 1,111 images with 2,028 objects. In total there are over 5,000 precisely segmented objects for training.\r\n\r\nSource: [Scene Parsing with Integration of Parametric and Non-parametric Models](https://arxiv.org/abs/1604.05848)\r\nImage Source: [http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html](http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html)", "price": 15, "created_at": "2025-01-09 13:44:40.897770", "keyword": ["Semantic Segmentation"], "id": "6c3ed1b3-7757-4315-b271-2ff15e35dfd1", "image_url": "https://production-media.paperswithcode.com/datasets/PASCAL_VOC_2011-0000003607-e5ba18f5.jpg"}, {"title": "PASCAL-Part", "short_description": "**PASCAL-Part** is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object. For categories that do not have a consistent set of parts (e.g., boat), it provides the silhouette annotation. \r\n\r\nIt can also serve as a set for human semantic part segmentation: It contains multiple humans per image in unconstrained poses and occlusions (1,716 for training and 1,817 for testing). It provides careful pixel-wise annotations for six body parts (i.e., head, torso, upper/lower-arms, and upper-/lower-legs).\r\n\r\nSource: [The Ultimate Theory of Human Parsing](https://arxiv.org/abs/2001.06804)\r\nImage Source: [https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png](https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png)", "price": 550, "created_at": "2025-01-09 13:44:40.897856", "keyword": ["Object Detection", "Human Part Segmentation", "Multi-Human Parsing", "Semantic Part Detection"], "id": "9023776e-b4d9-48fb-b264-15a1b29a4120", "image_url": "https://production-media.paperswithcode.com/datasets/PASCAL-Person-Part-0000000698-ac94e52f_uSZwyng.jpg"}, {"title": "PASCAL-S", "short_description": "**PASCAL-S** is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes.\r\n\r\nSource: [Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection](https://arxiv.org/abs/1909.04366)", "price": 328, "created_at": "2025-01-09 13:44:40.897932", "keyword": ["RGB Salient Object Detection", "Saliency Detection", "Salient Object Detection"], "id": "a8005951-0d36-4e23-b661-7bed02f75fef", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_11.10.27_AM.png"}, {"title": "PASCAL3D+", "short_description": "The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.\r\n\r\nSource: [Convolutional Models for Joint Object Categorization and Pose Estimation](https://arxiv.org/abs/1511.05175)\r\nImage Source: [Beyond PASCAL: A benchmark for 3D object detection in the wild](https://doi.org/10.1109/WACV.2014.6836101)", "price": 872, "created_at": "2025-01-09 13:44:40.898005", "keyword": ["Object Detection", "Pose Estimation", "Keypoint Detection", "Viewpoint Estimation", "Object Detection", "Pose Estimation", "Keypoint Detection", "Viewpoint Estimation"], "id": "8f771eb5-c030-447d-b78e-454972f68f48", "image_url": "https://production-media.paperswithcode.com/datasets/PASCAL3D-0000003449-d52b0751.jpg"}, {"title": "PAWS", "short_description": "Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.\r\n\r\nSource: [PAWS](https://github.com/google-research-datasets/paws)", "price": 454, "created_at": "2025-01-09 13:44:40.898077", "keyword": ["Text Classification", "Sequence-to-sequence Language Modeling", "Natural Language Inference", "Paraphrase Identification", "Data Augmentation"], "id": "cc01463a-682e-4045-8058-06e615873bee", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_18.39.14.png"}, {"title": "PCam", "short_description": "**PatchCamelyon** is an image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than ImageNet, trainable on a single GPU.", "price": 343, "created_at": "2025-01-09 13:44:40.898149", "keyword": ["Image Compression", "Breast Tumour Classification"], "id": "1bfde25b-221f-4558-82f1-9f6c5706699e", "image_url": "https://production-media.paperswithcode.com/datasets/pcam_Dj5Hqa9.jpg"}, {"title": "PETA", "short_description": "The PEdesTrian Attribute dataset (**PETA**) is a dataset fore recognizing pedestrian attributes, such as gender and clothing style, at a far distance. It is of interest in video surveillance scenarios where face and body close-shots and hardly available. It consists of 19,000 pedestrian images with 65 attributes (61 binary and 4 multi-class). Those images contain 8705 persons.\r\n\r\nSource: [Attribute Aware Pooling for Pedestrian Attribute Recognition](https://arxiv.org/abs/1907.11837)\r\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/PETA.html](http://mmlab.ie.cuhk.edu.hk/projects/PETA.html)", "price": 354, "created_at": "2025-01-09 13:44:40.898220", "keyword": ["Pedestrian Attribute Recognition"], "id": "bac823f0-b90f-4b18-8212-a97b68418c64", "image_url": "https://production-media.paperswithcode.com/datasets/PETA-0000000637-c3ed8546_bmmc8tH.jpg"}, {"title": "PGM", "short_description": "PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.\r\n\r\nSource: [Measuring abstract reasoning in neural networks](https://arxiv.org/pdf/1807.04225v1.pdf)\r\nImage Source: [Barrett et al](https://arxiv.org/abs/1807.04225)", "price": 543, "created_at": "2025-01-09 13:44:40.898292", "keyword": ["Image Classification", "Visual Reasoning", "Relational Reasoning"], "id": "85221110-3538-447c-90f7-fae4eeb13b18", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-11_at_19.49.28.png"}, {"title": "PHM2017", "short_description": "PHM2017 is a new dataset consisting of 7,192 English tweets across six diseases and conditions: Alzheimer\u2019s Disease, heart attack (any severity), Parkinson\u2019s disease, cancer (any type), Depression (any severity), and Stroke. The Twitter search API was used to retrieve the data using the colloquial disease names as search keywords, with the expectation of retrieving a high-recall, low precision dataset. After removing the re-tweets and replies, the tweets were manually annotated. The labels are:\r\n\r\n- self-mention. The tweet contains a health mention with a health self-report of the Twitter account owner, e.g., \"However, I worked hard and ran for Tokyo Mayer Election Campaign in January through February, 2014, without publicizing the cancer.\"\r\n- other-mention. The tweet contains a health mention of a health report about someone other than the account owner, e.g., \"Designer with Parkinson\u2019s couldn\u2019t work then engineer invents bracelet + changes her world\"\r\n- awareness. The tweet contains the disease name, but does not mention a specific person, e.g., \"A Month Before a Heart Attack, Your Body Will Warn You With These 8 Signals\"\r\n- non-health. The tweet contains the disease name, but the tweet topic is not about health. \"Now I can have cancer on my wall for all to see <3\"\r\n\r\nSource: [Did You Really Just Have a Heart Attack? Towards Robust Detection of Personal Health Mentions in Social Media](https://arxiv.org/pdf/1802.09130v2.pdf)", "price": 688, "created_at": "2025-01-09 13:44:40.898364", "keyword": ["Epidemiology"], "id": "ca9cbf58-3ef7-4d7a-8f5f-4f13740322b1", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-25_at_14.25.04.png"}, {"title": "PIRM", "short_description": "The PIRM dataset consists of 200 images, which are divided into two equal sets for validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images vary in size, and are typically ~300K pixels in resolution.", "price": 957, "created_at": "2025-01-09 13:44:40.898436", "keyword": ["Image Super-Resolution", "Image Restoration"], "id": "6f1c27dc-69b0-4a21-9218-b70f29ef5da6", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_2.35.07_PM.png"}, {"title": "PISC", "short_description": "The People in Social Context (PISC) dataset is a dataset that focuses on social relationships. It consists of 22,670 images of 9 types of social relationships. It has annotations for the bounding boxes of all people, as well as the social relationship between all pairs of people in the images. In addition, it also contains occupation annotation. \r\n\r\nSource: [PISC](https://zenodo.org/record/1059155)", "price": 671, "created_at": "2025-01-09 13:44:40.898523", "keyword": ["Visual Social Relationship Recognition", "Relational Reasoning", "Graph Generation"], "id": "ed66da8c-a7e5-4f57-9c2e-ae095d1b04c8", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_12.14.47.png"}, {"title": "PPI", "short_description": "protein roles\u2014in terms of their cellular functions from\r\ngene ontology\u2014in various protein-protein interaction (PPI) graphs, with each graph corresponding\r\nto a different human tissue [41]. positional gene sets are used, motif gene sets and immunological\r\nsignatures as features and gene ontology sets as labels (121 in total), collected from the Molecular\r\nSignatures Database [34]. The average graph contains 2373 nodes, with an average degree of 28.8.", "price": 144, "created_at": "2025-01-09 13:44:40.898602", "keyword": ["Node Classification", "Link Prediction"], "id": "051aaa2a-5907-43eb-8a86-9663279d79f1", "image_url": ""}, {"title": "PPMI", "short_description": "The **Parkinson\u2019s Progression Markers Initiative** (**PPMI**) dataset originates from an observational clinical and longitudinal study comprising evaluations of people with Parkinson\u2019s disease (PD), those people with high risk, and those who are healthy.\r\n\r\nSource: [Time-Guided High-Order Attention Model of Longitudinal Heterogeneous Healthcare Data](https://arxiv.org/abs/1912.00773)\r\nImage Source: [https://www.ppmi-info.org/2013/08/imaging-inventory-whats-in-the-ppmi-database-2/](https://www.ppmi-info.org/2013/08/imaging-inventory-whats-in-the-ppmi-database-2/)", "price": 725, "created_at": "2025-01-09 13:44:40.898676", "keyword": ["Reconstruction", "Medical Image Registration", "Diffeomorphic Medical Image Registration"], "id": "80891178-fcc9-4d48-94ef-c858641b7ba0", "image_url": "https://production-media.paperswithcode.com/datasets/PPMI-0000003730-538bafdb.jpg"}, {"title": "PRID2011", "short_description": "PRID 2011 is a person reidentification dataset that provides multiple person trajectories recorded from two different static surveillance cameras, monitoring crosswalks and sidewalks. The dataset shows a clean background, and the people in the dataset are rarely occluded. In the dataset, 200 people appear in both views. Among the 200 people, 178 people have more than 20 appearances\r\n\r\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)", "price": 680, "created_at": "2025-01-09 13:44:40.898750", "keyword": ["Person Re-Identification", "Unsupervised Person Re-Identification"], "id": "7fb729a7-efe5-4590-8b2f-2485dadb8f17", "image_url": ""}, {"title": "PROMISE12", "short_description": "The **PROMISE12** dataset was made available for the MICCAI 2012 prostate segmentation challenge. Magnetic Resonance (MR) images (T2-weighted) of 50 patients with various diseases were acquired at different locations with several MRI vendors and scanning protocols.\r\n\r\nSource: [Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions](https://arxiv.org/abs/1904.04205)\r\nImage Source: [https://promise12.grand-challenge.org/](https://promise12.grand-challenge.org/)", "price": 120, "created_at": "2025-01-09 13:44:40.898824", "keyword": ["Semantic Segmentation", "Medical Image Segmentation", "Data Augmentation", "Volumetric Medical Image Segmentation"], "id": "216f572f-cce6-4d5f-a8f5-6dd7d666f938", "image_url": "https://production-media.paperswithcode.com/datasets/PROMISE12-0000003744-5e5eb505.jpg"}, {"title": "PROTEINS", "short_description": "**PROTEINS** is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.\r\n\r\nSource: [Fast and Deep Graph Neural Networks](https://arxiv.org/abs/1911.08941)", "price": 908, "created_at": "2025-01-09 13:44:40.898908", "keyword": ["Graph Classification", "Graph Classification"], "id": "be4dd105-748f-47b6-91dc-5df3230fbb5b", "image_url": ""}, {"title": "PTC", "short_description": "**PTC** is a collection of 344 chemical compounds represented as graphs which report the carcinogenicity for rats. There are 19 node labels for each node.\r\n\r\nSource: [Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity](https://arxiv.org/abs/1904.01098)", "price": 903, "created_at": "2025-01-09 13:44:40.898978", "keyword": ["Graph Classification"], "id": "69d77434-8ecf-4f9e-aa86-e5fcfb9e5aba", "image_url": ""}, {"title": "Panlex", "short_description": "PanLex translates words in thousands of languages. Its database is panlingual (emphasizes coverage of every language) and lexical (focuses on words, not sentences).\r\n\r\nSource: [PANLEX](https://panlex.org/)\r\nImage Source: [http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf)", "price": 5, "created_at": "2025-01-09 13:44:40.899048", "keyword": ["Word Embeddings"], "id": "93eaf9cf-467e-44ac-adad-1aaf59c8b1a8", "image_url": "https://production-media.paperswithcode.com/datasets/Panlex-0000003555-e3518a90.jpg"}, {"title": "PanoContext", "short_description": "The **PanoContext** dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms.\r\n\r\nSource: [LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image](https://arxiv.org/abs/1803.08999)\r\nImage Source: [https://panocontext.cs.princeton.edu/paper.pdf](https://panocontext.cs.princeton.edu/paper.pdf)", "price": 363, "created_at": "2025-01-09 13:44:40.899119", "keyword": ["3D Room Layouts From A Single RGB Panorama"], "id": "96b58047-8c79-4202-b663-310f594dc106", "image_url": "https://production-media.paperswithcode.com/datasets/PanoContext-0000000851-b07cbc00_amV0EY4.jpg"}, {"title": "ParaCrawl", "short_description": "ParaCrawl v.7.1 is a parallel dataset with 41 language pairs primarily aligned with English (39 out of 41) and mined using the parallel-data-crawling tool Bitextor which includes downloading documents, preprocessing and normalization, aligning documents and segments, and filtering noisy data via Bicleaner. ParaCrawl focuses on European languages, but also includes 9 lower-resource, non-European language pairs in v7.1.\r\n\r\nSource: [Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets](https://arxiv.org/pdf/2103.12028.pdf)", "price": 179, "created_at": "2025-01-09 13:44:40.899189", "keyword": [], "id": "94784258-cf2e-4d15-9a09-173a46553d42", "image_url": ""}, {"title": "Paris6k", "short_description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled).\r\n\r\nProvide:\r\n\r\n* a high-level explanation of the dataset characteristics\r\n* explain motivations and summary of its content\r\n* potential use cases of the dataset", "price": 877, "created_at": "2025-01-09 13:44:40.899260", "keyword": ["Image Retrieval"], "id": "396a1504-a528-4903-9c43-59f3f62f36c5", "image_url": ""}, {"title": "PartNet", "short_description": "PartNet is a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others.\r\n\r\nSource: [PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding](/paper/partnet-a-large-scale-benchmark-for-fine)\r\nImage Source: [https://cs.stanford.edu/~kaichun/partnet/](https://cs.stanford.edu/~kaichun/partnet/)", "price": 197, "created_at": "2025-01-09 13:44:40.899330", "keyword": ["Semantic Segmentation", "Instance Segmentation", "3D Semantic Segmentation", "3D Instance Segmentation"], "id": "1fdd98d0-52bd-461b-80da-dd0ec8328007", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.40.05_PM.png"}, {"title": "Pavia University", "short_description": "The **Pavia University** dataset is a hyperspectral image dataset which gathered by a sensor known as the reflective optics system imaging spectrometer (ROSIS-3) over the city of Pavia, Italy. The image consists of 610\u00d7340 pixels with 115 spectral bands. The image is divided into 9 classes with a total of 42,776 labelled samples, including the asphalt, meadows, gravel, trees, metal sheet, bare soil, bitumen, brick, and shadow.\n\nSource: [Diversity in Machine Learning](https://arxiv.org/abs/1807.01477)\nImage Source: [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University)", "price": 289, "created_at": "2025-01-09 13:44:40.899400", "keyword": ["Hyperspectral Image Classification"], "id": "c6e9f2c4-97d3-43ee-b4eb-80c93a7bb88b", "image_url": "https://production-media.paperswithcode.com/datasets/Pavia_University-0000000501-c689b7d4_D6Onizf.jpg"}, {"title": "PeerRead", "short_description": "PearRead is a dataset of scientific peer reviews. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.\r\n\r\nSource: [https://github.com/allenai/PeerRead](https://github.com/allenai/PeerRead)", "price": 388, "created_at": "2025-01-09 13:44:40.899471", "keyword": ["Text Classification", "Language Modelling", "Decision Making"], "id": "be9e54fc-0d01-4581-a91e-85735d282ded", "image_url": ""}, {"title": "Penn Treebank", "short_description": "The English **Penn Treebank** (**PTB**) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).\r\nThe corpus is also commonly used for character-level and word-level Language Modelling.\r\n\r\nSource: [Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for Sequence Modelling](https://arxiv.org/abs/1904.04733)\r\nImage Source: [https://dl.acm.org/doi/10.5555/972470.972475](https://dl.acm.org/doi/10.5555/972470.972475)", "price": 939, "created_at": "2025-01-09 13:44:40.899544", "keyword": ["Language Modelling", "Open Information Extraction", "Dependency Parsing", "Part-Of-Speech Tagging", "Stochastic Optimization", "Chunking", "Constituency Parsing", "Missing Elements", "Unsupervised Dependency Parsing", "Language Modelling", "Open Information Extraction", "Dependency Parsing", "Part-Of-Speech Tagging", "Stochastic Optimization", "Chunking", "Constituency Parsing", "Missing Elements", "Unsupervised Dependency Parsing"], "id": "5f96b365-8b6b-4ef7-a55e-8fd61ccc5225", "image_url": "https://production-media.paperswithcode.com/datasets/treebank.png"}, {"title": "PeopleArt", "short_description": "People-Art is an object detection dataset which consists of people in 43 different styles. People contained in this dataset are quite different from those in common photographs. There are 42 categories of art styles and movements including Naturalism, Cubism, Socialist Realism, Impressionism, and Suprematism\n\nSource: [Point Linking Network for Object Detection](https://arxiv.org/abs/1706.03646)\nImage Source: [https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597](https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597)", "price": 266, "created_at": "2025-01-09 13:44:40.899615", "keyword": ["Object Detection", "Weakly Supervised Object Detection"], "id": "6f8d9810-1496-4c0a-9554-6d2498b41af8", "image_url": "https://production-media.paperswithcode.com/datasets/PeopleArt-0000000892-9a80adae_Mg3Z9sj.jpg"}, {"title": "Pinterest", "short_description": "The **Pinterest** dataset contains more than 1 million images associated to Pinterest users\u2019 who have \u201cpinned\u201d them.\n\nSource: [https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf](https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf)", "price": 270, "created_at": "2025-01-09 13:44:40.899686", "keyword": ["Recommendation Systems"], "id": "8187b144-6657-4c0f-a511-f1192c432a35", "image_url": ""}, {"title": "Pix3D", "short_description": "The **Pix3D** dataset is a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc.\r\n\r\nSource: [http://pix3d.csail.mit.edu/](http://pix3d.csail.mit.edu/)\r\nImage Source: [http://pix3d.csail.mit.edu/](http://pix3d.csail.mit.edu/)", "price": 38, "created_at": "2025-01-09 13:44:40.899757", "keyword": ["Pose Estimation", "3D Shape Reconstruction", "3D Shape Modeling", "3D Shape Classification"], "id": "4974c1d8-983a-46f7-a02f-6fd139ef950f", "image_url": "https://production-media.paperswithcode.com/datasets/Pix3D-0000001966-409c0128_KAgheJw.jpg"}, {"title": "Places", "short_description": "The **Places** dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.\r\n\r\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)\r\nImage Source: [http://places.csail.mit.edu/browser.html](http://places.csail.mit.edu/browser.html)", "price": 416, "created_at": "2025-01-09 13:44:40.899826", "keyword": ["Image Inpainting", "Cross-Domain Few-Shot", "Uncropping", "Image Inpainting", "Cross-Domain Few-Shot", "Uncropping"], "id": "ec6de224-6720-4a6d-8878-7df0b54f7993", "image_url": "https://production-media.paperswithcode.com/datasets/Places-0000003475-4b6da14b.jpg"}, {"title": "Places365", "short_description": "The **Places365** dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).\r\n\r\nSource: [Semantic-Aware Scene Recognition](https://arxiv.org/abs/1909.02410)\r\nImage Source: [Places](http://places2.csail.mit.edu/index.html)", "price": 788, "created_at": "2025-01-09 13:44:40.899897", "keyword": ["Image Classification", "Semantic Segmentation", "Out-of-Distribution Detection", "Image Inpainting", "Scene Recognition", "Image Outpainting", "Scene Classification"], "id": "50bb4edb-966c-4219-a69a-2b51e3e3a599", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.30.35_PM.png"}, {"title": "Polyvore", "short_description": "This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.\r\n\r\nSource: [GitHub](https://github.com/xthan/polyvore-dataset)\r\nImage Source: [https://arxiv.org/pdf/1707.05691.pdf](https://arxiv.org/pdf/1707.05691.pdf)", "price": 276, "created_at": "2025-01-09 13:44:40.899985", "keyword": ["Recommendation Systems", "Slot Filling"], "id": "49f020d9-aaa8-4023-bb7c-23d26e4d826e", "image_url": "https://production-media.paperswithcode.com/datasets/Polyvore-0000000887-1d478004_Pmp86mA.jpg"}, {"title": "PoseTrack", "short_description": "The **PoseTrack** dataset is a large-scale benchmark for multi-person pose estimation and tracking in videos. It requires not only pose estimation in single frames, but also temporal tracking across frames. It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively. For training videos, 30 frames from the center are annotated. For validation and test videos, besides 30 frames from the center, every fourth frame is also annotated for evaluating long range articulated tracking. The annotations include 15 body keypoints location, a unique person id and a head bounding box for each person instance.\r\n\r\nSource: [Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/abs/1804.06208)\r\nImage Source: [https://posetrack.net/](https://posetrack.net/)", "price": 105, "created_at": "2025-01-09 13:44:40.900054", "keyword": ["Multi-Person Pose Estimation", "Pose Tracking", "Multi-Person Pose Estimation and Tracking"], "id": "67ca6f03-cb38-4092-8454-fe71e391c2b8", "image_url": "https://production-media.paperswithcode.com/datasets/PoseTrack-0000003411-6b5321a6.gif"}, {"title": "Poser", "short_description": "The **Poser** dataset is a dataset for pose estimation which consists of 1927 training and 418 test images. These images are synthetically generated and tuned to unimodal predictions. The images were generated using the Poser software package.\n\nSource: [Overlapping Cover Local Regression Machines](https://arxiv.org/abs/1701.01218)\nImage Source: [https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487](https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487)", "price": 665, "created_at": "2025-01-09 13:44:40.900126", "keyword": ["Pose Estimation", "Gaussian Processes", "GPR"], "id": "d51ba8e1-8bec-4ffa-9af7-88a09c9fcd97", "image_url": "https://production-media.paperswithcode.com/datasets/Poser-0000003265-ec6316e9.jpg"}, {"title": "Pubmed", "short_description": "The **Pubmed** dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words.\r\n\r\nSource: [https://linqs.soe.ucsc.edu/data](https://linqs.soe.ucsc.edu/data)", "price": 270, "created_at": "2025-01-09 13:44:40.900197", "keyword": ["Node Classification", "Link Prediction", "Text Summarization", "Graph Classification", "Node Clustering", "Community Detection", "Graph Clustering", "Sentence Classification", "Unsupervised Extractive Summarization", "Node Classification", "Link Prediction", "Text Summarization", "Graph Classification", "Node Clustering", "Community Detection", "Graph Clustering", "Sentence Classification", "Unsupervised Extractive Summarization"], "id": "67104de6-23a6-41f3-80cd-65c45873cc63", "image_url": ""}, {"title": "QASC", "short_description": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/qasc)", "price": 592, "created_at": "2025-01-09 13:44:40.900266", "keyword": ["Question Answering", "Reading Comprehension", "Information Retrieval"], "id": "66a4234a-bad1-4eb0-9470-fa0681d31aab", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-11_at_16.06.20.png"}, {"title": "QM9", "short_description": "**QM9** provides quantum chemical properties (at DFT level) for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.\r\n\r\nSource: [QM9 Dataset](http://quantum-machine.org/datasets/)\r\nImage Source: [https://pubs.acs.org/doi/pdf/10.1021/ci300415d](https://pubs.acs.org/doi/pdf/10.1021/ci300415d)", "price": 756, "created_at": "2025-01-09 13:44:40.900334", "keyword": ["Drug Discovery", "Molecular Property Prediction", "Formation Energy", "Multi-Task Learning", "Time Series", "NMR J-coupling"], "id": "3b9c874d-34f3-4c2e-b462-127e1b7f8ba3", "image_url": "https://production-media.paperswithcode.com/datasets/QM9-0000000656-a9b4c029_2LeBxwM.jpg"}, {"title": "QNLI", "short_description": "The **QNLI** (**Question-answering NLI**) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLUE benchmark.\r\n\r\nSource: [https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)", "price": 756, "created_at": "2025-01-09 13:44:40.900402", "keyword": ["Text Generation", "Natural Language Inference", "Model Compression", "Few-Shot NLI", "QNLI"], "id": "82b0e0b7-b64c-4fe4-a230-5d75ea413890", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.07.50_PM.png"}, {"title": "QUASAR-T", "short_description": "**QUASAR-T** is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 43,013 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. The answers to these questions are free-form spans of text, though most are noun phrases.\r\n\r\nSource: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)\r\nImage Source: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)", "price": 214, "created_at": "2025-01-09 13:44:40.900473", "keyword": ["Question Answering", "Open-Domain Question Answering"], "id": "384d12a9-dace-4b0f-bce9-e2d0e485b8fd", "image_url": "https://production-media.paperswithcode.com/datasets/QUASAR-T-0000003384-b65925c4.jpg"}, {"title": "Quora Question Pairs", "short_description": "**Quora Question Pairs** (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.\r\n\r\nSource: [Bilateral Multi-Perspective Matching for Natural Language Sentences](/paper/bilateral-multi-perspective-matching-for)", "price": 640, "created_at": "2025-01-09 13:44:40.900542", "keyword": ["Text Generation", "Question Answering", "Text Classification", "Natural Language Inference", "Paraphrase Identification", "Paraphrase Generation", "Community Question Answering", "Paraphrase Identification within Bi-Encoder", "QQP"], "id": "136e199e-1983-4ec1-bed1-1f6e2828b549", "image_url": "https://production-media.paperswithcode.com/datasets/qqp.png"}, {"title": "R2R", "short_description": "R2R is a dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings, as illustrated in the demo above. For training, each instruction is associated with a Matterport3D Simulator trajectory. 22k instructions are available, with an average length of 29 words. There is a test evaluation server for this dataset available at EvalAI.\r\n\r\nSource: [Natural language interaction with robots](https://bringmeaspoon.org/)", "price": 558, "created_at": "2025-01-09 13:44:40.900610", "keyword": ["Visual Navigation", "Vision-Language Navigation"], "id": "23ef8695-4b3d-40e5-82a2-185b064d0076", "image_url": "https://production-media.paperswithcode.com/datasets/Room-to-Room-0000001564-ad9bbc28_QIUXr6b.jpg"}, {"title": "RACE", "short_description": "The **ReAding Comprehension dataset from Examinations** (**RACE**) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts.\r\n\r\nSource: [Dynamic Fusion Networks for Machine Reading Comprehension](https://arxiv.org/abs/1711.04964)\r\nImage Source: [Lai et al](https://arxiv.org/pdf/1704.04683v5.pdf)", "price": 772, "created_at": "2025-01-09 13:44:40.900678", "keyword": ["Text Generation", "Question Answering", "Reading Comprehension", "Distractor Generation"], "id": "3a483bcd-98ea-47cb-89a8-885e2e26a650", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.10.31_PM.png"}, {"title": "RAF-DB", "short_description": "The **Real-world Affective Faces** Database (**RAF-DB**) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.\r\n\r\nSource: [Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition](https://arxiv.org/abs/2007.10298)\r\nImage Source: [http://www.whdeng.cn/raf/model1.html](http://www.whdeng.cn/raf/model1.html)", "price": 29, "created_at": "2025-01-09 13:44:40.900746", "keyword": ["Facial Expression Recognition (FER)"], "id": "7495eff1-6cb8-4712-8a62-3df829459361", "image_url": "https://production-media.paperswithcode.com/datasets/RAF-DB-0000002219-fbb4a630_ID5n2bq.jpg"}, {"title": "RAP", "short_description": "The **Richly Annotated Pedestrian** (**RAP**) dataset is a dataset for pedestrian attribute recognition. It contains 41,585 images collected from indoor surveillance cameras. Each image is annotated with 72 attributes, while only 51 binary attributes with the positive ratio above 1% are selected for evaluation. There are 33,268 images for the training set and 8,317 for testing.\r\n\r\nSource: [Localization Guided Learning for Pedestrian Attribute Recognition](https://arxiv.org/abs/1808.09102)\r\nImage Source: [http://www.rapdataset.com/rapv1.html](http://www.rapdataset.com/rapv1.html)", "price": 260, "created_at": "2025-01-09 13:44:40.900816", "keyword": ["Pedestrian Attribute Recognition"], "id": "7bf6c81b-d258-4cdc-a502-a9ce269b1f9b", "image_url": "https://production-media.paperswithcode.com/datasets/RAP-0000000638-54f99f2d_YTw50Uk.jpg"}, {"title": "RCV1", "short_description": "The **RCV1** dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.\r\n\r\nSource: [Random Projections for Linear Support Vector Machines](https://arxiv.org/abs/1211.6085)\r\nImage Source: [https://www.nasdaq.com/publishers/reuters](https://www.nasdaq.com/publishers/reuters)", "price": 967, "created_at": "2025-01-09 13:44:40.900896", "keyword": ["Text Classification", "Multi-Label Text Classification", "Cross-Lingual Document Classification"], "id": "80730fe6-206e-4979-b4fc-3e70488afa2b", "image_url": "https://production-media.paperswithcode.com/datasets/RCV1-0000001947-12db340a_ZCgq2O8.jpg"}, {"title": "REDDIT-5K", "short_description": "Reddit-5K is a relational dataset extracted from Reddit.", "price": 702, "created_at": "2025-01-09 13:44:40.900966", "keyword": ["Graph Classification", "Topological Data Analysis"], "id": "57b0a6c0-f83f-436f-bfd4-867d4e1cfd89", "image_url": ""}, {"title": "REDDIT-BINARY", "short_description": "**REDDIT-BINARY** consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other\u2019s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.\r\n\r\nSource: [A simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508)", "price": 11, "created_at": "2025-01-09 13:44:40.901033", "keyword": ["Graph Classification", "Graph Representation Learning"], "id": "96933b78-5457-44f6-9e36-b933f756e0e1", "image_url": ""}, {"title": "RESISC45", "short_description": "RESISC45 dataset is a dataset for Remote Sensing Image Scene Classification (RESISC). It contains 31,500 RGB images of size 256\u00d7256 divided into 45 scene classes, each class containing 700 images. Among its notable features, RESISC45 contains varying spatial resolution ranging from 20cm to more than 30m/px.", "price": 857, "created_at": "2025-01-09 13:44:40.901101", "keyword": ["Image Classification", "Scene Classification"], "id": "7011e958-5af7-4556-8cb0-9b2200775d9a", "image_url": "https://production-media.paperswithcode.com/datasets/resisc.png"}, {"title": "ROPES", "short_description": "ROPES is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s), a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the back-ground passage in the context of the situation.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/ropes)", "price": 406, "created_at": "2025-01-09 13:44:40.901168", "keyword": ["Reading Comprehension", "Question Generation", "Decision Making"], "id": "20d98d03-56da-4e09-b111-c05c17dcef84", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-11_at_16.01.59.png"}, {"title": "RSICD", "short_description": "The **Remote Sensing Image Captioning Dataset** (**RSICD**) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image.\n\nSource: [https://github.com/201528014227051/RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)\nImage Source: [https://github.com/201528014227051/RSICD_optimal](https://github.com/201528014227051/RSICD_optimal)", "price": 456, "created_at": "2025-01-09 13:44:40.901235", "keyword": ["Image Captioning", "Cross-Modal Retrieval", "Image-to-Text Retrieval", "Text Retrieval", "Scene Classification"], "id": "16ba2653-49d2-4198-91aa-c7acefbdc0e4", "image_url": "https://production-media.paperswithcode.com/datasets/RSICD-0000005208-448f94f7.jpg"}, {"title": "RVL-CDIP", "short_description": "The **RVL-CDIP** dataset consists of scanned document images belonging to 16 classes such as letter, form, email, resume, memo, etc. The dataset has 320,000 training, 40,000 validation and 40,000 test images. The images are characterized by low quality, noise, and low resolution, typically 100 dpi.\r\n\r\nSource: [Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning](https://arxiv.org/abs/2009.14457)\r\nImage Source: [https://www.cs.cmu.edu/~aharley/rvl-cdip/](https://www.cs.cmu.edu/~aharley/rvl-cdip/)", "price": 325, "created_at": "2025-01-09 13:44:40.901301", "keyword": ["Image Classification", "Document Image Classification", "Document Layout Analysis", "Multi-Modal Document Classification"], "id": "187ff91d-a342-47c3-aa81-c3f9efbc943c", "image_url": "https://production-media.paperswithcode.com/datasets/RVL-CDIP-0000000502-f579eaab_GQ7QoTc.jpg"}, {"title": "RaFD", "short_description": "The **Radboud Faces Database** (**RaFD**) is a set of pictures of 67 models (both adult and children, males and females) displaying 8 emotional expressions.\r\n\r\nSource: [http://www.socsci.ru.nl:8180/RaFD2/RaFD](http://www.socsci.ru.nl:8180/RaFD2/RaFD)\r\nImage Source: [http://www.socsci.ru.nl:8180/RaFD2/RaFD](http://www.socsci.ru.nl:8180/RaFD2/RaFD)", "price": 225, "created_at": "2025-01-09 13:44:40.901371", "keyword": ["Image-to-Image Translation", "Facial Expression Recognition (FER)", "Image-to-Image Translation", "Facial Expression Recognition (FER)"], "id": "223256fe-e59e-4e67-81dc-c04b28d595c0", "image_url": "https://production-media.paperswithcode.com/datasets/RaFD-0000000596-591fcaf0_5jBZZrG.jpg"}, {"title": "Raider", "short_description": "The **Raider** dataset collects fMRI recordings of 1000 voxels from the ventral temporal cortex, for 10 healthy adult participants passively watching the full-length movie \u201cRaiders of the Lost Ark\u201d.\n\nSource: [Time-Resolved fMRI Shared Response Model using Gaussian Process Factor Analysis](https://arxiv.org/abs/2006.05572)\nImage Source: [https://arxiv.org/abs/1909.12537](https://arxiv.org/abs/1909.12537)", "price": 380, "created_at": "2025-01-09 13:44:40.901438", "keyword": ["Denoising"], "id": "e9ce76da-1aef-417f-a8d0-0cd3576db9bd", "image_url": "https://production-media.paperswithcode.com/datasets/Raider-0000003536-98450921.jpg"}, {"title": "RecipeQA", "short_description": "RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.\r\n\r\nSource: [RecipeQA](https://hucvl.github.io/recipeqa/)", "price": 978, "created_at": "2025-01-09 13:44:40.901506", "keyword": ["Object Detection", "Question Answering", "Common Sense Reasoning", "Natural Language Understanding"], "id": "cd849022-1a40-4a84-aeb5-f6e36c53e113", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_13.21.20.png"}, {"title": "Reddit", "short_description": "The **Reddit** dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or \u201csubreddit\u201d, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.\r\n\r\nSource: [https://arxiv.org/pdf/1706.02216.pdf](https://arxiv.org/pdf/1706.02216.pdf)\r\nImage Source: [https://minimaxir.com/2016/05/reddit-graph/](https://minimaxir.com/2016/05/reddit-graph/)", "price": 576, "created_at": "2025-01-09 13:44:40.901573", "keyword": ["Node Classification", "Sequence-to-sequence Language Modeling", "Text2text Generation", "Dynamic Link Prediction"], "id": "26a14265-22ff-47b5-ba29-dff967b2ff03", "image_url": "https://production-media.paperswithcode.com/datasets/Reddit-0000001419-68fa15d5_ZSmHUap.jpg"}, {"title": "ReferItGame", "short_description": "The ReferIt dataset contains 130,525 expressions for referring to 96,654 objects in 19,894 images of natural scenes.\r\n\r\nSource: [BiLingUNet: Image Segmentation by Modulating Top-Down and Bottom-Up Visual Processing with Referring Expressions](https://arxiv.org/abs/2003.12739)\r\nImage Source: [http://tamaraberg.com/referitgame/](http://tamaraberg.com/referitgame/)", "price": 241, "created_at": "2025-01-09 13:44:40.901640", "keyword": ["Semantic Segmentation", "Visual Question Answering (VQA)", "Image Captioning"], "id": "5c452ed0-5f7d-4e39-ba36-5c4390a04ca7", "image_url": "https://production-media.paperswithcode.com/datasets/ReferItGame-0000003418-3dbac4b4.jpg"}, {"title": "Ritter PoS", "short_description": "PTB-tagged English Tweets", "price": 246, "created_at": "2025-01-09 13:44:40.901707", "keyword": ["Part-Of-Speech Tagging"], "id": "3ea128be-5008-4214-b41b-ea4786414a21", "image_url": ""}, {"title": "RoboCup", "short_description": "**RoboCup** is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts.\r\n\r\nSource: [Robots as Actors in a Film: No War, A Robot Story](https://arxiv.org/abs/1910.12294)\r\nImage Source: [https://www.robocup.org/](https://www.robocup.org/)", "price": 557, "created_at": "2025-01-09 13:44:40.901774", "keyword": ["Text Generation", "Data-to-Text Generation", "Knowledge Graphs"], "id": "ad3a649c-a24e-45d6-a471-b65cfd15201e", "image_url": "https://production-media.paperswithcode.com/datasets/RoboCup-0000003561-951cc327.jpeg"}, {"title": "RotoWire", "short_description": "This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the \"rotowire\" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.\r\n\r\nSource: [Challenges in Data-to-Document Generation](https://arxiv.org/abs/1707.08052)\r\nImage Source: [https://arxiv.org/pdf/1707.08052v1.pdf](https://arxiv.org/pdf/1707.08052v1.pdf)", "price": 559, "created_at": "2025-01-09 13:44:40.901841", "keyword": ["Data-to-Text Generation", "Table-to-Text Generation", "Data-to-Text Generation", "Table-to-Text Generation"], "id": "7b4f4930-d98f-4a3d-b2de-a7c62c7856fe", "image_url": "https://production-media.paperswithcode.com/datasets/RotoWire-0000000516-ecfc9003_4VUYE04.jpg"}, {"title": "SALICON", "short_description": "The SALIency in CONtext (**SALICON**) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO.\r\nThe ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded.\r\nThe training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze.\r\nThe testing data contains only the image and resolution fields.\r\n\r\nSource: [DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations](https://arxiv.org/abs/1510.02927)\r\nImage Source: [http://salicon.net/explore/](http://salicon.net/explore/)", "price": 919, "created_at": "2025-01-09 13:44:40.901920", "keyword": ["Few-Shot Transfer Learning for Saliency Prediction", "Saliency Prediction"], "id": "336beaf7-d799-483c-86ce-0e32ee77a606", "image_url": "https://production-media.paperswithcode.com/datasets/SALICON-0000003401-4c1641df.jpg"}, {"title": "SALSA", "short_description": "A novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis.\r\n\r\nSource: [SALSA: A Novel Dataset for Multimodal Group Behavior Analysis](/paper/salsa-a-novel-dataset-for-multimodal-group)", "price": 547, "created_at": "2025-01-09 13:44:40.901986", "keyword": ["Person Re-Identification", "Gesture Recognition", "Dictionary Learning"], "id": "fe2a5ec3-1f79-47e2-9bdf-ea52a9ce46e4", "image_url": ""}, {"title": "SBD", "short_description": "The **Semantic Boundaries Dataset** (**SBD**) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes.\r\n\r\nSource: [Weakly Supervised Object Boundaries](https://arxiv.org/abs/1511.07803)\r\nImage Source: [http://home.bharathh.info/pubs/codes/SBD/download.html](http://home.bharathh.info/pubs/codes/SBD/download.html)", "price": 953, "created_at": "2025-01-09 13:44:40.902051", "keyword": ["Interactive Segmentation", "Edge Detection", "Semantic Contour Prediction", "Interactive Segmentation", "Edge Detection", "Semantic Contour Prediction"], "id": "548dacb5-2d6a-4998-bfd7-f73f707895e6", "image_url": "https://production-media.paperswithcode.com/datasets/SBD-0000000497-0ff29893_CpY2Sji.jpg"}, {"title": "SCAN", "short_description": "SCAN is a dataset for grounded navigation which consists of a set of simple compositional navigation commands paired with the corresponding action sequences. \r\n\r\nSource: [Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks](https://arxiv.org/pdf/1711.00350v3.pdf)", "price": 168, "created_at": "2025-01-09 13:44:40.902117", "keyword": ["Semantic Parsing", "Systematic Generalization"], "id": "78ad7ada-422f-4493-930b-8600b3b2cc18", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.43.57_PM.png"}, {"title": "SCUT-CTW1500", "short_description": "The **SCUT-CTW1500** dataset contains 1,500 images: 1,000 for training and 500 for testing. In particular, it provides 10,751 cropped text instance images, including 3,530 with curved text. The images are manually harvested from the Internet, image libraries such as Google Open-Image, or phone cameras. The dataset contains a lot of horizontal and multi-oriented text.\r\n\r\nSource: [Text Recognition in the Wild: A Survey](https://arxiv.org/abs/2005.03492)\r\nImage Source: [https://github.com/Yuliang-Liu/Curve-Text-Detector](https://github.com/Yuliang-Liu/Curve-Text-Detector)", "price": 564, "created_at": "2025-01-09 13:44:40.902182", "keyword": ["Scene Text Detection", "Text Spotting", "Curved Text Detection"], "id": "fee5b6d9-a554-410b-9282-509b2d17c576", "image_url": "https://production-media.paperswithcode.com/datasets/SCUT-CTW1500-0000000838-970db3b0_aC3TtV1.jpg"}, {"title": "SEED", "short_description": "The **SEED** dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones.\r\n\r\nSource: [http://bcmi.sjtu.edu.cn/home/seed/index.html](http://bcmi.sjtu.edu.cn/home/seed/index.html)\r\nImage Source: [http://bcmi.sjtu.edu.cn/home/seed/index.html](http://bcmi.sjtu.edu.cn/home/seed/index.html)", "price": 940, "created_at": "2025-01-09 13:44:40.902250", "keyword": ["Emotion Recognition", "Electroencephalogram (EEG)", "EEG Emotion Recognition"], "id": "8cbd4eb6-6881-43cc-917f-e6a343237e32", "image_url": "https://production-media.paperswithcode.com/datasets/SEED-0000003303-6693c44d.gif"}, {"title": "SEMAINE", "short_description": "The **SEMAINE** videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips.\r\n\r\nSource: [ROBUST MODELING OF EPISTEMIC MENTAL STATES](https://arxiv.org/abs/2005.13982)", "price": 276, "created_at": "2025-01-09 13:44:40.902316", "keyword": ["Emotion Recognition in Conversation"], "id": "16c9d55b-251e-4c62-814c-bdcaa1cb71dc", "image_url": "https://production-media.paperswithcode.com/datasets/semaine.jpeg"}, {"title": "SFEW", "short_description": "The Static Facial Expressions in the Wild (**SFEW**) dataset is a dataset for facial expression recognition. It was created by selecting static frames from the AFEW database by computing key frames based on facial point clustering. The most commonly used version, SFEW 2.0, was the benchmarking data for the SReco sub-challenge in EmotiW 2015. SFEW 2.0 has been divided into three sets: Train (958 samples), Val (436 samples) and Test (372 samples). Each of the images is assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happiness, sadness, and surprise. The expression labels of the training and validation sets are publicly available, whereas those of the testing set are held back by the challenge organizer.\r\n\r\nSource: [Deep Facial Expression Recognition: A Survey](https://arxiv.org/abs/1804.08348)\r\nImage Source: [https://computervisiononline.com/dataset/1105138659](https://computervisiononline.com/dataset/1105138659)", "price": 341, "created_at": "2025-01-09 13:44:40.902382", "keyword": ["Facial Expression Recognition (FER)", "Facial Expression Recognition (FER)"], "id": "3375abcd-c6ed-4921-a204-458303119ff0", "image_url": "https://production-media.paperswithcode.com/datasets/SFEW-0000000272-8121abab_qOC5CXw.jpg"}, {"title": "SHREC", "short_description": "The **SHREC** dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset.\n\nSource: [Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition](https://arxiv.org/abs/1803.10435)\nImage Source: [http://tosca.cs.technion.ac.il/book/shrec.html](http://tosca.cs.technion.ac.il/book/shrec.html)", "price": 797, "created_at": "2025-01-09 13:44:40.902448", "keyword": ["Skeleton Based Action Recognition", "Gesture Recognition", "Hand Gesture Recognition", "3D Object Recognition", "Point Cloud Super Resolution"], "id": "7ac8a6b9-abf8-456a-a648-866a020bba1e", "image_url": "https://production-media.paperswithcode.com/datasets/SHREC-0000003304-98ec563f.jpg"}, {"title": "SICK", "short_description": "The **Sentences Involving Compositional Knowledge** (**SICK**) dataset is a dataset for compositional distributional semantics. It includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson\u2019s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm.\r\n\r\nSource: [Multi-Label Transfer Learning for Multi-Relational Semantic Similarity](https://arxiv.org/abs/1805.12501)\r\nImage Source: [https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619](https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619)", "price": 245, "created_at": "2025-01-09 13:44:40.902513", "keyword": ["Natural Language Inference", "Semantic Textual Similarity", "Semantic Similarity", "Natural Language Inference", "Semantic Textual Similarity", "Semantic Similarity"], "id": "4ea23c81-1034-4345-bf5d-b1c42acd0e42", "image_url": "https://production-media.paperswithcode.com/datasets/SICK-0000001661-d194b8fe_UJ5no1C.jpg"}, {"title": "SIXray", "short_description": "The **SIXray** dataset is constructed by the Pattern Recognition and Intelligent System Development Laboratory, University of Chinese Academy of Sciences. It contains 1,059,231 X-ray images which are collected from some several subway stations. There are six common categories of prohibited items, namely, gun, knife, wrench, pliers, scissors and hammer. It has three subsets called SIXray10, SIXray100 and SIXray1000, There are image-level annotations provided by human security inspectors for the whole dataset. In addition the images in the test set are annotated with a bounding-box for each prohibited item to evaluate the performance of object localization.\n\nSource: [https://github.com/MeioJane/SIXray](https://github.com/MeioJane/SIXray)\nImage Source: [https://github.com/MeioJane/SIXray](https://github.com/MeioJane/SIXray)", "price": 515, "created_at": "2025-01-09 13:44:40.902578", "keyword": ["Object Detection", "Instance Segmentation", "Object Localization"], "id": "9ec80eab-53ee-4485-adfd-6479b5e612c4", "image_url": "https://production-media.paperswithcode.com/datasets/SIXray-0000001057-8c3d6a3d.jpg"}, {"title": "SK-LARGE", "short_description": "**SK-LARGE** is a benchmark dataset for object skeleton detection, built on the MS COCO dataset. It contains 1491 images, 746 for training and 745 for testing.\n\nSource: [DeepFlux for Skeletons in the Wild](https://arxiv.org/abs/1811.12608)\nImage Source: [http://kaizhao.net/sk-large](http://kaizhao.net/sk-large)", "price": 47, "created_at": "2025-01-09 13:44:40.902644", "keyword": ["Object Skeleton Detection"], "id": "cf32436a-153e-4f9d-982c-4e2280b09bac", "image_url": "https://production-media.paperswithcode.com/datasets/SK-LARGE-0000000499-6983d91a_MfTrUeK.jpg"}, {"title": "SKU110K", "short_description": "The Sku110k dataset provides 11,762 images with more than 1.7 million annotated bounding boxes captured in densely packed scenarios, including 8,233 images for training, 588 images for validation, and 2,941 images for testing. There are around 1,733,678 instances in total. The images are collected from thousands of supermarket stores and are of various scales, viewing angles, lighting conditions, and noise levels. All the images are resized into a resolution of one megapixel. Most of the instances in the dataset are tightly packed and typically of a certain orientation in the rage of [\u221215\u2218, 15\u2218].\n\nSource: [Rethinking Object Detection in Retail Stores](https://arxiv.org/abs/2003.08230)\nImage Source: [https://github.com/eg4000/SKU110K_CVPR19](https://github.com/eg4000/SKU110K_CVPR19)", "price": 396, "created_at": "2025-01-09 13:44:40.902709", "keyword": ["Object Detection", "One-Shot Object Detection", "Dense Object Detection"], "id": "61577626-a185-4a4d-9d03-fcfb9c89f719", "image_url": "https://production-media.paperswithcode.com/datasets/SKU110K-0000003393-f08b9745.jpeg"}, {"title": "SMS-WSJ", "short_description": "Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) consists of artificially mixed speech taken from the WSJ database, but unlike earlier databases this one considers all WSJ0+1 utterances and takes care of strictly separating the speaker sets present in the training, validation and test sets. \r\n\r\nSource: [SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition](/paper/sms-wsj-database-performance-measures-and)", "price": 17, "created_at": "2025-01-09 13:44:40.902775", "keyword": ["Speech Recognition", "Audio Source Separation"], "id": "1cb20b6f-6bed-4ab8-9ea5-f9c7d456e2e2", "image_url": ""}, {"title": "SNIPS", "short_description": "The **SNIPS** Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:\r\n\r\n* SearchCreativeWork (e.g. Find me the I, Robot television show),\r\n* GetWeather (e.g. Is it windy in Boston, MA right now?),\r\n* BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night),\r\n* PlayMusic (e.g. Play the last track from Beyonc\u00e9 off Spotify),\r\n* AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist),\r\n* RateBook (e.g. Give 6 stars to Of Mice and Men),\r\n* SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris).\r\nThe training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.\r\n\r\nSource: [https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/](https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/)", "price": 157, "created_at": "2025-01-09 13:44:40.902839", "keyword": ["Zero-Shot Learning", "Intent Detection", "Slot Filling", "Open Intent Discovery", "Intent Discovery", "Out of Distribution (OOD) Detection", "Zero-Shot Learning", "Intent Detection", "Slot Filling", "Open Intent Discovery", "Intent Discovery", "Out of Distribution (OOD) Detection"], "id": "3fdb3859-74d4-48d2-9188-64319b7b3084", "image_url": "https://production-media.paperswithcode.com/datasets/snips.png"}, {"title": "SNLI", "short_description": "The **SNLI** dataset (**Stanford Natural Language Inference**) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as \u201centailment\u201d, \u201cneutral\u201d, \u201ccontradiction\u201d or \u201c-\u201d, where \u201c-\u201d indicates that an agreement could not be reached.\r\n\r\nSource: [Breaking NLI Systemswith Sentences that Require Simple Lexical Inferences](https://arxiv.org/abs/1805.02266)", "price": 519, "created_at": "2025-01-09 13:44:40.902917", "keyword": ["Sequence-to-sequence Language Modeling", "Natural Language Inference", "Sequence-to-sequence Language Modeling", "Natural Language Inference"], "id": "3b70ba5d-2746-4834-b53f-c6814afdbbbb", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-27_at_12.45.57_PM.png"}, {"title": "SQA", "short_description": "The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.\r\n\r\nSource: [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253)", "price": 538, "created_at": "2025-01-09 13:44:40.902981", "keyword": ["Semantic Parsing"], "id": "0aa5e7d3-32f1-4688-aec4-512d6d6012d8", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-10_at_13.07.50.png"}, {"title": "SQuAD", "short_description": "The **Stanford Question Answering Dataset** (**SQuAD**) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.\r\n\r\nSource: [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)\r\nImage Source: [https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html)", "price": 167, "created_at": "2025-01-09 13:44:40.903046", "keyword": ["Question Answering", "Sequence-to-sequence Language Modeling", "Reading Comprehension", "Open-Domain Question Answering", "Question Generation", "text2text-generation", "Question Answering", "Sequence-to-sequence Language Modeling", "Reading Comprehension", "Open-Domain Question Answering", "Question Generation", "text2text-generation"], "id": "1e3d63a7-4121-4e95-aec8-b901e364ab11", "image_url": "https://production-media.paperswithcode.com/datasets/SQuAD-0000000201-f1042aa7_sx8KtbI.jpg"}, {"title": "SST", "short_description": "The **Stanford Sentiment Treebank** is a corpus with fully labeled parse trees that allows for a\r\ncomplete analysis of the compositional effects of\r\nsentiment in language. The corpus is based on\r\nthe dataset introduced by Pang and Lee (2005) and\r\nconsists of 11,855 single sentences extracted from\r\nmovie reviews. It was parsed with the Stanford\r\nparser and includes a total of 215,154 unique phrases\r\nfrom those parse trees, each annotated by 3 human judges.\r\n\r\nEach phrase is labelled as either *negative*, *somewhat negative*, *neutral*, *somewhat positive* or *positive*.\r\nThe corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (*negative* or *somewhat negative* vs *somewhat positive* or *positive* with *neutral* sentences discarded) refer to the dataset as SST-2 or SST binary.", "price": 277, "created_at": "2025-01-09 13:44:40.903112", "keyword": ["Text Generation", "Text Classification", "Sentiment Analysis", "Few-Shot Learning", "Out-of-Distribution Detection", "Few-Shot Text Classification", "Explanation Fidelity Evaluation", "SST-2", "Sentiment Classification", "Text Generation", "Text Classification", "Sentiment Analysis", "Few-Shot Learning", "Out-of-Distribution Detection", "Few-Shot Text Classification", "Explanation Fidelity Evaluation", "SST-2", "Sentiment Classification"], "id": "341b0f14-d92d-4b88-89b2-13cb43302c6b", "image_url": "https://production-media.paperswithcode.com/datasets/sst.jpg"}, {"title": "STARE", "short_description": "The **STARE** (**Structured Analysis of the Retina**) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700\u00d7605) color fundus images. For each image, two groups of annotations are provided..\r\n\r\nSource: [DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels](https://arxiv.org/abs/2009.12053)\r\nImage Source: [https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756](https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756)", "price": 780, "created_at": "2025-01-09 13:44:40.903177", "keyword": ["Retinal Vessel Segmentation", "Retinal Vessel Segmentation"], "id": "76fec0f8-322d-42f4-9885-d0fc38df61a6", "image_url": "https://production-media.paperswithcode.com/datasets/STARE-0000000646-548764a1_UCFFrzi.jpg"}, {"title": "STL-10", "short_description": "The **STL-10** is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96\u00d796 pixels in size.\r\n\r\nSource: [Unsupervised Feature Learning with C-SVDDNet](https://arxiv.org/abs/1412.7259)\r\nImage Source: [https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)", "price": 893, "created_at": "2025-01-09 13:44:40.903241", "keyword": ["Image Classification", "Image Generation", "Anomaly Detection", "Semi-Supervised Image Classification", "Out-of-Distribution Detection", "Image Clustering", "Fine-Grained Image Classification", "Neural Architecture Search", "Unsupervised Anomaly Detection", "Image Compression", "Unsupervised Image Classification", "Self-Supervised Learning", "Unsupervised Anomaly Detection with Specified Settings -- 30% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 20% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 0.1% anomaly", "Unsupervised Anomaly Detection with Specified Settings -- 10% anomaly"], "id": "08f1877d-8c95-4379-879d-6d2ba63e0544", "image_url": "https://production-media.paperswithcode.com/datasets/STL-10-0000000425-6637c81e_kvalK8q.jpg"}, {"title": "SUBJ", "short_description": "Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.\r\n\r\nSource: [SUBJ](http://www.cs.cornell.edu/people/pabo/movie-review-data/)", "price": 6, "created_at": "2025-01-09 13:44:40.903305", "keyword": ["Subjectivity Analysis"], "id": "e34cc7fd-a8fa-4f99-90c7-d6ac5884c564", "image_url": ""}, {"title": "SUN RGB-D", "short_description": "The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively.\r\n\r\nSource: [Mix and match networks: multi-domain alignment for unpaired image-to-image translation](https://arxiv.org/abs/1903.04294)\r\nImage Source: [https://rgbd.cs.princeton.edu/](https://rgbd.cs.princeton.edu/)", "price": 514, "created_at": "2025-01-09 13:44:40.903369", "keyword": ["Object Detection", "Semantic Segmentation", "3D Object Detection", "Panoptic Segmentation", "Monocular Depth Estimation", "Monocular 3D Object Detection", "Scene Segmentation", "Scene Recognition", "Scene Classification (unified classes)", "Robust Semi-Supervised RGBD Semantic Segmentation", "Object Detection In Indoor Scenes", "Room Layout Estimation"], "id": "47da6824-1cd5-46f2-8fc3-844bb73b6a9a", "image_url": "https://production-media.paperswithcode.com/datasets/SUN_RGB-D-0000003450-ce5806f7.jpeg"}, {"title": "SUN09", "short_description": "The **SUN09** dataset consists of 12,000 annotated images with more than 200 object categories. It consists of natural, indoor and outdoor images. Each image contains an average of 7 different annotated objects and the average occupancy of each object is 5% of image size. The frequencies of object categories follow a power law distribution.\r\n\r\nSource: [A Pooling Approach to Modelling Spatial Relations forImage Retrieval and Annotation](https://arxiv.org/abs/1411.5190)\r\nImage Source: [http://people.csail.mit.edu/myungjin/HContext.html](http://people.csail.mit.edu/myungjin/HContext.html)", "price": 625, "created_at": "2025-01-09 13:44:40.903433", "keyword": ["Left Ventricle Segmentation"], "id": "5464b9f0-1b27-4e7f-b34d-7b76ef23a992", "image_url": "https://production-media.paperswithcode.com/datasets/SUN09-0000002017-453f251f_n8PY6nA.jpg"}, {"title": "SUN360", "short_description": "The goal of the **SUN360** panorama database is to provide academic researchers in computer vision, computer graphics and computational photography, cognition and neuroscience, human perception, machine learning and data mining, with a comprehensive collection of annotated panoramas covering 360x180-degree full view for a large variety of environmental scenes, places and the objects within. To build the core of the dataset, the authors download a huge number of high-resolution panorama images from the Internet, and group them into different place categories. Then, they designed a WebGL annotation tool for annotating the polygons and cuboids for objects in the scene.\r\n\r\nSource: [Scene UNderstanding 360\u00b0 panorama](https://vision.cs.princeton.edu/projects/2012/SUN360/data/)\r\nImage Source: [http://3dvision.princeton.edu/projects/2012/SUN360/](http://3dvision.princeton.edu/projects/2012/SUN360/)", "price": 546, "created_at": "2025-01-09 13:44:40.903500", "keyword": ["Outdoor Light Source Estimation"], "id": "3ce67502-a2a5-4774-b82c-dc85e9db5efd", "image_url": "https://production-media.paperswithcode.com/datasets/SUN360-0000000936-40e30eb4_NVrMMYd.jpg"}, {"title": "SUN3D", "short_description": "**SUN3D** contains a large-scale RGB-D video database, with 8 annotated sequences. Each frame has a semantic segmentation of the objects in the scene and information about the camera pose. It is composed by 415 sequences captured in 254 different spaces, in 41 different buildings. Moreover, some places have been captured multiple times at different moments of the day.\r\n\r\nSource: [A Review on Deep Learning TechniquesApplied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\r\nImage Source: [http://sun3d.cs.princeton.edu/](http://sun3d.cs.princeton.edu/)", "price": 532, "created_at": "2025-01-09 13:44:40.903565", "keyword": ["Semantic Segmentation", "Pose Estimation", "Depth Estimation", "Semantic Segmentation", "Pose Estimation", "Depth Estimation"], "id": "82b44140-2535-4d1b-959b-efd546cdcad4", "image_url": "https://production-media.paperswithcode.com/datasets/SUN3D-0000003495-7ddf1842.jpg"}, {"title": "SUNCG", "short_description": "**SUNCG** is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations.\r\n\r\nThe dataset is currently not available.\r\n\r\nSource: [https://sscnet.cs.princeton.edu/](https://sscnet.cs.princeton.edu/)\r\nImage Source: [https://sscnet.cs.princeton.edu/](https://sscnet.cs.princeton.edu/)", "price": 651, "created_at": "2025-01-09 13:44:40.903633", "keyword": ["Semantic Segmentation", "Depth Estimation", "Visual Navigation"], "id": "99ec29bc-10ca-40aa-b396-9401a6db09ec", "image_url": "https://production-media.paperswithcode.com/datasets/SUNCG-0000003451-dda46ba2.jpg"}, {"title": "SVHN", "short_description": "Street View House Numbers (**SVHN**) is a digit classification benchmark dataset that contains 600,000 32\u00d732 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process.\r\n\r\nSource: [Competitive Multi-scale Convolution](https://arxiv.org/abs/1511.05635)\r\nImage Source: [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)", "price": 286, "created_at": "2025-01-09 13:44:40.903698", "keyword": ["Image Classification", "Domain Adaptation", "Anomaly Detection", "Semi-Supervised Image Classification", "Unsupervised Image Classification", "Novel Class Discovery", "Sparse Representation-based Classification"], "id": "3035e468-a6df-4b5e-b03f-0ec6f0b63538", "image_url": "https://production-media.paperswithcode.com/datasets/SVHN-0000000424-c12734ed_mMXUnWD.jpg"}, {"title": "SWAG", "short_description": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.\r\n\r\nThe dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. The authors aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.\r\n\r\nSource: [SWAG](https://rowanzellers.com/swag/)\r\nImage Source: [Zellers et al](https://arxiv.org/pdf/1808.05326v1.pdf)", "price": 344, "created_at": "2025-01-09 13:44:40.903765", "keyword": ["Question Answering", "Common Sense Reasoning", "Multiple-choice"], "id": "ddf2f566-9945-42b6-a0bf-bc31851e8e12", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-07_at_17.36.30.png"}, {"title": "SYNTHIA", "short_description": "The **SYNTHIA** dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 \u00d7 960.\r\n\r\nSource: [Orientation-aware Semantic Segmentation on Icosahedron Spheres](https://arxiv.org/abs/1907.12849)\r\nImage Source: [https://synthia-dataset.net/](https://synthia-dataset.net/)", "price": 522, "created_at": "2025-01-09 13:44:40.903829", "keyword": ["Semantic Segmentation", "Domain Adaptation", "Image-to-Image Translation", "Unsupervised Domain Adaptation", "Novel View Synthesis", "Synthetic-to-Real Translation", "One-shot Unsupervised Domain Adaptation", "Semantic Segmentation", "Domain Adaptation", "Image-to-Image Translation", "Unsupervised Domain Adaptation", "Novel View Synthesis", "Synthetic-to-Real Translation", "One-shot Unsupervised Domain Adaptation"], "id": "66e6180c-77fc-46c9-ae68-fc747763a77e", "image_url": "https://production-media.paperswithcode.com/datasets/SYNTHIA-0000003445-124dc9de.jpeg"}, {"title": "Scan2CAD", "short_description": "**Scan2CAD** is an alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoints pairs between 14225 (3049 unique) CAD models from ShapeNet and their counterpart objects in the scans. The top 3 annotated model classes are chairs, tables and cabinets which arises due to the nature of indoor scenes in ScanNet. The number of objects aligned per scene ranges from 1 to 40 with an average of 9.3.\r\n\r\nAdditionally, all ShapeNet CAD models used in the Scan2CAD dataset are annotated with their rotational symmetries: either none, 2-fold, 4-fold or infinite rotational symmetries around a canonical axis of the object.\r\n\r\nSource: [Scan2CAD: Learning CAD Model Alignment in RGB-D Scans](https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb/)\r\nImage Source: [Scan2CAD: Learning CAD Model Alignment in RGB-D Scans](https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb/)", "price": 774, "created_at": "2025-01-09 13:44:40.903893", "keyword": ["3D Reconstruction"], "id": "e2cf67c2-461b-40e3-a826-173fcb42193e", "image_url": "https://production-media.paperswithcode.com/datasets/Scan2CAD-0000000709-02badb3f_yL2hpa4.jpg"}, {"title": "ScanNet", "short_description": "**ScanNet** is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.\r\n\r\nSource: [A Review of Point Cloud Semantic Segmentation](https://arxiv.org/abs/1908.08854)\r\nImage Source: [http://www.scan-net.org/](http://www.scan-net.org/)", "price": 493, "created_at": "2025-01-09 13:44:40.903960", "keyword": ["Semantic Segmentation", "3D Object Detection", "Depth Estimation", "3D Reconstruction", "Panoptic Segmentation", "Scene Segmentation", "Generalized Zero-Shot Learning", "3D Instance Segmentation", "Scene Recognition", "Surface Normals Estimation", "3D Semantic Instance Segmentation", "Unsupervised 3D Semantic Segmentation"], "id": "d4faa269-0c6d-47d7-bfb8-bfa2f68678fb", "image_url": "https://production-media.paperswithcode.com/datasets/ScanNet-0000000495-e5ee93dc_oEzDrxU.jpeg"}, {"title": "SceneNN", "short_description": "SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design.\r\nAll scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses.\r\n\r\nSource: [SceneNN: A Scene Meshes Dataset with aNNotations](http://103.24.77.34/scenenn/home/)\r\nImage Source: [http://103.24.77.34/scenenn/home/](http://103.24.77.34/scenenn/home/)", "price": 110, "created_at": "2025-01-09 13:44:40.904026", "keyword": ["3D Instance Segmentation"], "id": "0d71ade3-d0e7-4767-8733-928c39be987a", "image_url": "https://production-media.paperswithcode.com/datasets/SceneNN-0000001568-e5d7d24c_PPA5lrE.jpg"}, {"title": "SciCite", "short_description": "**SciCite** is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC.\n\nSource: [Structural Scaffolds for Citation Intent Classification in Scientific Publications](https://arxiv.org/abs/1904.01608)\nImage Source: [https://arxiv.org/pdf/1904.01608v2.pdf](https://arxiv.org/pdf/1904.01608v2.pdf)", "price": 727, "created_at": "2025-01-09 13:44:40.904091", "keyword": ["Sentence Classification", "Citation Intent Classification"], "id": "723ac565-522a-4749-8097-7c8713174ae7", "image_url": "https://production-media.paperswithcode.com/datasets/SciCite-0000000850-2edaeb61_jEhbNNO.jpg"}, {"title": "SciERC", "short_description": "**SciERC** dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.\r\n\r\nSource: [http://nlp.cs.washington.edu/sciIE/](http://nlp.cs.washington.edu/sciIE/)\r\nImage Source: [http://nlp.cs.washington.edu/sciIE/](http://nlp.cs.washington.edu/sciIE/)", "price": 125, "created_at": "2025-01-09 13:44:40.904158", "keyword": ["Named Entity Recognition (NER)", "Relation Extraction", "Joint Entity and Relation Extraction", "Few-Shot Relation Classification", "Continual Pretraining"], "id": "abf7a831-a6be-4e12-9a7f-2c31650869a3", "image_url": "https://production-media.paperswithcode.com/datasets/SciERC-0000000828-6e85c564_aHA1ts9.jpg"}, {"title": "SciTail", "short_description": "The **SciTail** dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create the SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples with neutral label.\r\n\r\nSource: [Allen Institute for AI](https://allenai.org/data/scitail)\r\nImage source: [Allen Institute for AI](https://allenai.org/data/scitail)", "price": 217, "created_at": "2025-01-09 13:44:40.904224", "keyword": ["Natural Language Inference"], "id": "e28cb4bd-e08c-4990-8ebc-a92f49c6c43a", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-18_at_10.48.17.png"}, {"title": "ScribbleSup", "short_description": "The **PASCAL-Scribble Dataset** is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set.\r\nIn the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated.\r\n\r\nSource: [https://jifengdai.org/downloads/scribble_sup/](https://jifengdai.org/downloads/scribble_sup/)\r\nImage Source: [https://jifengdai.org/downloads/scribble_sup/](https://jifengdai.org/downloads/scribble_sup/)", "price": 946, "created_at": "2025-01-09 13:44:40.904289", "keyword": ["Semantic Segmentation", "Instance Segmentation", "Weakly-Supervised Semantic Segmentation"], "id": "074eaef2-ba3c-4af5-b672-d2d0f837b683", "image_url": "https://production-media.paperswithcode.com/datasets/ScribbleSup-0000003483-e0314f9d.jpg"}, {"title": "SearchQA", "short_description": "SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. \r\n\r\nSource: [SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine](https://arxiv.org/pdf/1704.05179.pdf)", "price": 319, "created_at": "2025-01-09 13:44:40.904354", "keyword": ["Open-Domain Question Answering", "Open-Domain Question Answering"], "id": "a42eee02-0e37-4840-ae73-2ce21b7e6010", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_13.27.29.png"}, {"title": "Semantic3D", "short_description": "**Semantic3D** is a point cloud dataset of scanned outdoor scenes with over 3 billion points. It contains 15 training and 15 test scenes annotated with 8 class labels. This large labelled 3D point cloud data set of natural covers a range of diverse urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles to name just a few. The point clouds provided are scanned statically with state-of-the-art equipment and contain very fine details.\r\n\r\nSource: [Tangent Convolutions for Dense Prediction in 3D](https://arxiv.org/abs/1807.02443)\r\nImage Source: [http://www.semantic3d.net/](http://www.semantic3d.net/)", "price": 15, "created_at": "2025-01-09 13:44:40.904419", "keyword": ["Semantic Segmentation"], "id": "2e46a8f3-39b3-4d25-986c-7035e0b6d482", "image_url": "https://production-media.paperswithcode.com/datasets/Semantic3D-0000001375-92c33dee_jPwwsda.jpg"}, {"title": "SentEval", "short_description": "SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders.\r\n\r\nSource: [SentEval: An Evaluation Toolkit for Universal Sentence Representations](/paper/senteval-an-evaluation-toolkit-for-universal)", "price": 235, "created_at": "2025-01-09 13:44:40.904483", "keyword": ["Semantic Textual Similarity", "Linear-Probe Classification"], "id": "5f38905d-48c8-4567-aee9-ac98fbfb0582", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.05.07_PM.png"}, {"title": "Set11", "short_description": "**Set11** is a dataset of 11 grayscale images. It is a dataset used for image reconstruction and image compression.\n\nSource: [ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing](https://arxiv.org/abs/1706.07929)\nImage Source: [https://arxiv.org/pdf/1706.07929.pdf](https://arxiv.org/pdf/1706.07929.pdf)", "price": 151, "created_at": "2025-01-09 13:44:40.904547", "keyword": ["Compressive Sensing", "Image Compressed Sensing"], "id": "0e4381b0-3251-4026-8559-47a74fc72d1f", "image_url": "https://production-media.paperswithcode.com/datasets/Set11-0000003400-42d3e0ab.jpg"}, {"title": "Set12", "short_description": "**Set12** is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256\u00d7256.\r\n\r\nSource: [Designing and Training of A Dual CNN for Image Denoising](https://arxiv.org/abs/2007.03951)\r\nImage Source: [https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598](https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598)", "price": 449, "created_at": "2025-01-09 13:44:40.904611", "keyword": ["Grayscale Image Denoising", "Grayscale Image Denoising"], "id": "e823dba7-99c0-4dde-9bd0-85a99ad09926", "image_url": "https://production-media.paperswithcode.com/datasets/Set12-0000003300-bc4aa511.jpg"}, {"title": "Set14", "short_description": "The **Set14** dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models.\r\nImage Source: [https://www.ece.rice.edu/~wakin/images/](https://www.ece.rice.edu/~wakin/images/)", "price": 377, "created_at": "2025-01-09 13:44:40.904676", "keyword": ["Image Super-Resolution", "Blind Super-Resolution"], "id": "56b082b7-f96e-4145-a02b-6790ba32c9d3", "image_url": "https://production-media.paperswithcode.com/datasets/Set14-0000003442-673ab28f.jpg"}, {"title": "Set5", "short_description": "The **Set5** dataset is a dataset consisting of 5 images (\u201cbaby\u201d, \u201cbird\u201d, \u201cbutterfly\u201d, \u201chead\u201d, \u201cwoman\u201d) commonly used for testing performance of Image Super-Resolution models.\r\nImage Source: [http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html)", "price": 10, "created_at": "2025-01-09 13:44:40.904740", "keyword": ["Image Super-Resolution", "Blind Super-Resolution", "Compressive Sensing", "Image Super-Resolution", "Blind Super-Resolution", "Compressive Sensing"], "id": "4365f2c6-f742-48cb-a10e-5a0f2e4d5a6d", "image_url": "https://production-media.paperswithcode.com/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg"}, {"title": "ShanghaiTech", "short_description": "The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.\r\n\r\nSource: [Iterative Crowd Counting](https://arxiv.org/abs/1807.09959)\r\n\r\nImage Source: [Li et al](https://www.researchgate.net/figure/Test-images-from-the-ShanghaiTech-A-28-dataset-The-goal-of-this-paper-is-to-calculate_fig1_322652466)", "price": 928, "created_at": "2025-01-09 13:44:40.904807", "keyword": ["Anomaly Detection", "Crowd Counting", "Abnormal Event Detection In Video", "Cross-Part Crowd Counting"], "id": "1d31c922-12b5-4408-9776-0f242d2c4c01", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.19.39_PM.png"}, {"title": "ShapeNet", "short_description": "**ShapeNet** is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).\r\n\r\nSource: [A review on deep learning techniques for 3D sensed data classification](https://arxiv.org/abs/1907.04444)\r\nImage Source: [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/abs/1512.03012)", "price": 915, "created_at": "2025-01-09 13:44:40.904871", "keyword": ["Semantic Segmentation", "3D Reconstruction", "Novel View Synthesis", "Single-View 3D Reconstruction", "Point Cloud Generation", "3D Object Reconstruction", "Point Cloud Completion", "3D Part Segmentation", "Training-free 3D Part Segmentation", "Semantic Segmentation", "3D Reconstruction", "Novel View Synthesis", "Single-View 3D Reconstruction", "Point Cloud Generation", "3D Object Reconstruction", "Point Cloud Completion", "3D Part Segmentation", "Training-free 3D Part Segmentation"], "id": "06a4c6a6-4aef-45d6-8561-7f185441a37a", "image_url": "https://production-media.paperswithcode.com/datasets/7c995288-df15-4560-8faf-cc4604df2a7f.png"}, {"title": "Sim10k", "short_description": "SIM10k is a synthetic dataset containing 10,000 images, which is rendered from the video game Grand Theft Auto V (GTA5).\r\n\r\nSource: [Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation](https://arxiv.org/abs/2003.10275)\r\nImage Source: [https://arxiv.org/pdf/1610.01983.pdf](https://arxiv.org/pdf/1610.01983.pdf)", "price": 654, "created_at": "2025-01-09 13:44:40.904936", "keyword": ["Domain Adaptation", "Unsupervised Domain Adaptation"], "id": "99b7e51f-5746-4af2-ba05-6f37d9bc0b8f", "image_url": "https://production-media.paperswithcode.com/datasets/Sim10k-0000003291-4e35cf32.jpg"}, {"title": "SimpleQuestions", "short_description": "**SimpleQuestions** is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively.\r\n\r\nSource: [Hierarchical Memory Networks](https://arxiv.org/abs/1605.07427)\r\nImage Source: [https://paperswithcode.com/paper/large-scale-simple-question-answering-with/](https://paperswithcode.com/paper/large-scale-simple-question-answering-with/)", "price": 43, "created_at": "2025-01-09 13:44:40.905000", "keyword": ["Question Answering", "Question Answering"], "id": "d126b663-bbc7-4f9f-b30c-5f1e0b3abf7d", "image_url": "https://production-media.paperswithcode.com/datasets/SimpleQuestions-0000000616-9a0cdef7_p1aGpXa.jpg"}, {"title": "Sketch", "short_description": "The **Sketch** dataset contains over 20,000 sketches evenly distributed over 250 object categories.\r\n\r\nSource: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)\r\nImage Source: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)", "price": 220, "created_at": "2025-01-09 13:44:40.905064", "keyword": ["Continual Learning"], "id": "b0a5e651-46be-4472-9508-6d31f5c1da18", "image_url": "https://production-media.paperswithcode.com/datasets/Sketch-0000003423-154e87e7.jpg"}, {"title": "Sports-1M", "short_description": "The **Sports-1M** dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class.\r\n\r\nSource: [Review of Action Recognition and Detection Methods](https://arxiv.org/abs/1610.06906)\r\n\r\nImage Source: [Computer Vision for Sports](https://www.researchgate.net/publication/316477606_Computer_vision_for_sports_Current_applications_and_research_topics)", "price": 388, "created_at": "2025-01-09 13:44:40.905128", "keyword": ["Action Recognition", "Action Recognition In Videos"], "id": "18881774-dd09-4876-94e1-3787b2227731", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.05.22_PM.png"}, {"title": "Sprites", "short_description": "The **Sprites** dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.\r\n\r\nSource: [Challenges in Disentangling Independent Factors of Variation](https://arxiv.org/abs/1711.02245)", "price": 360, "created_at": "2025-01-09 13:44:40.905192", "keyword": ["Video Prediction", "Imputation", "Disentanglement"], "id": "acfd9b81-b969-4da5-9ab2-80961ba9fa1c", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_8.49.11_PM.png"}, {"title": "Stanford Cars", "short_description": "The **Stanford Cars** dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360\u00d7240.\r\n\r\nSource: [View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network](https://arxiv.org/abs/1702.01721)\r\nImage Source: [https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)", "price": 64, "created_at": "2025-01-09 13:44:40.905255", "keyword": ["Image Classification", "Image Generation", "Few-Shot Image Classification", "Continual Learning", "Image Clustering", "Fine-Grained Image Classification", "Neural Architecture Search", "Prompt Engineering", "Learning with coarse labels"], "id": "41e158bc-668c-4743-800b-2438de8626b9", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_10.52.45_AM.png"}, {"title": "Stanford Dogs", "short_description": "The **Stanford Dogs** dataset contains 20,580 images of 120 classes of dogs from around the world, which are divided into 12,000 images for training and 8,580 images for testing.\n\nSource: [Universal-to-Specific Framework for Complex Action Recognition](https://arxiv.org/abs/2007.06149)\nImage Source: [https://www.tensorflow.org/datasets/catalog/stanford_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs)", "price": 259, "created_at": "2025-01-09 13:44:40.905320", "keyword": ["Image Generation", "Few-Shot Image Classification", "Image Clustering", "Fine-Grained Image Classification"], "id": "43ab1dfa-a973-4cfd-9e41-84db73e40a8a", "image_url": "https://production-media.paperswithcode.com/datasets/Stanford_Dogs-0000000577-91cb15b5_1ABtNf7.jpg"}, {"title": "Stanford Online Products", "short_description": "**Stanford Online Products** (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing\r\n\r\nSource: [Deep Metric Learning with Alternating Projections onto Feasible Sets](https://arxiv.org/abs/1907.07585)\r\nImage Source: [https://cvgl.stanford.edu/projects/lifted_struct/](https://cvgl.stanford.edu/projects/lifted_struct/)", "price": 304, "created_at": "2025-01-09 13:44:40.905384", "keyword": ["Image Classification", "Image Retrieval", "Fine-Grained Image Classification", "Metric Learning", "Learning with coarse labels"], "id": "4c28b1a9-605c-4971-a918-4306be8019b3", "image_url": "https://production-media.paperswithcode.com/datasets/Stanford_Online_Products-0000001546-f2a51db7_ZmycM26.jpeg"}, {"title": "SumMe", "short_description": "The **SumMe** dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\r\n\r\nSource: [https://gyglim.github.io/me/vsum/index.html](https://gyglim.github.io/me/vsum/index.html)\r\nImage Source: [https://gyglim.github.io/me/vsum/index.html](https://gyglim.github.io/me/vsum/index.html)", "price": 275, "created_at": "2025-01-09 13:44:40.905447", "keyword": ["Video Summarization", "Unsupervised Video Summarization", "Supervised Video Summarization"], "id": "da978a7e-3005-4b88-816f-843418a8c4cb", "image_url": "https://production-media.paperswithcode.com/datasets/SumMe-0000000410-19aa1ce2_rBL4aRd.jpg"}, {"title": "Switchboard-1 Corpus", "short_description": "The Switchboard-1 Telephone Speech Corpus (LDC97S62) consists of approximately 260 hours of speech and was originally collected by Texas Instruments in 1990-1, under DARPA sponsorship. The first release of the corpus was published by NIST and distributed by the LDC in 1992-3.\r\n\r\nSwitchboard is a collection of about 2,400 two-sided telephone conversations among 543 speakers (302 male, 241 female) from all areas of the United States. A computer-driven robot operator system handled the calls, giving the caller appropriate recorded prompts, selecting and dialing another person (the callee) to take part in a conversation, introducing a topic for discussion and recording the speech from the two subjects into separate channels until the conversation was finished. About 70 topics were provided, of which about 50 were used frequently. Selection of topics and callees was constrained so that: (1) no two speakers would converse together more than once and (2) no one spoke more than once on a given topic.\r\n\r\nSource: [https://catalog.ldc.upenn.edu/LDC97S62](https://catalog.ldc.upenn.edu/LDC97S62)", "price": 547, "created_at": "2025-01-09 13:44:40.905511", "keyword": ["Dialogue Act Classification"], "id": "f348328e-fe02-482c-880c-7bf882a205e2", "image_url": ""}, {"title": "T-LESS", "short_description": "**T-LESS** is a dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects.\r\n\r\nSource: [http://cmp.felk.cvut.cz/t-less/](http://cmp.felk.cvut.cz/t-less/)\r\nImage Source: [http://cmp.felk.cvut.cz/t-less/](http://cmp.felk.cvut.cz/t-less/)", "price": 378, "created_at": "2025-01-09 13:44:40.905575", "keyword": ["6D Pose Estimation using RGBD", "6D Pose Estimation using RGB"], "id": "c88c4a81-6935-464c-a213-dc5eb855b036", "image_url": "https://production-media.paperswithcode.com/datasets/T-LESS-0000000752-ad283df4_LqkL3IS.jpg"}, {"title": "TDIUC", "short_description": "**Task Directed Image Understanding Challenge** (**TDIUC**) dataset is a Visual Question Answering dataset which consists of 1.6M questions and 170K images sourced from MS COCO and the Visual Genome Dataset. The image-question pairs are split into 12 categories and 4 additional evaluation matrices which help evaluate models\u2019 robustness against answer imbalance and its ability to answer questions that require higher reasoning capability. The TDIUC dataset divides the VQA paradigm into 12 different task directed question types. These include questions that require a simpler task (e.g., object presence, color attribute) and more complex tasks (e.g., counting, positional reasoning). The dataset includes also an \u201cAbsurd\u201d question category in which questions are irrelevant to the image contents to help balance the dataset.\r\n\r\nSource: [Question-Agnostic Attention for Visual Question Answering](https://arxiv.org/abs/1908.03289)\r\nImage Source: [https://kushalkafle.com/projects/tdiuc.html](https://kushalkafle.com/projects/tdiuc.html)", "price": 771, "created_at": "2025-01-09 13:44:40.905641", "keyword": ["Visual Question Answering (VQA)"], "id": "4720c7ec-a138-4e6b-ac43-b58b637af6a3", "image_url": "https://production-media.paperswithcode.com/datasets/TDIUC-0000002087-9b21c5f9_FqZ1Krv.jpg"}, {"title": "TGIF", "short_description": "The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.\r\n\r\nSource: [GitHub](https://github.com/raingo/TGIF-Release)", "price": 352, "created_at": "2025-01-09 13:44:40.905705", "keyword": ["Question Answering", "Video Retrieval", "Video Question Answering", "Video Captioning"], "id": "07f72679-de1e-4cda-9af1-a1f03e97d68f", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-08_at_16.52.04.png"}, {"title": "THCHS-30", "short_description": "THCHS-30 is a free Chinese speech database\r\nTHCHS-30 that can be used to build a full-fledged\r\nChinese speech recognition system.\r\n\r\nSource: [THCHS-30 : A Free Chinese Speech Corpus](/paper/thchs-30-a-free-chinese-speech-corpus)", "price": 83, "created_at": "2025-01-09 13:44:40.905770", "keyword": ["Speech Recognition", "Speech Synthesis", "Multi-Task Learning"], "id": "1e8a4b37-772f-4b3b-a884-b4cf6733bfe2", "image_url": ""}, {"title": "TIMIT", "short_description": "The **TIMIT** Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used for evaluation of automatic speech recognition systems. It consists of recordings of 630 speakers of 8 dialects of American English each reading 10 phonetically-rich sentences. It also comes with the word and phone-level transcriptions of the speech.\r\n\r\nSource: [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580)\r\nImage Source: [https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/](https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/)", "price": 682, "created_at": "2025-01-09 13:44:40.905833", "keyword": ["Speech Recognition", "Clustering"], "id": "73d1be8f-9757-4415-b562-6c5c419ed7c3", "image_url": "https://production-media.paperswithcode.com/datasets/TIMIT-0000000368-e91a71e9_MKlcc72.jpg"}, {"title": "TREC-10", "short_description": "A question type classification dataset with 6 classes for questions about a person, location, numeric information, etc. The test split has 500 questions, and the training split has 5452 questions.\r\n\r\nPaper: [Learning Question Classifiers](https://www.aclweb.org/anthology/C02-1150/)", "price": 556, "created_at": "2025-01-09 13:44:40.905897", "keyword": ["Text Classification"], "id": "7075aa3b-b198-40fd-ae9c-a8560c1a4f22", "image_url": ""}, {"title": "TVQA", "short_description": "The **TVQA** dataset is a large-scale vido dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer.\r\n\r\nSource: [Two-stream Spatiotemporal Feature for Video QA Task](https://arxiv.org/abs/1907.05006)\r\nImage Source: [https://arxiv.org/abs/1809.01696](https://arxiv.org/abs/1809.01696)", "price": 631, "created_at": "2025-01-09 13:44:40.905961", "keyword": ["Zero-Shot Learning", "Video Question Answering", "Zero-Shot Video Question Answer"], "id": "0ae7bc9b-40e2-4c66-ba7b-2ab8114b8377", "image_url": "https://production-media.paperswithcode.com/datasets/TVQA-0000002861-5830f9fb_AJSRM4i.jpg"}, {"title": "TableBank", "short_description": "To address the need for a standard open domain table benchmark dataset, the author propose a novel weak supervision approach to automatically create the TableBank, which is orders of magnitude larger than existing human labeled datasets for table analysis. Distinct from traditional weakly supervised training set, our approach can obtain not only large scale but also high quality training data.\r\n\r\nNowadays, there are a great number of electronic documents on the web such as Microsoft Word (.docx) and Latex (.tex) files. These online documents contain mark-up tags for tables in their source code by nature. Intuitively, one can manipulate these source code by adding bounding box using the mark-up language within each document. For Word documents, the internal Office XML code can be modified where the borderline of each table is identified. For Latex documents, the tex code can be also modified where bounding boxes of tables are recognized. In this way, high-quality labeled data is created for a variety of domains such as business documents, official fillings, research papers etc, which is tremendously beneficial for large-scale table analysis tasks.\r\n\r\nThe TableBank dataset totally consists of 417,234 high quality labeled tables as well as their original documents in a variety of domains.\r\n\r\nSource: [TableBank](https://github.com/doc-analysis/TableBank)", "price": 215, "created_at": "2025-01-09 13:44:40.906024", "keyword": ["Table Detection"], "id": "6efd92f4-b237-4ece-bc67-5cc4f168776b", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-12_at_09.48.34.png"}, {"title": "Tatoeba", "short_description": "Tatoeba is a free collection of example sentences with translations geared towards foreign language learners. It is available in more than 400 languages. Its name comes from the Japanese phrase \u201ctatoeba\u201d (\u4f8b\u3048\u3070), meaning \u201cfor example\u201d. It is written and maintained by a community of volunteers through a model of open collaboration. Individual contributors are known as Tatoebans.", "price": 606, "created_at": "2025-01-09 13:44:40.906089", "keyword": ["Machine Translation", "Language Modelling", "Sequence-to-sequence Language Modeling", "Cross-Lingual Transfer", "Translation deu-eng", "Translation eng-deu", "Translation eng-ces", "Translation eng-fra", "Translation fra-eng", "Translation ces-eng", "Translation rus-eng", "Translation eng-rus", "Translation eng-spa", "Translation spa-eng", "Translation fin-eng", "Translation fra-spa", "Translation spa-fra", "Translation deu-spa", "Translation eng-fin", "Translation eng-tur", "Translation tur-eng", "Translation rus-fra", "Translation rus-spa", "Translation deu-rus", "Translation fra-rus", "Translation spa-rus", "Translation ces-rus", "Translation eng-est", "Translation eng-isl", "Translation eng-lav", "Translation eng-lit", "Translation eng-ron", "Translation est-eng", "Translation isl-eng", "Translation lav-eng", "Translation lit-eng", "Translation pol-eng", "Translation eng-hrv", "Translation eng-srp_Cyrl", "Translation afr-deu", "Translation afr-eng", "Translation deu-afr", "Translation eng-afr", "Translation eng-nld", "Translation nld-eng", "Translation rus-deu", "Translation bel-fra", "Translation ukr-fra", "Translation rus-ces", "Translation eng-ara", "Translation eng-hun", "Translation eng-ita", "Translation ara-eng", "Translation hun-eng", "Translation ita-eng", "Translation cat-ita", "Translation fra-ita", "Translation ita-cat", "Translation ita-fra", "Translation ita-ron", "Translation ita-spa", "Translation ron-ita", "Translation spa-ita", "Translation nob-swe", "Translation afr-nld", "Translation deu-nld", "Translation ltz-deu", "Translation ltz-eng", "Translation ltz-nld", "Translation nld-afr", "Translation nld-deu", "Translation fry-eng", "Translation fry-nld", "Translation hrx-deu", "Translation hrx-eng", "Translation nds-deu", "Translation nds-eng", "Translation nds-nld", "Translation nld-fry", "Translation nld-nds", "Translation ukr-eng", "Translation ukr-ces", "Translation ukr-hun", "Translation ukr-tur", "Translation rus-lav", "Translation rus-lit", "Translation rus-fin", "Translation ukr-deu", "Translation fin-rus", "Translation rus-dan", "Translation rus-nob", "Translation rus-swe", "Translation rus-ita", "Translation ukr-ita", "Translation bel-ita", "Translation rus-por", "Translation ukr-por", "Translation ukr-spa", "Translation bel-spa", "Translation rus-ukr", "Translation ukr-rus", "Translation rus-bul", "Translation rus-slv", "Translation rus-srp_Cyrl", "Translation ukr-bul", "Translation ukr-hrv", "Translation ukr-srp_Cyrl", "Translation lav-rus", "Translation lit-rus", "Translation ces-ukr", "Translation deu-ukr", "Translation eng-ukr", "Translation fra-ukr", "Translation dan-rus", "Translation nob-rus", "Translation swe-rus", "Translation ita-rus", "Translation por-rus", "Translation por-ukr", "Translation bul-rus", "Translation bul-ukr", "Translation hrv-ukr", "Translation slv-rus", "Translation srp_Cyrl-rus", "Translation srp_Cyrl-ukr", "Translation pol-rus", "Translation eng-bul", "Translation eng-cat", "Translation eng-ell", "Translation eng-dan", "Translation eng-nob", "Translation eng-swe", "Translation eng-por", "Translation bul-eng", "Translation cat-eng", "Translation cym-eng", "Translation gle-eng", "Translation ell-eng", "Translation dan-eng", "Translation nob-eng", "Translation swe-eng", "Translation hrv-eng", "Translation srp_Cyrl-eng", "Translation heb-eng", "Translation mkd-eng", "Translation slv-eng", "Translation kor-eng", "Translation cat-fra", "Translation cat-por", "Translation cat-spa", "Translation fra-cat", "Translation fra-por", "Translation fra-ron", "Translation glg-por", "Translation glg-spa", "Translation ita-por", "Translation oci-fra", "Translation por-cat", "Translation por-fra", "Translation por-glg", "Translation por-ita", "Translation por-ron", "Translation por-spa", "Translation ron-fra", "Translation ron-por", "Translation ron-spa", "Translation spa-cat", "Translation spa-glg", "Translation spa-por", "Translation spa-ron", "Translation dan-nob", "Translation dan-swe", "Translation nob-dan", "Translation swe-dan", "Translation swe-nob", "Translation bul-fra", "Translation bul-ita", "Translation bul-spa", "Translation hrv-fra", "Translation hrv-spa", "Translation mkd-spa", "Translation dan-fra", "Translation dan-ita", "Translation dan-por", "Translation dan-spa", "Translation isl-ita", "Translation isl-spa", "Translation nob-fra", "Translation nob-spa", "Translation swe-fra", "Translation swe-ita", "Translation swe-por", "Translation swe-spa", "Translation ita-ara", "Translation spa-ara", "Translation rus-ron", "Translation ukr-cat", "Translation fra-heb", "Translation ita-heb", "Translation por-heb", "Translation spa-heb", "Translation swe-pol", "Translation fra-tur", "Translation ita-tur", "Translation por-tur", "Translation ron-tur", "Translation spa-tur", "Translation ara-fra", "Translation ara-ita", "Translation ara-spa", "Translation ita-lit", "Translation spa-lit", "Translation bul-deu", "Translation hrv-deu", "Translation slv-deu", "Translation dan-tur", "Translation swe-tur", "Translation heb-fra", "Translation heb-ita", "Translation heb-por", "Translation heb-spa", "Translation deu-dan", "Translation deu-isl", "Translation deu-nob", "Translation deu-swe", "Translation fas-fra", "Translation deu-nds", "Translation gos-deu", "Translation gos-eng", "Translation gos-nld", "Translation bel-eng", "Translation bel-deu", "Translation multi-fra", "Translation bel-rus", "Translation bel-ukr", "Translation rus-bel", "Translation ukr-bel", "Translation rus-hbs", "Translation rus-srp_Latn", "Translation ukr-hbs", "Translation ukr-srp_Latn", "Translation bel-pol", "Translation rus-pol", "Translation ukr-pol", "Translation deu-bel", "Translation eng-bel", "Translation fra-bel", "Translation hun-ukr", "Translation ita-bel", "Translation ita-ukr", "Translation spa-bel", "Translation spa-ukr", "Translation tur-ukr", "Translation hbs-rus", "Translation hbs-ukr", "Translation srp_Latn-rus", "Translation srp_Latn-ukr", "Translation pol-bel", "Translation pol-ukr", "Translation eng-nno", "Translation bre-eng", "Translation fao-eng", "Translation nno-eng", "Translation bos_Latn-eng", "Translation hbs-eng", "Translation srp_Latn-eng", "Translation lad-spa", "Translation lad_Latn-spa", "Translation pms-ita", "Translation nno-nob", "Translation nob-nno", "Translation hbs-fra", "Translation hbs-ita", "Translation hbs-spa", "Translation srp_Latn-ita", "Translation zho-jpn", "Translation hbs-deu", "Translation srp_Latn-deu", "Translation spa-eus", "Translation eus-spa", "Translation eng-hin", "Translation eng-jpg", "Translation eng-mld", "Translation eng-bos_Latn", "Translation eng-hbs", "Translation eng-srp_Latn", "Translation bar-bar", "Translation deu-deu", "Translation deu-tpi", "Translation drt-deu", "Translation drt-eng", "Translation drt-fry", "Translation drt-nld", "Translation eng-eng", "Translation eng-sco", "Translation eng-srn", "Translation eng-tpi", "Translation enm-deu", "Translation enm-eng", "Translation enm-fry", "Translation enm-ltz", "Translation enm-nld", "Translation fry-deu", "Translation fry-ltz", "Translation gos-afr", "Translation gos-fry", "Translation gsw-deu", "Translation gsw-nld", "Translation lim-deu", "Translation lim-eng", "Translation lim-nld", "Translation ltz-fry", "Translation multi-multi", "Translation nld-nld", "Translation nld-sco", "Translation ofs-bar", "Translation pdc-deu", "Translation pdc-eng", "Translation sco-eng", "Translation sco-nld", "Translation srn-eng", "Translation stq-deu", "Translation stq-eng", "Translation stq-nld", "Translation swg-eng", "Translation swg-nld", "Translation tpi-deu", "Translation tpi-eng", "Translation zea-deu", "Translation zea-eng", "Translation zea-fry", "Translation zea-nds", "Translation zea-nld", "Sentence Embedding", "Machine Translation", "Language Modelling", "Sequence-to-sequence Language Modeling", "Cross-Lingual Transfer", "Translation deu-eng", "Translation eng-deu", "Translation eng-ces", "Translation eng-fra", "Translation fra-eng", "Translation ces-eng", "Translation rus-eng", "Translation eng-rus", "Translation eng-spa", "Translation spa-eng", "Translation fin-eng", "Translation fra-spa", "Translation spa-fra", "Translation deu-spa", "Translation eng-fin", "Translation eng-tur", "Translation tur-eng", "Translation rus-fra", "Translation rus-spa", "Translation deu-rus", "Translation fra-rus", "Translation spa-rus", "Translation ces-rus", "Translation eng-est", "Translation eng-isl", "Translation eng-lav", "Translation eng-lit", "Translation eng-ron", "Translation est-eng", "Translation isl-eng", "Translation lav-eng", "Translation lit-eng", "Translation pol-eng", "Translation eng-hrv", "Translation eng-srp_Cyrl", "Translation afr-deu", "Translation afr-eng", "Translation deu-afr", "Translation eng-afr", "Translation eng-nld", "Translation nld-eng", "Translation rus-deu", "Translation bel-fra", "Translation ukr-fra", "Translation rus-ces", "Translation eng-ara", "Translation eng-hun", "Translation eng-ita", "Translation ara-eng", "Translation hun-eng", "Translation ita-eng", "Translation cat-ita", "Translation fra-ita", "Translation ita-cat", "Translation ita-fra", "Translation ita-ron", "Translation ita-spa", "Translation ron-ita", "Translation spa-ita", "Translation nob-swe", "Translation afr-nld", "Translation deu-nld", "Translation ltz-deu", "Translation ltz-eng", "Translation ltz-nld", "Translation nld-afr", "Translation nld-deu", "Translation fry-eng", "Translation fry-nld", "Translation hrx-deu", "Translation hrx-eng", "Translation nds-deu", "Translation nds-eng", "Translation nds-nld", "Translation nld-fry", "Translation nld-nds", "Translation ukr-eng", "Translation ukr-ces", "Translation ukr-hun", "Translation ukr-tur", "Translation rus-lav", "Translation rus-lit", "Translation rus-fin", "Translation ukr-deu", "Translation fin-rus", "Translation rus-dan", "Translation rus-nob", "Translation rus-swe", "Translation rus-ita", "Translation ukr-ita", "Translation bel-ita", "Translation rus-por", "Translation ukr-por", "Translation ukr-spa", "Translation bel-spa", "Translation rus-ukr", "Translation ukr-rus", "Translation rus-bul", "Translation rus-slv", "Translation rus-srp_Cyrl", "Translation ukr-bul", "Translation ukr-hrv", "Translation ukr-srp_Cyrl", "Translation lav-rus", "Translation lit-rus", "Translation ces-ukr", "Translation deu-ukr", "Translation eng-ukr", "Translation fra-ukr", "Translation dan-rus", "Translation nob-rus", "Translation swe-rus", "Translation ita-rus", "Translation por-rus", "Translation por-ukr", "Translation bul-rus", "Translation bul-ukr", "Translation hrv-ukr", "Translation slv-rus", "Translation srp_Cyrl-rus", "Translation srp_Cyrl-ukr", "Translation pol-rus", "Translation eng-bul", "Translation eng-cat", "Translation eng-ell", "Translation eng-dan", "Translation eng-nob", "Translation eng-swe", "Translation eng-por", "Translation bul-eng", "Translation cat-eng", "Translation cym-eng", "Translation gle-eng", "Translation ell-eng", "Translation dan-eng", "Translation nob-eng", "Translation swe-eng", "Translation hrv-eng", "Translation srp_Cyrl-eng", "Translation heb-eng", "Translation mkd-eng", "Translation slv-eng", "Translation kor-eng", "Translation cat-fra", "Translation cat-por", "Translation cat-spa", "Translation fra-cat", "Translation fra-por", "Translation fra-ron", "Translation glg-por", "Translation glg-spa", "Translation ita-por", "Translation oci-fra", "Translation por-cat", "Translation por-fra", "Translation por-glg", "Translation por-ita", "Translation por-ron", "Translation por-spa", "Translation ron-fra", "Translation ron-por", "Translation ron-spa", "Translation spa-cat", "Translation spa-glg", "Translation spa-por", "Translation spa-ron", "Translation dan-nob", "Translation dan-swe", "Translation nob-dan", "Translation swe-dan", "Translation swe-nob", "Translation bul-fra", "Translation bul-ita", "Translation bul-spa", "Translation hrv-fra", "Translation hrv-spa", "Translation mkd-spa", "Translation dan-fra", "Translation dan-ita", "Translation dan-por", "Translation dan-spa", "Translation isl-ita", "Translation isl-spa", "Translation nob-fra", "Translation nob-spa", "Translation swe-fra", "Translation swe-ita", "Translation swe-por", "Translation swe-spa", "Translation ita-ara", "Translation spa-ara", "Translation rus-ron", "Translation ukr-cat", "Translation fra-heb", "Translation ita-heb", "Translation por-heb", "Translation spa-heb", "Translation swe-pol", "Translation fra-tur", "Translation ita-tur", "Translation por-tur", "Translation ron-tur", "Translation spa-tur", "Translation ara-fra", "Translation ara-ita", "Translation ara-spa", "Translation ita-lit", "Translation spa-lit", "Translation bul-deu", "Translation hrv-deu", "Translation slv-deu", "Translation dan-tur", "Translation swe-tur", "Translation heb-fra", "Translation heb-ita", "Translation heb-por", "Translation heb-spa", "Translation deu-dan", "Translation deu-isl", "Translation deu-nob", "Translation deu-swe", "Translation fas-fra", "Translation deu-nds", "Translation gos-deu", "Translation gos-eng", "Translation gos-nld", "Translation bel-eng", "Translation bel-deu", "Translation multi-fra", "Translation bel-rus", "Translation bel-ukr", "Translation rus-bel", "Translation ukr-bel", "Translation rus-hbs", "Translation rus-srp_Latn", "Translation ukr-hbs", "Translation ukr-srp_Latn", "Translation bel-pol", "Translation rus-pol", "Translation ukr-pol", "Translation deu-bel", "Translation eng-bel", "Translation fra-bel", "Translation hun-ukr", "Translation ita-bel", "Translation ita-ukr", "Translation spa-bel", "Translation spa-ukr", "Translation tur-ukr", "Translation hbs-rus", "Translation hbs-ukr", "Translation srp_Latn-rus", "Translation srp_Latn-ukr", "Translation pol-bel", "Translation pol-ukr", "Translation eng-nno", "Translation bre-eng", "Translation fao-eng", "Translation nno-eng", "Translation bos_Latn-eng", "Translation hbs-eng", "Translation srp_Latn-eng", "Translation lad-spa", "Translation lad_Latn-spa", "Translation pms-ita", "Translation nno-nob", "Translation nob-nno", "Translation hbs-fra", "Translation hbs-ita", "Translation hbs-spa", "Translation srp_Latn-ita", "Translation zho-jpn", "Translation hbs-deu", "Translation srp_Latn-deu", "Translation spa-eus", "Translation eus-spa", "Translation eng-hin", "Translation eng-jpg", "Translation eng-mld", "Translation eng-bos_Latn", "Translation eng-hbs", "Translation eng-srp_Latn", "Translation bar-bar", "Translation deu-deu", "Translation deu-tpi", "Translation drt-deu", "Translation drt-eng", "Translation drt-fry", "Translation drt-nld", "Translation eng-eng", "Translation eng-sco", "Translation eng-srn", "Translation eng-tpi", "Translation enm-deu", "Translation enm-eng", "Translation enm-fry", "Translation enm-ltz", "Translation enm-nld", "Translation fry-deu", "Translation fry-ltz", "Translation gos-afr", "Translation gos-fry", "Translation gsw-deu", "Translation gsw-nld", "Translation lim-deu", "Translation lim-eng", "Translation lim-nld", "Translation ltz-fry", "Translation multi-multi", "Translation nld-nld", "Translation nld-sco", "Translation ofs-bar", "Translation pdc-deu", "Translation pdc-eng", "Translation sco-eng", "Translation sco-nld", "Translation srn-eng", "Translation stq-deu", "Translation stq-eng", "Translation stq-nld", "Translation swg-eng", "Translation swg-nld", "Translation tpi-deu", "Translation tpi-eng", "Translation zea-deu", "Translation zea-eng", "Translation zea-fry", "Translation zea-nds", "Translation zea-nld", "Sentence Embedding"], "id": "09d3da0f-3492-4bb6-baf7-4307c298df3a", "image_url": "https://production-media.paperswithcode.com/datasets/tatoeba.png"}, {"title": "Text8", "short_description": "Desc: [About of Text8](http://mattmahoney.net/dc/textdata.html)", "price": 432, "created_at": "2025-01-09 13:44:40.906153", "keyword": ["Language Modelling"], "id": "fdb51d6a-260d-48ec-8159-50e2b73e1bde", "image_url": ""}, {"title": "Tiny Images", "short_description": "The image dataset TinyImages contains 80 million images of size 32\u00d732 collected from the Internet, crawling the words in WordNet. \r\n\r\n**The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.**\r\n\r\nSource: [Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments](https://arxiv.org/abs/1502.03032)", "price": 955, "created_at": "2025-01-09 13:44:40.906216", "keyword": ["Image Classification", "Image Retrieval", "Quantization"], "id": "00f2a620-db3a-4b94-b927-d05ec4e46e35", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_6.19.39_PM.png"}, {"title": "Tobacco-3482", "short_description": "The Tobacco-3482 dataset consists of document images belonging to 10 classes such as letter, form, email, resume, memo, etc. The dataset has 3482 images.", "price": 503, "created_at": "2025-01-09 13:44:40.906280", "keyword": ["Document Text Classification", "Document Image Classification", "Multi-Modal Document Classification"], "id": "7e9c474a-f58d-44ef-a89a-1782b23797ed", "image_url": ""}, {"title": "Total-Text", "short_description": "**Total-Text** is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively.\r\n\r\nSource: [Convolutional Character Networks](https://arxiv.org/abs/1910.07954)\r\nImage Source: [https://github.com/cs-chan/Total-Text-Dataset](https://github.com/cs-chan/Total-Text-Dataset)", "price": 876, "created_at": "2025-01-09 13:44:40.906343", "keyword": ["Scene Text Detection", "Text Spotting"], "id": "f58907d7-0fb0-4401-97dd-16e09eac0789", "image_url": "https://production-media.paperswithcode.com/datasets/Total-Text-0000000286-285bd548_FafV5x8.gif"}, {"title": "TotalCapture", "short_description": "The **TotalCapture** dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions.\r\n\r\nSource: [Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras](https://arxiv.org/abs/1908.03030)\r\nImage Source: [https://cvssp.org/data/totalcapture/](https://cvssp.org/data/totalcapture/)", "price": 317, "created_at": "2025-01-09 13:44:40.906410", "keyword": ["Pose Estimation", "3D Human Pose Estimation", "3D Absolute Human Pose Estimation"], "id": "2a543475-8241-49cd-98c0-1b1620cd09d1", "image_url": "https://production-media.paperswithcode.com/datasets/TotalCapture-0000003301-ec891e11.jpg"}, {"title": "TrackingNet", "short_description": "**TrackingNet** is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames.\r\n\r\nSource: [Learning the Model Update for Siamese Trackers](https://arxiv.org/abs/1908.00855)\r\nImage Source: [https://arxiv.org/abs/1803.10794](https://arxiv.org/abs/1803.10794)", "price": 434, "created_at": "2025-01-09 13:44:40.906478", "keyword": ["Visual Object Tracking", "Visual Tracking"], "id": "aa2b4214-f648-433a-a090-91c9d7f1db27", "image_url": "https://production-media.paperswithcode.com/datasets/TrackingNet-0000000310-70fd8829_Bodhk7Q.jpg"}, {"title": "TrajNet", "short_description": "The **TrajNet** Challenge represents a large multi-scenario forecasting benchmark. The challenge consists on  predicting 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t\u22127,t\u22126,\u2026,t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t+1,\u2026,t+12. The 8-12-value protocol is consistent with the most trajectory forecasting approaches, usually focused on the 5-dataset ETH-univ + ETH-hotel + UCY-zara01 + UCY-zara02 + UCY-univ. Trajnet extends substantially the 5-dataset scenario by diversifying the training data, thus stressing the flexibility and generalization one approach has to exhibit when it comes to unseen scenery/situations. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel (orthogonal bird\u2019s eye flight view, moving people), 2) Crowds UCY (3 datasets, tilted bird\u2019s eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS (multisensor, different human activities) and 4) Stanford Drone Dataset (8 scenes, high orthogonal bird\u2019s eye flight view, different agents as people, cars etc. ), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants).\r\n\r\nSource: [Transformer Networks for Trajectory Forecasting](https://arxiv.org/abs/2003.08111)\r\nImage Source: [http://trajnet.stanford.edu/](http://trajnet.stanford.edu/)", "price": 628, "created_at": "2025-01-09 13:44:40.906542", "keyword": ["Trajectory Prediction", "Trajectory Forecasting", "Decision Making"], "id": "0b3965b0-b39e-49d5-a1c9-e57c9698b95d", "image_url": "https://production-media.paperswithcode.com/datasets/TrajNet-0000003299-d4de1e43.jpg"}, {"title": "TrecQA", "short_description": "**Text Retrieval Conference Question Answering** (**TrecQA**) is a dataset created from the TREC-8 (1999) to TREC-13 (2004) Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ. The commonly used clean version of the dataset excludes questions in development and test sets with no answers or only positive/negative answers. The clean version has 1,229/65/68 questions and 53,417/1,117/1,442 question-answer pairs for the train/dev/test split.\r\n\r\nSource: [A Gated Self-attention Memory Network for Answer Selection](https://arxiv.org/abs/1909.09696)", "price": 755, "created_at": "2025-01-09 13:44:40.906608", "keyword": ["Question Answering", "Text Classification", "Answer Selection"], "id": "c6181f7d-f98f-4b9c-bf81-f4ceeb0a9635", "image_url": "https://production-media.paperswithcode.com/datasets/TrecQA-0000000620-7e287162_mF9eBq4.jpg"}, {"title": "TriviaQA", "short_description": "**TriviaQA** is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.\r\n\r\nSource: [Episodic Memory Reader: Learning What to Rememberfor Question Answering from Streaming Data](https://arxiv.org/abs/1903.06164)\r\nImage Source: [Joshi et al](https://arxiv.org/pdf/1705.03551v2.pdf)", "price": 548, "created_at": "2025-01-09 13:44:40.906672", "keyword": ["Text Generation", "Question Answering", "Reading Comprehension", "Open-Domain Question Answering", "Question Generation", "Text Generation", "Question Answering", "Reading Comprehension", "Open-Domain Question Answering", "Question Generation"], "id": "6f32d64e-e8a9-4220-aebd-ed5b79297253", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_2.06.45_PM.png"}, {"title": "UBIRIS.v2", "short_description": "The **UBIRIS.v2** iris dataset contains 11,102 iris images from 261 subjects with 10 images each subject. The images were captured under unconstrained conditions (at-a-distance, on-the-move and on the visible wavelength), with realistic noise factors.\n\nSource: [Constrained Design of Deep Iris Networks](https://arxiv.org/abs/1905.09481)\nImage Source: [https://arxiv.org/pdf/1905.09481.pdf](https://arxiv.org/pdf/1905.09481.pdf)", "price": 525, "created_at": "2025-01-09 13:44:40.906736", "keyword": ["Iris Segmentation", "Iris Recognition"], "id": "91978d0a-4d33-4cc5-abd2-9176e78fe817", "image_url": "https://production-media.paperswithcode.com/datasets/UBIRIS.v2-0000003394-8d4007c1.jpeg"}, {"title": "UCF101", "short_description": "**UCF101** dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \u00d7 240.\r\n\r\nSource: [Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification](https://arxiv.org/abs/1711.03273)\r\nImage Source: [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)", "price": 19, "created_at": "2025-01-09 13:44:40.906800", "keyword": ["Action Recognition", "Temporal Action Localization", "Few-Shot Learning", "Skeleton Based Action Recognition", "Action Classification", "Action Recognition In Videos", "Video Frame Interpolation", "Prompt Engineering", "Video Generation", "Zero-Shot Action Recognition", "Self-Supervised Action Recognition", "Text-to-Video Generation", "Few Shot Action Recognition", "Early Action Prediction", "Self-supervised Video Retrieval", "Open Set Action Recognition", "Self-Supervised Action Recognition Linear", "Action Recognition", "Temporal Action Localization", "Few-Shot Learning", "Skeleton Based Action Recognition", "Action Classification", "Action Recognition In Videos", "Video Frame Interpolation", "Prompt Engineering", "Video Generation", "Zero-Shot Action Recognition", "Self-Supervised Action Recognition", "Text-to-Video Generation", "Few Shot Action Recognition", "Early Action Prediction", "Self-supervised Video Retrieval", "Open Set Action Recognition", "Self-Supervised Action Recognition Linear"], "id": "b270072f-02e4-487e-8775-c381173cea74", "image_url": "https://production-media.paperswithcode.com/datasets/UCF101-0000000066-730e25b3_Nq9c4VT.jpg"}, {"title": "UCY", "short_description": "The **UCY** dataset consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (\u0394t=0.4s). It is composed of three sequences (Zara01, Zara02, and UCY), taken in public spaces from top-view.\r\n\r\nSource: [Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data](https://arxiv.org/abs/2001.03093)\r\nImage Source: [http://trajnet.stanford.edu/data.php?n=1](http://trajnet.stanford.edu/data.php?n=1)", "price": 32, "created_at": "2025-01-09 13:44:40.906868", "keyword": ["Trajectory Prediction", "Trajectory Forecasting", "motion prediction"], "id": "51132e0c-939d-4586-98cd-65b2b2a18063", "image_url": "https://production-media.paperswithcode.com/datasets/UCY-0000003254-7421e50a.jpg"}, {"title": "USF", "short_description": "The **USF** **Human ID Gait Challenge Dataset** is a dataset of videos for gait recognition. It has videos from 122 subjects in up to 32 possible combinations of variations in factors.\n\nSource: [http://www.eng.usf.edu/cvprg/Gait_Data.html](http://www.eng.usf.edu/cvprg/Gait_Data.html)", "price": 227, "created_at": "2025-01-09 13:44:40.906933", "keyword": ["Age Estimation", "Gait Recognition"], "id": "327a5651-8c17-4a54-a8b2-2c966f334705", "image_url": ""}, {"title": "USPS", "short_description": "**USPS** is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16\u00d716 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles.\r\n\r\nSource: [Hallucinating Agnostic Images to Generalize Across Domains](https://arxiv.org/abs/1808.01102)\r\nImage Source: [https://ieeexplore.ieee.org/document/291440](https://ieeexplore.ieee.org/document/291440)", "price": 288, "created_at": "2025-01-09 13:44:40.906997", "keyword": ["Domain Adaptation", "Image Clustering", "Deep Clustering", "Domain Adaptation", "Image Clustering", "Deep Clustering"], "id": "b9159bf0-8cf5-40d9-a965-a716fdbc4406", "image_url": "https://production-media.paperswithcode.com/datasets/USPS-0000001055-6cd416b0_D96Rryg.jpg"}, {"title": "UTKFace", "short_description": "The **UTKFace** dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.\r\n\r\nSource: [https://susanqq.github.io/UTKFace/](https://susanqq.github.io/UTKFace/)\r\nImage Source: [https://susanqq.github.io/UTKFace/](https://susanqq.github.io/UTKFace/)", "price": 424, "created_at": "2025-01-09 13:44:40.907061", "keyword": ["Age Estimation", "Facial Attribute Classification", "Fairness", "Age/Bias-conflicting", "Race/Unbiased", "Race/Bias-conflicting", "Age/Unbiased", "Age Estimation", "Facial Attribute Classification", "Fairness", "Age/Bias-conflicting", "Race/Unbiased", "Race/Bias-conflicting", "Age/Unbiased"], "id": "7e9756cd-a9e4-4ab6-9722-fe10eb7385c9", "image_url": "https://production-media.paperswithcode.com/datasets/UTKFace-0000000710-4976bb64_rQrd7aQ.jpg"}, {"title": "United Nations Parallel Corpus", "short_description": "The first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.\r\n\r\nSource: [The United Nations Parallel Corpus v1.0](https://www.aclweb.org/anthology/L16-1561)", "price": 65, "created_at": "2025-01-09 13:44:40.907125", "keyword": ["Word Embeddings"], "id": "af5646fe-7616-4a65-8787-0ea858ac66c9", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_5.27.49_PM.png"}, {"title": "Universal Dependencies", "short_description": "The **Universal Dependencies** (UD) project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for multiple languages. The first version of the dataset was released in 2015 and consisted of 10 treebanks over 10 languages. Version 2.7 released in 2020 consists of 183 treebanks over 104 languages. The annotation consists of UPOS (universal part-of-speech tags), XPOS (language-specific part-of-speech tags), Feats (universal morphological features), Lemmas, dependency heads and universal dependency labels.\r\n\r\nSource: [Evaluating Contextualized Embeddings on 54 Languagesin POS Tagging, Lemmatization and Dependency Parsing](https://arxiv.org/abs/1908.07448)\r\nImage Source: [https://universaldependencies.org/introduction.html](https://universaldependencies.org/introduction.html)", "price": 208, "created_at": "2025-01-09 13:44:40.907189", "keyword": ["Token Classification", "Dependency Parsing", "Part-Of-Speech Tagging", "Language Identification", "POS", "Cross-lingual zero-shot dependency parsing", "Cross-Lingual POS Tagging", "TAG", "SENTS", "MORPH", "UNLABELED_DEPENDENCIES", "LABELED_DEPENDENCIES", "LEMMA", "Token Classification", "Dependency Parsing", "Part-Of-Speech Tagging", "Language Identification", "POS", "Cross-lingual zero-shot dependency parsing", "Cross-Lingual POS Tagging", "TAG", "SENTS", "MORPH", "UNLABELED_DEPENDENCIES", "LABELED_DEPENDENCIES", "LEMMA"], "id": "3890a454-2e4d-4c63-be65-726c52f03657", "image_url": "https://production-media.paperswithcode.com/datasets/Universal_Dependencies-0000002712-74dffc3c_jUiQ5Qi.jpg"}, {"title": "Urban100", "short_description": "The **Urban100** dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models.\r\nImage Source: [http://vllab.ucmerced.edu/wlai24/LapSRN/](http://vllab.ucmerced.edu/wlai24/LapSRN/)", "price": 870, "created_at": "2025-01-09 13:44:40.907253", "keyword": ["Image Super-Resolution", "Color Image Denoising", "Grayscale Image Denoising", "Image Denoising", "Blind Super-Resolution", "Joint Demosaicing and Denoising", "Compressive Sensing", "Image Super-Resolution", "Color Image Denoising", "Grayscale Image Denoising", "Image Denoising", "Blind Super-Resolution", "Joint Demosaicing and Denoising", "Compressive Sensing"], "id": "84d64c72-35c3-47fe-8e48-b6980902ec99", "image_url": "https://production-media.paperswithcode.com/datasets/Urban100-0000003447-44a59270.jpg"}, {"title": "VATEX", "short_description": "**VATEX** is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\r\n\r\nSource: [https://arxiv.org/pdf/1904.03493.pdf](https://arxiv.org/pdf/1904.03493.pdf)\r\nImage Source: [https://arxiv.org/pdf/1904.03493.pdf](https://arxiv.org/pdf/1904.03493.pdf)", "price": 942, "created_at": "2025-01-09 13:44:40.907320", "keyword": ["Video Retrieval", "Video Captioning", "Zero-Shot Video Retrieval"], "id": "4abc63ce-7298-41de-af6b-5271191b1508", "image_url": "https://production-media.paperswithcode.com/datasets/VATEX-0000001598-6d509c46_RQv8JBa.jpg"}, {"title": "VCR", "short_description": "**Visual Commonsense Reasoning** (**VCR**) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes.\r\n\r\nSource: [Visual Commonsense R-CNN](https://arxiv.org/abs/2002.12204)\r\nImage Source: [From Recognition to Cognition: Visual Commonsense Reasoning](https://paperswithcode.com/paper/from-recognition-to-cognition-visual/)", "price": 786, "created_at": "2025-01-09 13:44:40.907384", "keyword": ["Visual Question Answering (VQA)", "Visual Commonsense Reasoning", "Explanation Generation"], "id": "05d430d3-d76d-445a-8ab8-777d54213a61", "image_url": "https://production-media.paperswithcode.com/datasets/VCR-0000000216-581da9d9_ccHlrxk.jpg"}, {"title": "VCTK", "short_description": "This CSTR **VCTK** Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.\r\n\r\nSource: [CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)](https://datashare.is.ed.ac.uk/handle/10283/3443)", "price": 212, "created_at": "2025-01-09 13:44:40.907447", "keyword": ["Audio Super-Resolution", "Directional Hearing", "Bandwidth Extension", "Real-time Directional Hearing"], "id": "b81f5e22-0a4c-4db9-9907-8dc8730ec4a1", "image_url": ""}, {"title": "VGGFace2", "short_description": "The **VGGFace2** dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity. The dataset is divided into two splits, one for the training and one for test. The latter contains around 170000 images divided into 500 identities while all the other images belong to the remaining 8631 classes available for training. While constructing the datasets, the authors focused their efforts on reaching a very low label noise and a high pose and age diversity thus, making the VGGFace2 dataset a suitable choice to train state-of-the-art deep learning models on face-related tasks. The images of the training set have an average resolution of 137x180 pixels, with less than 1% at a resolution below 32 pixels (considering the shortest side).\r\n\r\n**CAUTION**: Authors note that the distribution of identities in the VGG-Face dataset may not be representative of the global human population. Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.\r\n\r\nSource: [Cross-Resolution Learning for Face Recognition](https://arxiv.org/abs/1912.02851)", "price": 476, "created_at": "2025-01-09 13:44:40.907511", "keyword": ["Image Super-Resolution", "Facial Inpainting"], "id": "0702a544-bcf6-404b-99bc-06c1c1a94066", "image_url": ""}, {"title": "VIPeR", "short_description": "The **Viewpoint Invariant Pedestrian Recognition** (**VIPeR**) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128\u00d748 pixels. It provides the pose angle of each person as 0\u00b0 (front), 45\u00b0, 90\u00b0 (right), 135\u00b0, and 180\u00b0 (back).\r\n\r\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)\r\n\r\nImage Source: [Qin et al](https://www.researchgate.net/figure/Some-examples-from-the-VIPeR-dataset-Each-column-is-one-of-632-same-person-example_fig8_331482460)", "price": 985, "created_at": "2025-01-09 13:44:40.907574", "keyword": ["Person Re-Identification", "Metric Learning", "Patch Matching", "Person Re-Identification", "Metric Learning", "Patch Matching"], "id": "291bc812-4ba0-49a3-b4d2-00043fd55037", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_1.00.36_PM.png"}, {"title": "VIST", "short_description": "The **Visual Storytelling** Dataset (**VIST**) consists of 210,819 unique photos and 50,000 stories. The images were collected from albums on Flickr. The albums included 10 to 50 images and all the images in an album are taken in a 48-hour span. The stories were created by workers on Amazon Mechanical Turk, where the workers were instructed to choose five images from the album and write a story about them. Every story has five sentences, and every sentence is paired with its appropriate image. The dataset is split into 3 subsets, a training set (80%), a validation set (10%) and a test set (10%). All the words and interpunction signs in the stories are separated by a space character and all the location names are replaced with the word location. All the names of people are replaced with the words male or female depending on the gender of the person.\r\n\r\nSource: [Stories for Images-in-Sequence by using Visual and Narrative Components This research was partially funded by Pendulibrium and the Faculty of computer science and engineering, Ss. Cyril and Methodius University in Skopje.](https://arxiv.org/abs/1805.05622)\r\nImage Source: [https://arxiv.org/pdf/1604.03968.pdf](https://arxiv.org/pdf/1604.03968.pdf)", "price": 224, "created_at": "2025-01-09 13:44:40.907637", "keyword": ["Visual Storytelling", "Story Continuation"], "id": "716bec0f-da1d-47f3-a19e-94e47998748a", "image_url": "https://production-media.paperswithcode.com/datasets/VIST-0000002374-a16bee4a_dWsHEkF.jpg"}, {"title": "VOT2014", "short_description": "The dataset comprises 25 short sequences showing various objects in challenging backgrounds. Eight sequences are from the VOT2013 challenge (bolt, bicycle, david, diving, gymnastics, hand, sunshade, woman). The new sequences show complementary objects and backgrounds, for example a fish underwater or a surfer riding a big wave. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 25 sequences sample evenly well the existing pool.\r\n\r\nSource: [VOT2014](https://www.votchallenge.net/vot2014/dataset.html)", "price": 825, "created_at": "2025-01-09 13:44:40.907701", "keyword": ["Object Tracking", "Visual Object Tracking", "Visual Tracking", "Video Object Tracking"], "id": "a72f843f-eb14-4454-a3e5-52a2891eb090", "image_url": "https://production-media.paperswithcode.com/datasets/jogging.gif"}, {"title": "VOT2016", "short_description": "**VOT2016** is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects.\r\n\r\nSource: [Video Saliency Detection by 3D Convolutional Neural Networks](https://arxiv.org/abs/1807.04514)\r\nImage Source: [https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png](https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png)", "price": 356, "created_at": "2025-01-09 13:44:40.907765", "keyword": ["Visual Object Tracking", "Visual Object Tracking"], "id": "b977696d-cb50-4582-8eee-ee211bd38ac8", "image_url": "https://production-media.paperswithcode.com/datasets/VOT2016-0000000928-3ac5717f_pz6XGEN.jpg"}, {"title": "VOT2017", "short_description": "**VOT2017** is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes.\r\n\r\nSource: [GradNet: Gradient-Guided Network for Visual Object Tracking](https://arxiv.org/abs/1909.06800)\r\nImage Source: [https://ieeexplore.ieee.org/document/8265440](https://ieeexplore.ieee.org/document/8265440)", "price": 795, "created_at": "2025-01-09 13:44:40.907829", "keyword": ["Object Tracking", "Visual Object Tracking"], "id": "d4c4db45-e52d-4d92-861f-d519716414b0", "image_url": "https://production-media.paperswithcode.com/datasets/VOT2017-0000000314-fc9cd5ac_LewxZD2.jpg"}, {"title": "VOT2018", "short_description": "**VOT2018** is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets.\r\n\r\nSource: [Remove Cosine Window from Correlation Filter-based Visual Trackers: When and How](https://arxiv.org/abs/1905.06648)\r\nImage Source: [https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770](https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770)", "price": 728, "created_at": "2025-01-09 13:44:40.907933", "keyword": ["Object Tracking", "Visual Object Tracking", "Object Tracking", "Visual Object Tracking"], "id": "128caa61-181c-4761-8901-5e794f67c434", "image_url": "https://production-media.paperswithcode.com/datasets/VOT2018-0000000312-ab97f9c7_wyTsxDv.jpg"}, {"title": "VQG", "short_description": "**VQG** is a collection of datasets for visual question generation. VQG questions were collected by crowdsourcing the task on Amazon Mechanical Turk (AMT). The authors provided details on the prompt and the specific instructions for all the crowdsourcing tasks in this paper in the supplementary material. The prompt was successful at capturing nonliteral questions. Images were taken from the MSCOCO dataset.\r\n\r\nSource: [What BERT Sees: Cross-Modal Transfer for Visual Question Generation](https://arxiv.org/pdf/2002.10832v3.pdf)", "price": 325, "created_at": "2025-01-09 13:44:40.908062", "keyword": ["Text Generation", "Visual Question Answering (VQA)", "Question Generation"], "id": "d50f47d0-b56c-4b9f-99c3-160672218df2", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-02-01_at_7.03.52_PM.png"}, {"title": "VRD", "short_description": "The Visual Relationship Dataset (**VRD**) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario.\r\n\r\nSource: [Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation](https://arxiv.org/abs/1910.00462)\r\nImage Source: [https://cs.stanford.edu/people/ranjaykrishna/vrd/](https://cs.stanford.edu/people/ranjaykrishna/vrd/)", "price": 222, "created_at": "2025-01-09 13:44:40.908159", "keyword": ["Visual Relationship Detection", "Scene Graph Generation", "Scene Graph Detection", "Relationship Detection"], "id": "2d83f385-c785-466f-becc-0102bed59396", "image_url": "https://production-media.paperswithcode.com/datasets/VRD-0000000900-1b07415d_bUpE2hs.jpg"}, {"title": "VeRi-776", "short_description": "**VeRi-776** is a vehicle re-identification dataset which contains 49,357 images of 776 vehicles from 20 cameras. The dataset is collected in the real traffic scenario, which is close to the setting of CityFlow. The dataset contains bounding boxes, types, colors and brands.\r\n\r\nSource: [VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification](https://arxiv.org/abs/2004.06305)\r\nImage Source: [https://vehiclereid.github.io/VeRi/](https://vehiclereid.github.io/VeRi/)", "price": 484, "created_at": "2025-01-09 13:44:40.908223", "keyword": ["Vehicle Re-Identification", "Unsupervised Vehicle Re-Identification"], "id": "878dddd1-82cd-4093-a6fe-7eb41f730e2d", "image_url": "https://production-media.paperswithcode.com/datasets/VeRi-776-0000002795-1449014b_LdLJvpm.jpg"}, {"title": "VegFru", "short_description": "**VegFru** is a domain-specific dataset for fine-grained visual categorization. VegFru categorizes vegetables and fruits according to their eating characteristics, and each image contains at least one edible part of vegetables or fruits with the same cooking usage. Particularly, all the images are labelled hierarchically. The current version covers vegetables and fruits of 25 upper-level categories and 292 subordinate classes. And it contains more than 160,000 images in total and at least 200 images for each subordinate class.\n\nSource: [https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf)\nImage Source: [https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf)", "price": 302, "created_at": "2025-01-09 13:44:40.908293", "keyword": ["Image Retrieval", "Color Constancy", "Fine-Grained Visual Categorization"], "id": "efb61b21-8774-48b5-b46f-2ee5856dcae8", "image_url": "https://production-media.paperswithcode.com/datasets/VegFru-0000005637-559f18f5.jpg"}, {"title": "VehicleID", "short_description": "The \u201c**VehicleID**\u201d dataset contains CARS captured during the daytime by multiple real-world surveillance cameras distributed in a small city in China. There are 26,267 vehicles (221,763 images in total) in the entire dataset. Each image is attached with an id label corresponding to its identity in real world. In addition, the dataset contains manually labelled 10319 vehicles (90196 images in total) of their vehicle model information(i.e.\u201cMINI-cooper\u201d, \u201cAudi A6L\u201d and \u201cBWM 1 Series\u201d).\r\n\r\nSource: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)\r\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)", "price": 466, "created_at": "2025-01-09 13:44:40.908358", "keyword": ["Vehicle Re-Identification"], "id": "33156372-21a4-40d2-80f0-37080c6f2e4d", "image_url": "https://production-media.paperswithcode.com/datasets/VehicleID-0000000104-c2853a2c_48OajUG.jpg"}, {"title": "Video2GIF", "short_description": "The **Video2GIF** dataset contains over 100,000 pairs of GIFs and their source videos. The GIFs were collected from two popular GIF websites (makeagif.com, gifsoup.com) and the corresponding source videos were collected from YouTube in Summer 2015. IDs and URLs of the GIFs and the videos are provided, along with temporal alignment of GIF segments to their source videos. The dataset shall be used to evaluate GIF creation and video highlight techniques.\r\n\r\nIn addition to the 100K GIF-video pairs, the dataset contains 357 pairs of GIFs and their source videos as the test set. The 357 videos come with a Creative Commons CC-BY license, which allowed the authors to redistribute the material with appropriate credit to make the results on test set reproducible even when some of the videos become unavailable.\r\n\r\nSource: [Video2GIF](https://github.com/gifs/personalized-highlights-dataset)", "price": 985, "created_at": "2025-01-09 13:44:40.908423", "keyword": ["Emotion Recognition", "Video Summarization", "Sentence Embedding"], "id": "2c52bb40-874a-4df1-a84b-945ecda2e386", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-26_at_14.32.58.png"}, {"title": "VisDA-2017", "short_description": "**VisDA-2017** is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO..\r\n\r\nSource: [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183)\r\nImage Source: [http://ai.bu.edu/visda-2017/](http://ai.bu.edu/visda-2017/)", "price": 758, "created_at": "2025-01-09 13:44:40.908487", "keyword": ["Semantic Segmentation", "Domain Adaptation", "Unsupervised Domain Adaptation", "Partial Domain Adaptation", "Universal Domain Adaptation", "Source-Free Domain Adaptation", "Semi-supervised Domain Adaptation"], "id": "658ea19a-37d9-4a30-8f8f-ef03a30c63ad", "image_url": "https://production-media.paperswithcode.com/datasets/vda17.png"}, {"title": "VisDial", "short_description": "**Visual Dialog** (**VisDial**) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a \u2018questioner\u2019 and the other person acted as an \u2018answerer\u2019. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to \u201cimagine the scene better\u201d. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max.\r\n\r\n**VisDial v1.0** contains 123K dialogues on MS COCO (2017 training set) for training split, 2K dialogues with validation images for validation split and 8K dialogues on test set for test-standard set. The previously released v0.5 and v0.9 versions of VisDial dataset (corresponding to older splits of MS COCO) are considered deprecated.\r\n\r\nSource: [Granular Multimodal Attention Networks for Visual Dialog](https://arxiv.org/abs/1910.05728)\r\nImage Source: [https://arxiv.org/pdf/1611.08669.pdf](https://arxiv.org/pdf/1611.08669.pdf)", "price": 588, "created_at": "2025-01-09 13:44:40.908565", "keyword": ["Question Answering", "Visual Question Answering (VQA)", "Common Sense Reasoning", "Visual Dialog", "Chat-based Image Retrieval"], "id": "a608d830-1ce7-43f7-8652-287d4212ba01", "image_url": "https://production-media.paperswithcode.com/datasets/VisDial-0000003368-deeb9b6d.jpg"}, {"title": "Visual Genome", "short_description": "**Visual Genome** contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.\r\n\r\nSource: [RaAM: A Relation-aware Attention Model for Visual Question Answering](https://arxiv.org/abs/1903.12314)\r\nImage Source: [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision/)", "price": 878, "created_at": "2025-01-09 13:44:40.908630", "keyword": ["Object Detection", "Visual Question Answering (VQA)", "Layout-to-Image Generation", "Visual Relationship Detection", "Scene Graph Generation", "Phrase Grounding", "Unsupervised KG-to-Text Generation", "Scene Graph Detection", "Predicate Classification", "Image Generation from Scene Graphs", "Multi-label Image Recognition with Partial Labels", "Scene Graph Classification", "Unsupervised semantic parsing", "Bidirectional Relationship Classification", "Unbiased Scene Graph Generation", "Dense Captioning"], "id": "5770dc3a-0ebd-4206-a161-364559d9fbe3", "image_url": "https://production-media.paperswithcode.com/datasets/Visual_Genome-0000000087-aaf04589_6fjxO1x.jpg"}, {"title": "Visual Question Answering", "short_description": "**Visual Question Answering (VQA)** is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. The first version of the dataset was released in October 2015. [VQA v2.0](/dataset/visual-question-answering-v2-0) was released in April 2017.", "price": 939, "created_at": "2025-01-09 13:44:40.908695", "keyword": [], "id": "571d05df-c671-497f-8433-8a3c0d60cfad", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_1.54.19_PM.png"}, {"title": "Visual Question Answering v2.0", "short_description": "Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the [VQA](https://www.paperswithcode.com/dataset/vqa) dataset.\r\n\r\n- 265,016 images (COCO and abstract scenes)\r\n- At least 3 questions (5.4 questions on average) per image\r\n- 10 ground truth answers per question\r\n- 3 plausible (but likely incorrect) answers per question\r\n- Automatic evaluation metric\r\n\r\nThe [first version of the dataset](/dataset/visual-question-answering) was released in October 2015.", "price": 817, "created_at": "2025-01-09 13:44:40.908759", "keyword": ["Visual Question Answering (VQA)", "Visual Question Answering"], "id": "3b3bdb78-048d-43f4-a9ed-c8485913b1e2", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_1.57.28_PM.png"}, {"title": "Visual7W", "short_description": "**Visual7W** is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.\r\n\r\nSource: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)\r\nImage Source: [http://ai.stanford.edu/~yukez/visual7w/](http://ai.stanford.edu/~yukez/visual7w/)", "price": 310, "created_at": "2025-01-09 13:44:40.908823", "keyword": ["Visual Question Answering (VQA)", "Image Comprehension"], "id": "5a9901ec-d34d-4116-948e-4d53036ef9e0", "image_url": "https://production-media.paperswithcode.com/datasets/Visual7W-0000000194-7101edc9_HNGRebb.jpg"}, {"title": "VizDoom", "short_description": "ViZDoom is an AI research platform based on the classical First Person Shooter game Doom. The most popular game mode is probably the so-called Death Match, where several players join in a maze and fight against each other. After a fixed time, the match ends and all the players are ranked by the FRAG scores defined as kills minus suicides. During the game, each player can access various observations, including the first-person view screen pixels, the corresponding depth-map and segmentation-map (pixel-wise object labels), the bird-view maze map, etc. The valid actions include almost all the keyboard-stroke and mouse-control a human player can take, accounting for moving, turning, jumping, shooting, changing weapon, etc. ViZDoom can run a game either synchronously or asynchronously, indicating whether the game core waits until all players\u2019 actions are collected or runs in a constant frame rate without waiting.\r\n\r\nSource: [Arena: a toolkit for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1907.09467)\r\nImage Source: [https://github.com/mwydmuch/ViZDoom](https://github.com/mwydmuch/ViZDoom)", "price": 964, "created_at": "2025-01-09 13:44:40.908887", "keyword": ["Image Generation", "Scene Generation", "Decision Making", "Game of Doom", "Q-Learning", "Starcraft"], "id": "e30e47aa-3ece-4d0c-a25c-9d9d55c9fc00", "image_url": "https://production-media.paperswithcode.com/datasets/VizDoom-0000003537-5892c5cb.gif"}, {"title": "VizWiz", "short_description": "The **VizWiz**-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered.\r\n\r\nSource: [https://vizwiz.org/tasks-and-datasets/vqa/](https://vizwiz.org/tasks-and-datasets/vqa/)\r\nImage Source: [https://vizwiz.org/tasks-and-datasets/vqa/](https://vizwiz.org/tasks-and-datasets/vqa/)", "price": 824, "created_at": "2025-01-09 13:44:40.908950", "keyword": ["Visual Question Answering (VQA)", "Image Captioning", "Visual Question Answering"], "id": "5c8b9c14-5288-4e77-8ee0-5752cc564641", "image_url": "https://production-media.paperswithcode.com/datasets/VizWiz-0000000943-21defb78_V6w845I.jpg"}, {"title": "Volleyball", "short_description": "**Volleyball** is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\r\n\r\nSource: [https://github.com/mostafa-saad/deep-activity-rec#dataset](https://github.com/mostafa-saad/deep-activity-rec#dataset)\r\nImage Source: [https://github.com/mostafa-saad/deep-activity-rec#dataset](https://github.com/mostafa-saad/deep-activity-rec#dataset)", "price": 376, "created_at": "2025-01-09 13:44:40.909015", "keyword": ["Action Recognition", "Sports Ball Detection and Tracking", "Group Activity Recognition"], "id": "2d8a4bba-e9b8-4ee6-b23f-9ee6ce129db0", "image_url": "https://production-media.paperswithcode.com/datasets/Volleyball-0000000370-8dfe92a4_JEuzQ8Z.jpg"}, {"title": "VoxCeleb2", "short_description": "**VoxCeleb2** is a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected \u2018in the wild\u2019, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example \u2013 visual speech synthesis, speech separation, cross-modal transfer from face to voice or vice versa and training face recognition from video to complement existing face recognition datasets.\r\n\r\nSource: [VoxCeleb2: Deep Speaker Recognition](https://arxiv.org/pdf/1806.05622v2.pdf)\r\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/voxceleb/](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)", "price": 255, "created_at": "2025-01-09 13:44:40.909078", "keyword": ["Speech Separation", "Talking Head Generation", "Speaker Verification"], "id": "728cdfe1-54ef-447b-a735-505d6c3b1157", "image_url": "https://production-media.paperswithcode.com/datasets/VoxCeleb2-0000003283-33a0f74d.jpg"}, {"title": "WFLW", "short_description": "The **Wider Facial Landmarks in the Wild** or **WFLW** database contains 10000 faces (7500 for training and 2500 for testing) with 98 annotated landmarks. This database also features rich attribute annotations in terms of occlusion, head pose, make-up, illumination, blur and expressions.\r\n\r\nSource: [Deep Entwined Learning Head Pose and Face Alignment Inside an Attentional Cascade with Doubly-Conditional fusion](https://arxiv.org/abs/2004.06558)\r\nImage Source: [https://wywu.github.io/projects/LAB/WFLW.html](https://wywu.github.io/projects/LAB/WFLW.html)", "price": 220, "created_at": "2025-01-09 13:44:40.909146", "keyword": ["Face Alignment", "Head Pose Estimation", "Face Alignment", "Head Pose Estimation"], "id": "fea467c3-e725-4bb7-8f7f-2ff3ac65815e", "image_url": "https://production-media.paperswithcode.com/datasets/WFLW-0000001245-b6e3108a_jqVs40I.jpg"}, {"title": "WMCA", "short_description": "The Wide Multi Channel Presentation Attack (WMCA) database consists of 1941 short video recordings of both bonafide and presentation attacks from 72 different identities. The data is recorded from several channels including color, depth, infra-red, and thermal.\r\n\r\nAdditionally, the pulse reading data for bonafide recordings is also provided.\r\n\r\nPreprocessed images for some of the channels are also provided for part of the data used in the reference publication.\r\n\r\nThe WMCA database is produced at Idiap within the framework of \u201cIARPA BATL\u201d and \u201cH2020 TESLA\u201d projects and it is intended for investigation of presentation attack detection (PAD) methods for face recognition systems.", "price": 317, "created_at": "2025-01-09 13:44:40.909209", "keyword": ["Face Presentation Attack Detection"], "id": "102f0e65-266e-4771-931f-f78ccfd3bfa5", "image_url": "https://production-media.paperswithcode.com/datasets/wmca.jpeg"}, {"title": "WMT 2014", "short_description": "**WMT 2014** is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:\r\n\r\n* a news translation task,\r\n* a quality estimation task,\r\n* a metrics task,\r\n* a medical text translation task.\r\n\r\nSource: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)", "price": 579, "created_at": "2025-01-09 13:44:40.909277", "keyword": ["Machine Translation", "Sequence-to-sequence Language Modeling", "Translation deu-eng", "Translation eng-deu", "Unsupervised Machine Translation"], "id": "e1b47ad6-9726-47da-b807-47f01b7c1573", "image_url": "https://production-media.paperswithcode.com/datasets/WMT_2014-0000003376-2729c0b9.jpg"}, {"title": "WMT 2015", "short_description": "**WMT 2015** is a collection of datasets used in shared tasks of the Tenth Workshop on Statistical Machine Translation. The workshop featured five tasks:\r\n\r\n* a news translation task,\r\n* a metrics task,\r\n* a tuning task,\r\n* a quality estimation task,\r\n* an automatic post-editing task.\r\n\r\nSource: [https://www.aclweb.org/anthology/W15-3001.pdf](https://www.aclweb.org/anthology/W15-3001.pdf)", "price": 987, "created_at": "2025-01-09 13:44:40.909342", "keyword": ["Machine Translation", "Translation deu-eng", "Translation eng-deu"], "id": "6779e889-eb73-44f1-b746-f77efb07881b", "image_url": "https://production-media.paperswithcode.com/datasets/WMT_2015-0000003379-d9314e94.jpg"}, {"title": "WMT 2015 News", "short_description": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, French, Russian) and additional 1500 sentences from each of the 5 languages translated to English. The sentences are taken from newspaper articles for each language pair, except for French, where the test set was drawn from user-generated comments on the news articles (from Guardian and Le Monde). The translation was done by professional translators.\n\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\nSome training corpora were identical from WMT 2014 (Europarl, United Nations, French-English 10\u2079 corpus, CzEng, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (News Commentary, monolingual news data). Additionally, the Finnish Europarl and Finnish-English Wikipedia Headline corpus were added.\n\nSource: [https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/](https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/)\nImage Source: [httpshttps://www.aclweb.org/anthology/W15-3001.pdf](httpshttps://www.aclweb.org/anthology/W15-3001.pdf)", "price": 336, "created_at": "2025-01-09 13:44:40.909405", "keyword": ["Machine Translation", "Automatic Post-Editing"], "id": "c1884e3b-bd28-451c-bbed-1465d56baf3d", "image_url": "https://production-media.paperswithcode.com/datasets/WMT_2015_News-0000003380-d55844de.jpg"}, {"title": "WMT 2016", "short_description": "**WMT 2016** is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\r\n\r\nThe conference featured ten shared tasks:\r\n\r\n- a news translation task,\r\n- an IT domain translation task,\r\n- a biomedical translation task,\r\n- an automatic post-editing task,\r\n- a metrics task (assess MT quality given reference translation).\r\n- a quality estimation task (assess MT quality without access to any reference),\r\n- a tuning task (optimize a given MT system),\r\n- a pronoun translation task,\r\n- a bilingual document alignment task,\r\n- a multimodal translation task.\r\n\r\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)", "price": 764, "created_at": "2025-01-09 13:44:40.909469", "keyword": ["Machine Translation", "Sequence-to-sequence Language Modeling", "Translation deu-eng", "Translation eng-deu", "Unsupervised Machine Translation", "Translation"], "id": "19f94743-f0f5-43dc-9be1-a30090bfae9d", "image_url": "https://production-media.paperswithcode.com/datasets/WMT_2016-0000003370-a7220982.jpg"}, {"title": "WMT 2016 News", "short_description": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators.\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\nSome training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10\u2079 corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl.\n\nSource: [https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/](https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/)\nImage Source: [https://www.aclweb.org/anthology/W16-2301.pdf](https://www.aclweb.org/anthology/W16-2301.pdf)", "price": 548, "created_at": "2025-01-09 13:44:40.909533", "keyword": ["Machine Translation", "Unsupervised Machine Translation", "Word Alignment", "Automatic Post-Editing"], "id": "2dff9ff2-1164-499d-9603-0c243c82b9c8", "image_url": "https://production-media.paperswithcode.com/datasets/WMT_2016_News-0000003371-565666c5.jpg"}, {"title": "WN18", "short_description": "The **WN18** dataset has 18 relations scraped from WordNet for roughly 41,000 synsets, resulting in 141,442 triplets. It was found out that a large number of the test triplets can be found in the training set with another relation or the inverse relation. Therefore, a new version of the dataset WN18RR has been proposed to address this issue.\r\n\r\nSource: [http://nlpprogress.com/english/relation_prediction.html](http://nlpprogress.com/english/relation_prediction.html)", "price": 796, "created_at": "2025-01-09 13:44:40.909597", "keyword": ["Link Prediction", "Knowledge Graph Completion", "Ancestor-descendant prediction", "Link Prediction", "Knowledge Graph Completion", "Ancestor-descendant prediction"], "id": "7cf7eab8-89a1-4b50-bc34-5d1bb5db8ef7", "image_url": ""}, {"title": "WN18RR", "short_description": "**WN18RR** is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.\r\n\r\nSource: [End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion](https://arxiv.org/abs/1811.04441)", "price": 836, "created_at": "2025-01-09 13:44:40.909661", "keyword": ["Link Prediction", "Knowledge Graph Completion", "Ancestor-descendant prediction"], "id": "561b225d-0e4f-4b65-b5e6-c6809b49c240", "image_url": ""}, {"title": "Washington RGB-D", "short_description": "**Washington RGB-D** is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating.\r\n\r\nSource: [Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work](https://arxiv.org/abs/1702.08513)\r\nImage Source: [https://rgbd-dataset.cs.washington.edu/](https://rgbd-dataset.cs.washington.edu/)", "price": 897, "created_at": "2025-01-09 13:44:40.909725", "keyword": ["Object Detection", "Object Recognition", "3D Object Recognition"], "id": "83f31a1d-edb3-4399-a3b7-23d88c53b918", "image_url": "https://production-media.paperswithcode.com/datasets/Washington_RGB-D-0000003520-28d13843.jpg"}, {"title": "Washington RGB-D Scenes", "short_description": "The RGB-D Scenes Dataset contains 8 scenes annotated with objects that belong to the Washington RGB-D Object Dataset. Each scene is a single video sequence consisting of multiple RGB-D frames.\n\nSource: [https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/](https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/)\nImage Source: [https://arxiv.org/abs/1904.02530](https://arxiv.org/abs/1904.02530)", "price": 705, "created_at": "2025-01-09 13:44:40.909791", "keyword": [], "id": "29ec3c2e-9797-4e6e-b8dc-a89ee01cddf6", "image_url": "https://production-media.paperswithcode.com/datasets/Washington_RGB-D_Scenes-0000003530-200c2052.jpg"}, {"title": "WebNLG", "short_description": "The **WebNLG** corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.\r\n\r\nInitially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.\r\nThe corpus is also used for a reverse task of triplets extraction.\r\n\r\nVersioning history of the dataset can be found [here](https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/).\r\n\r\nSource: [Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation](https://arxiv.org/abs/1904.03396)\r\nImage Source: [https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/](https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/)\r\n\r\nIt's also available here: https://huggingface.co/datasets/web_nlg\r\nNote: \"The v3 release (release_v3.0_en, release_v3.0_ru) for the WebNLG2020 challenge also supports a semantic parsing task.\"", "price": 583, "created_at": "2025-01-09 13:44:40.909856", "keyword": ["Relation Extraction", "Data-to-Text Generation", "Joint Entity and Relation Extraction", "KG-to-Text Generation", "Table-to-Text Generation", "Unsupervised KG-to-Text Generation", "Graph-to-Sequence", "Unsupervised semantic parsing"], "id": "f8bf0f8f-086e-4408-9728-7407e18bb5e2", "image_url": "https://production-media.paperswithcode.com/datasets/WebNLG-0000000520-8ff404a9_x3gGdAj.jpg"}, {"title": "WebQuestions", "short_description": "The **WebQuestions** dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities.\r\n\r\nExample questions (answers) in the dataset include \u201cWhere did Edgar Allan Poe died?\u201d (baltimore) or \u201cWhat degrees did Barack Obama get?\u201d (bachelor_of_arts, juris_doctor).\r\n\r\nSource: [Question Answering with Subgraph Embeddings](https://arxiv.org/abs/1406.3676)\r\nImage Source: [Berant et al](https://www.aclweb.org/anthology/D13-1160)", "price": 804, "created_at": "2025-01-09 13:44:40.909920", "keyword": ["Question Answering", "Open-Domain Question Answering", "KG-to-Text Generation", "Question Answering", "Open-Domain Question Answering", "KG-to-Text Generation"], "id": "3c1c56b9-f64b-45b4-bf3f-cf64a590e4b2", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-29_at_2.01.27_PM.png"}, {"title": "WebText", "short_description": "**WebText** is an internal OpenAI corpus created by scraping web pages with emphasis on\r\ndocument quality. The authors scraped all outbound links from\r\nReddit which received at least 3\r\nkarma. The authors used the approach as a heuristic indicator for\r\nwhether other users found the link interesting, educational,\r\nor just funny.\r\n\r\nWebText contains the text subset of these 45 million links. It consists of over 8 million documents\r\nfor a total of 40 GB of text. All Wikipedia\r\ndocuments were removed from WebText since it is a common data source\r\nfor other datasets.", "price": 484, "created_at": "2025-01-09 13:44:40.909987", "keyword": [], "id": "ca09ee0f-0b3f-41ff-9334-c046a25ac3c4", "image_url": "https://production-media.paperswithcode.com/datasets/webtext.png"}, {"title": "WebVision", "short_description": "The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. \r\n\r\nThe same 1,000 concepts as the ILSVRC 2012 dataset are used for querying images, such that a bunch of existing approaches can be directly investigated and compared to the models trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains 50,000 images (50 images per category) is provided to facilitate the algorithmic development.", "price": 715, "created_at": "2025-01-09 13:44:40.910051", "keyword": ["Image Classification", "Learning with noisy labels"], "id": "4775e5f6-2383-4ffc-9d0c-44a7e52c2c7a", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_15.54.02.png"}, {"title": "Who-did-What", "short_description": "**Who-did-What** collects its corpus from news and provides options for questions similar to CBT. Each question is formed from two independent articles: an article is treated as context to be read and a separate article on the same event is used to form the query.\r\n\r\nSource: [ChID: A Large-scale Chinese IDiom Dataset for Cloze Test](https://arxiv.org/abs/1906.01265)\r\nImage Source: [https://tticnlp.github.io/who_did_what/sample.html](https://tticnlp.github.io/who_did_what/sample.html)", "price": 925, "created_at": "2025-01-09 13:44:40.910119", "keyword": ["Question Answering", "Reading Comprehension", "Machine Reading Comprehension"], "id": "089566f6-1abf-4127-90b7-bc513672708b", "image_url": "https://production-media.paperswithcode.com/datasets/Who-did-What-0000003576-e107f700.jpg"}, {"title": "WiderPerson", "short_description": "WiderPerson contains a total of 13,382 images with 399,786 annotations, i.e., 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild.\r\n\r\nSource: [WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild](/paper/widerperson-a-diverse-dataset-for-dense)\r\nImage Source: [http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/)", "price": 586, "created_at": "2025-01-09 13:44:40.910184", "keyword": ["Object Detection"], "id": "3307be7d-5182-4356-8171-8750e7d2d06c", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_16.08.19.png"}, {"title": "WikiArt", "short_description": "**WikiArt** contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.\r\n\r\nSource: [Adding New Tasks to a Single Network with Weight Transformations using Binary Masks](https://arxiv.org/abs/1805.11119)\r\nImage Source: [https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e](https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e)", "price": 807, "created_at": "2025-01-09 13:44:40.910248", "keyword": ["Continual Learning", "Style Transfer", "Continual Learning", "Style Transfer"], "id": "12d1894a-cfbb-4537-ac42-05fe8e957ba9", "image_url": "https://production-media.paperswithcode.com/datasets/WikiArt-0000003290-f71d447c.jpg"}, {"title": "WikiBio", "short_description": "This dataset gathers 728,321 biographies from English Wikipedia. It aims at evaluating text generation algorithms. For each article, we provide the first paragraph and the infobox (both tokenized).", "price": 709, "created_at": "2025-01-09 13:44:40.910311", "keyword": ["Table-to-Text Generation", "Table-to-Text Generation"], "id": "6425e2fc-40b7-4990-a930-a3ad740f56af", "image_url": "https://production-media.paperswithcode.com/datasets/3dd52ced-5018-4807-8780-423c544b9468.png"}, {"title": "WikiConv", "short_description": "A corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations---including not only comments and replies, but also their modifications, deletions and restorations---this data offers an unprecedented view of online conversation.\r\n\r\nSource: [WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community](/paper/wikiconv-a-corpus-of-the-complete)", "price": 674, "created_at": "2025-01-09 13:44:40.910376", "keyword": ["Question Answering", "Knowledge Graphs", "Abuse Detection"], "id": "97e96394-4ea4-4266-bbe2-a8c0fa41addb", "image_url": ""}, {"title": "WikiHop", "short_description": "**WikiHop** is a multi-hop question-answering dataset. The query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WikiReading. A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WikiHop is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided. The task is to predict the correct answer given a query and multiple supporting documents.\r\n\r\nThe dataset includes a masked variant, where all candidates and their mentions in the supporting documents are replaced by random but consistent placeholder tokens.\r\n\r\nSource: [Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs](https://arxiv.org/abs/1905.07374)\r\nImage Source: [http://qangaroo.cs.ucl.ac.uk/](http://qangaroo.cs.ucl.ac.uk/)", "price": 57, "created_at": "2025-01-09 13:44:40.910440", "keyword": ["Question Answering", "Paraphrase Identification"], "id": "0386ced7-93e4-4626-8b94-964b126c1e14", "image_url": "https://production-media.paperswithcode.com/datasets/WikiHop-0000000621-5378e6a0_DWJ66Gt.jpg"}, {"title": "WikiHow", "short_description": "**WikiHow** is a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and represent high diversity styles.\r\n\r\nSource: [WikiHow: A Large Scale Text Summarization Dataset](https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization/)\r\nImage Source: [WikiHow: A Large Scale Text Summarization Dataset](https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization/)", "price": 50, "created_at": "2025-01-09 13:44:40.910545", "keyword": ["Text Summarization", "Sequence-to-sequence Language Modeling", "Abstractive Text Summarization"], "id": "b1a96674-e489-49fd-87c1-af36b63e2d7d", "image_url": "https://production-media.paperswithcode.com/datasets/WikiHow-0000002690-098cfdfb_LSuMUda.jpg"}, {"title": "WikiLarge", "short_description": "**WikiLarge** comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references\r\n\r\nSource: [Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders](https://arxiv.org/abs/2004.14693)\r\nImage Source: [https://arxiv.org/pdf/1904.02767.pdf](https://arxiv.org/pdf/1904.02767.pdf)", "price": 6, "created_at": "2025-01-09 13:44:40.910612", "keyword": ["Text Simplification", "Lexical Simplification"], "id": "7dffb53b-a03f-4ce9-ad32-a26958787fb5", "image_url": "https://production-media.paperswithcode.com/datasets/WikiLarge-0000000036-2cb04558_Wqy6bNy.jpg"}, {"title": "WikiMovies", "short_description": "WikiMovies is a dataset for question answering for movies content. It contains ~100k questions in the movie domain, and was designed to be answerable by using either a perfect KB (based on OMDb),\r\n\r\nSource: [ WikiMovies](https://research.fb.com/downloads/babi/)", "price": 486, "created_at": "2025-01-09 13:44:40.910676", "keyword": ["Question Answering", "Reading Comprehension", "Open-Domain Question Answering", "Question Answering", "Reading Comprehension", "Open-Domain Question Answering"], "id": "bad51c70-1a77-4e7d-93e3-cc6576c2fd74", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_13.42.59.png"}, {"title": "WikiQA", "short_description": "The **WikiQA** corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. In order to reflect the true information need of general users, Bing query logs were used as the question source. Each question is linked to a Wikipedia page that potentially has the answer. Because the summary section of a Wikipedia page provides the basic and usually most important information about the topic, sentences in this section were used as the candidate answers. The corpus includes 3,047 questions and 29,258 sentences, where 1,473 sentences were labeled as answer sentences to their corresponding questions.\r\n\r\nSource: [http://aka.ms/WikiQA](http://aka.ms/WikiQA)\r\nImage Source: [Yang et al](https://www.aclweb.org/anthology/D15-1237)", "price": 476, "created_at": "2025-01-09 13:44:40.910741", "keyword": ["Question Answering", "Answer Selection", "Question Answering", "Answer Selection"], "id": "d9fb2fa1-da34-426e-ae10-4e24c4810ce5", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_9.48.24_PM.png"}, {"title": "WikiReading", "short_description": "WikiReading is a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs).\r\n\r\nSource: [WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia](https://www.aclweb.org/anthology/P16-1145.pdf)\r\nImage Source: [Hewlett et al](https://arxiv.org/pdf/1608.03542v2.pdf)", "price": 57, "created_at": "2025-01-09 13:44:40.910805", "keyword": ["Question Answering", "Reading Comprehension", "Information Retrieval"], "id": "722a6ffb-9d3d-4cb7-8c40-f63ab9355874", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-27_at_13.54.23.png"}, {"title": "WikiSQL", "short_description": "**WikiSQL** consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases.\r\n\r\nSource: [SQL-to-Text Generation with Graph-to-Sequence Model](https://arxiv.org/abs/1809.05255)\r\nImage Source: [https://blog.einstein.ai/how-to-talk-to-your-database/](https://blog.einstein.ai/how-to-talk-to-your-database/)", "price": 726, "created_at": "2025-01-09 13:44:40.910872", "keyword": ["Semantic Parsing", "Code Generation", "SQL-to-Text", "Sql Chatbots", "Semantic Parsing", "Code Generation", "SQL-to-Text", "Sql Chatbots"], "id": "94d100da-2f42-4248-a043-d674c7208305", "image_url": "https://production-media.paperswithcode.com/datasets/WikiSQL-0000000026-0230f53d_9oGhCvq.jpg"}, {"title": "WikiSum", "short_description": "**WikiSum** is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively.\r\n\r\nSource: [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf)\r\nImage Source: [https://arxiv.org/pdf/1801.10198.pdf](https://arxiv.org/pdf/1801.10198.pdf)", "price": 727, "created_at": "2025-01-09 13:44:40.910937", "keyword": ["Abstractive Text Summarization", "Document Summarization", "Multi-Document Summarization"], "id": "b074f986-2b8c-47d7-b5dd-8ce6f176dfd8", "image_url": "https://production-media.paperswithcode.com/datasets/WikiSum-0000003570-0f9fc440.jpg"}, {"title": "WikiTableQuestions", "short_description": "**WikiTableQuestions** is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. WikiTableQuestions contains 22,033 questions. The questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets and datasets for querying knowledge bases. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions.\r\n\r\nSource: [Explaining Queries over Web Tables to Non-Experts](https://arxiv.org/abs/1808.04614)\r\nImage Source: [https://ppasupat.github.io/WikiTableQuestions/](https://ppasupat.github.io/WikiTableQuestions/)", "price": 271, "created_at": "2025-01-09 13:44:40.911001", "keyword": ["Semantic Parsing"], "id": "ba78b59e-2087-4a3e-b953-345591ced84f", "image_url": "https://production-media.paperswithcode.com/datasets/WikiTableQuestions-0000002557-a27d8922_j4dwyIS.jpg"}, {"title": "WikiText-103", "short_description": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\r\n\r\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\r\n\r\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)", "price": 512, "created_at": "2025-01-09 13:44:40.911065", "keyword": ["Text Generation", "Language Modelling", "Text Generation", "Language Modelling"], "id": "fd488553-f1a2-4544-9022-c23b49834a5c", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-08_at_09.24.11_n92zV88.png"}, {"title": "WikiText-2", "short_description": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\r\n\r\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\r\n\r\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)", "price": 982, "created_at": "2025-01-09 13:44:40.911129", "keyword": ["Text Generation", "Language Modelling", "Masked Language Modeling", "Causal Language Modeling", "Text Generation", "Language Modelling", "Masked Language Modeling", "Causal Language Modeling"], "id": "4cbd6662-e371-4812-963a-e9044b30d55e", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-08_at_09.24.11.png"}, {"title": "WildDash", "short_description": "WildDash is a benchmark evaluation method is presented that uses the meta-information to calculate the robustness of a given algorithm with respect to the individual hazards.\r\n\r\nSource: [WildDash - Creating Hazard-Aware Benchmarks](/paper/wilddash-creating-hazard-aware-benchmarks)\r\nImage Source: [https://wilddash.cc/](https://wilddash.cc/)", "price": 896, "created_at": "2025-01-09 13:44:40.911194", "keyword": ["Semantic Segmentation", "Instance Segmentation", "Domain Generalization", "Autonomous Driving"], "id": "2a87aae7-9c2b-4791-8c8b-060525854506", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_19.04.46.png"}, {"title": "WinoBias", "short_description": "**WinoBias** contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.\r\n\r\nSource: [WinoBias](https://uclanlp.github.io/corefBias/overview)\r\nImage Source: [https://uclanlp.github.io/corefBias/overview](https://uclanlp.github.io/corefBias/overview)", "price": 542, "created_at": "2025-01-09 13:44:40.911258", "keyword": ["Coreference Resolution", "Fairness", "Word Embeddings"], "id": "b4aa75a3-7c3e-4c0d-ae6a-ed991d10ff91", "image_url": "https://production-media.paperswithcode.com/datasets/WinoBias-0000003568-dfb50605.jpg"}, {"title": "Wireframe", "short_description": "The **Wireframe** dataset consists of 5,462 images (5,000 for training, 462 for test) of indoor and outdoor man-made scenes.\r\n\r\nSource: [MCMLSD: A Probabilistic Algorithm and Evaluation Framework for Line Segment Detection](https://arxiv.org/abs/2001.01788)\r\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf)", "price": 56, "created_at": "2025-01-09 13:44:40.911322", "keyword": ["Multi-Task Learning", "Line Segment Detection"], "id": "51f01bac-ccad-4b18-9be4-a7f98b0a58c0", "image_url": "https://production-media.paperswithcode.com/datasets/Wireframe-0000003424-06900425.jpg"}, {"title": "Wizard-of-Oz", "short_description": "The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.\r\n\r\nDescription from [NLP Progress](http://nlpprogress.com/english/dialogue.html)\r\n\r\nImage source: [Mrk\u0161i\u0107 et al.](https://arxiv.org/pdf/1606.03777.pdf)", "price": 898, "created_at": "2025-01-09 13:44:40.911387", "keyword": ["Dialogue State Tracking"], "id": "0eec6eb1-700d-45ed-a210-907ea3128fc2", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-15_at_16.47.48.png"}, {"title": "XNLI", "short_description": "The **Cross-lingual Natural Language Inference** (**XNLI**) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples.\r\n\r\nSource: [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\r\nImage Source: [https://github.com/facebookresearch/XNLI](https://github.com/facebookresearch/XNLI)", "price": 251, "created_at": "2025-01-09 13:44:40.911451", "keyword": ["Text Classification", "Natural Language Inference", "Chinese Sentence Pair Classification", "FLUE", "Cross-Lingual Natural Language Inference", "Text Classification", "Natural Language Inference", "Chinese Sentence Pair Classification", "FLUE", "Cross-Lingual Natural Language Inference"], "id": "d4a22567-4578-415f-bf5b-a1b8cbdddca5", "image_url": "https://production-media.paperswithcode.com/datasets/XNLI-0000000232-ad27411e_qDwJhwB.jpg"}, {"title": "XSum", "short_description": "The Extreme Summarization (**XSum**) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question \u201cWhat is the article about?\u201d. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.\n\nSource: [https://arxiv.org/pdf/1808.08745.pdf](https://arxiv.org/pdf/1808.08745.pdf)\nImage Source: [https://arxiv.org/pdf/1808.08745.pdf](https://arxiv.org/pdf/1808.08745.pdf)", "price": 905, "created_at": "2025-01-09 13:44:40.911516", "keyword": ["Text Summarization", "Sequence-to-sequence Language Modeling", "Summarization", "Abstractive Text Summarization", "Extreme Summarization"], "id": "c1440d7c-a516-44a9-a243-71d10d9bdbb4", "image_url": "https://production-media.paperswithcode.com/datasets/XSum-0000003374-68ecada6.jpg"}, {"title": "YAGO", "short_description": "**Yet Another Great Ontology** (**YAGO**) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each.\r\n\r\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\r\nImage Source: [https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/)", "price": 984, "created_at": "2025-01-09 13:44:40.911579", "keyword": ["Link Prediction", "Time-interval Prediction", "Triple Classification", "Link Prediction", "Time-interval Prediction", "Triple Classification"], "id": "773782cd-919a-4f81-806d-cd583dc05036", "image_url": "https://production-media.paperswithcode.com/datasets/yago.png"}, {"title": "YCB-Video", "short_description": "The **YCB-Video** dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames.\r\n\r\nSource: [https://rse-lab.cs.washington.edu/projects/posecnn/](https://rse-lab.cs.washington.edu/projects/posecnn/)\r\nImage Source: [https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565](https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565)", "price": 768, "created_at": "2025-01-09 13:44:40.911643", "keyword": ["Pose Estimation", "", "6D Pose Estimation using RGBD", "6D Pose Estimation using RGB", "6D Pose Estimation", "Occluded 3D Object Symmetry Detection", "Symmetry Detection"], "id": "78dcc696-96ac-4950-9eee-c5dc4b6b5eb6", "image_url": "https://production-media.paperswithcode.com/datasets/YCB-Video-0000000399-4c88470d_8o7pP6O.jpg"}, {"title": "YFCC100M", "short_description": "YFCC100M is a that dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014.\r\n\r\nSource: [YFCC100M: The New Data in Multimedia Research](https://arxiv.org/pdf/1503.01817v2.pdf)\r\nImage Source: [Thomee et al](https://arxiv.org/pdf/1503.01817v2.pdf)", "price": 771, "created_at": "2025-01-09 13:44:40.911710", "keyword": ["Image Classification", "Image Retrieval"], "id": "4d41590a-ce74-4f8e-8453-2913f4c9d52e", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-01-28_at_14.42.38.png"}, {"title": "Yahoo! Answers", "short_description": "The Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information.\r\nSource:[github](https://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset/tree/master/dataset)", "price": 161, "created_at": "2025-01-09 13:44:40.911774", "keyword": ["Text Classification", "Unsupervised Text Classification", "Topic Classification"], "id": "8f037c48-5e45-452e-9311-2414e5b787a4", "image_url": ""}, {"title": "YouCook2", "short_description": "**YouCook2** is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\r\n\r\nSource: [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)\r\nImage Source: [https://competitions.codalab.org/competitions/20594](https://competitions.codalab.org/competitions/20594)", "price": 530, "created_at": "2025-01-09 13:44:40.911838", "keyword": ["Video Retrieval", "Action Classification", "Video Captioning", "Zero-Shot Video Retrieval", "Dense Video Captioning", "Zero-shot dense video captioning"], "id": "6b8256dd-38e6-4cfe-8d6d-0e068b0541ed", "image_url": "https://production-media.paperswithcode.com/datasets/YouCook2-0000001953-4c22ada7_SfOsMGA.jpg"}, {"title": "YouTube-8M", "short_description": "The **YouTube-8M** dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\r\n\r\nSource: [Audio-Visual Embedding for Cross-Modal Music Video Retrieval through Supervised Deep CCA](https://arxiv.org/abs/1908.03744)", "price": 821, "created_at": "2025-01-09 13:44:40.911902", "keyword": ["Video Prediction", "Video Classification"], "id": "fcdce8b3-cf5e-4e2b-be5a-3b0ec428d208", "image_url": "https://production-media.paperswithcode.com/datasets/youtube8.jpg"}, {"title": "YouTube-VOS 2018", "short_description": "Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration.\r\n\r\nSource: [CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing](https://arxiv.org/abs/1910.00132)\r\nImage Source: [https://youtube-vos.org/](https://youtube-vos.org/)", "price": 581, "created_at": "2025-01-09 13:44:40.911966", "keyword": ["Video Object Segmentation", "Visual Object Tracking", "Semi-Supervised Video Object Segmentation", "Video Inpainting", "One-shot visual object segmentation"], "id": "f06fda4b-38b3-4a74-a8ea-5c206ef4a72b", "image_url": "https://production-media.paperswithcode.com/datasets/YouTube-VOS-0000001220-93134e8f_7QW7R3Y.jpg"}, {"title": "ZINC", "short_description": "**ZINC** is a free database of commercially-available compounds for virtual screening. ZINC contains over 230 million purchasable compounds in ready-to-dock, 3D formats. ZINC also contains over 750 million purchasable compounds that can be searched for analogs.\r\n\r\nSource: [ZINC15](http://zinc15.docking.org/)\r\nImage Source: [https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559)", "price": 174, "created_at": "2025-01-09 13:44:40.912033", "keyword": ["Graph Regression", "Graph Ranking", "Molecular Graph Generation"], "id": "1a835253-e35a-4faf-ab46-1ac3c0109d29", "image_url": "https://production-media.paperswithcode.com/datasets/ZINC-0000002261-91b5cf9b_VahtKkm.jpg"}, {"title": "aPY", "short_description": "**aPY** is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, \u2026, zebra).\r\n\r\nSource: [From Classical to Generalized Zero-Shot Learning: a Simple Adaptation Process](https://arxiv.org/abs/1809.10120)\r\nImage Source: [https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf](https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf)", "price": 698, "created_at": "2025-01-09 13:44:40.912098", "keyword": ["Zero-Shot Learning", "Few-Shot Image Classification", "Generalized Zero-Shot Learning"], "id": "c5fbe45f-fb4e-4e1c-8d1e-df67d91c41a4", "image_url": "https://production-media.paperswithcode.com/datasets/aPY-0000003428-8743aa94.jpg"}, {"title": "decaNLP", "short_description": "Natural Language Decathlon Benchmark (decaNLP) is a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. The tasks as cast as question answering over a context.\r\n\r\nSource: [The Natural Language Decathlon: Multitask Learning as Question Answering](https://arxiv.org/pdf/1806.08730v1.pdf)\r\nImage Source: [http://decanlp.com/](http://decanlp.com/)", "price": 187, "created_at": "2025-01-09 13:44:40.912162", "keyword": ["Question Answering", "Natural Language Understanding"], "id": "4bbfb6ae-85f2-446f-84a5-0bd9b12df42c", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-02_at_12.47.32.png"}, {"title": "fMoW", "short_description": "Functional Map of the World (fMoW) is a dataset that aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. \r\n\r\nSource: [Functional Map of the World](/paper/functional-map-of-the-world)\r\nImage Source: [Christie et al](https://arxiv.org/pdf/1711.07846.pdf)", "price": 649, "created_at": "2025-01-09 13:44:40.912225", "keyword": ["Image Classification", "Object Detection", "Disaster Response"], "id": "b4f4cef3-38b0-4266-aacc-31e4cbd16fa7", "image_url": "https://production-media.paperswithcode.com/datasets/Screenshot_2021-02-01_at_15.30.25.png"}, {"title": "iKala", "short_description": "The **iKala** dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.\r\n\r\nThis dataset is not available anymore.\r\n\r\nSource: [http://mac.citi.sinica.edu.tw/ikala/](http://mac.citi.sinica.edu.tw/ikala/)", "price": 101, "created_at": "2025-01-09 13:44:40.912289", "keyword": ["Speech Separation", "Information Retrieval", "Style Transfer", "Music Information Retrieval"], "id": "cdaab45e-e41f-4af2-9aae-7654e5864069", "image_url": ""}, {"title": "iLIDS-VID", "short_description": "The **iLIDS-VID** dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.\n\nSource: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)\nImage Source: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)", "price": 311, "created_at": "2025-01-09 13:44:40.912353", "keyword": ["Person Re-Identification", "Unsupervised Person Re-Identification", "Video-Based Person Re-Identification"], "id": "d797dca0-5cc2-40d5-a817-0823628307eb", "image_url": "https://production-media.paperswithcode.com/datasets/iLIDS-VID-0000000094-f63d8099_YlfH8nF.jpg"}, {"title": "iNaturalist", "short_description": "The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category \u201cPlantae (Plant)\u201d has 196,613 images from 2,101 categories; whereas the smallest super-category \u201cProtozoa\u201d only has 381 images from 4 categories.\r\n\r\nSource: [Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning](https://arxiv.org/abs/1806.06193)\r\nImage Source: [https://github.com/visipedia/inat_comp/tree/master/2017](https://github.com/visipedia/inat_comp/tree/master/2017)", "price": 119, "created_at": "2025-01-09 13:44:40.912421", "keyword": ["Image Classification", "Image Generation", "Few-Shot Image Classification", "Image Retrieval", "Fine-Grained Image Classification", "Long-tail Learning"], "id": "0eaf7c6b-baac-4118-8e02-c2557b4e6075", "image_url": "https://production-media.paperswithcode.com/datasets/iNaturalist-0000000484-786cf0fc_WX6hVg3.jpg"}, {"title": "iPinYou", "short_description": "The **iPinYou** Global RTB(Real-Time Bidding) Bidding Algorithm Competition is organized by iPinYou from April 1st, 2013 to December 31st, 2013.The competition has been divided into three seasons. For each season, a training dataset is released to the competition participants, the testing dataset is reserved by iPinYou. The complete testing dataset is randomly divided into two parts: one part is the leaderboard testing dataset to score and rank the participating teams on the leaderboard, and the other part is reserved for the final offline evaluation. The participant's last offline submission is evaluated by the reserved testing dataset to get a team's offline final score. This dataset contains all three seasons training datasets and leaderboard testing datasets.The reserved testing datasets are withheld by iPinYou. The training dataset includes a set of processed iPinYou DSP bidding, impression, click, and conversion logs.\n\nSource: [iPinYou Global RTB Bidding Algorithm Competition Dataset](https://contest.ipinyou.com/)\nImage Source: [http://contest.ipinyou.com/ipinyou-dataset.pdf](http://contest.ipinyou.com/ipinyou-dataset.pdf)", "price": 428, "created_at": "2025-01-09 13:44:40.912487", "keyword": ["Click-Through Rate Prediction", "Click-Through Rate Prediction"], "id": "760787e6-9360-43c5-8e12-823d20c618bf", "image_url": "https://production-media.paperswithcode.com/datasets/iPinYou-0000000675-30f1d6ce_XmD0qbt.jpg"}, {"title": "iSUN", "short_description": "**iSUN** is a ground truth of gaze traces on images from the SUN dataset. The collection is partitioned into 6,000 images for training, 926 for validation and 2,000 for test.\r\n\r\nSource: [End-to-end Convolutional Network for Saliency Prediction](https://arxiv.org/abs/1507.01422)\r\nImage Source: [http://turkergaze.cs.princeton.edu/](http://turkergaze.cs.princeton.edu/)", "price": 766, "created_at": "2025-01-09 13:44:40.912555", "keyword": ["Out-of-Distribution Detection", "Saliency Detection", "Saliency Prediction"], "id": "817de045-dec8-4914-967a-7190b35ded0b", "image_url": "https://production-media.paperswithcode.com/datasets/iSUN-0000003500-0606ae02.jpg"}, {"title": "mini-Imagenet", "short_description": "mini-Imagenet is proposed by  **Matching Networks for One Shot Learning\r\n. In NeurIPS, 2016**. This dataset consists of 50000 training images and 10000 testing images, evenly\r\ndistributed across 100 classes.", "price": 694, "created_at": "2025-01-09 13:44:40.912620", "keyword": ["Few-Shot Image Classification", "Few-Shot Learning", "Cross-Domain Few-Shot", "Unsupervised Few-Shot Image Classification", "Few-Shot Class-Incremental Learning"], "id": "50a1e75f-cf8b-4249-86e5-66b56de7d0be", "image_url": ""}, {"title": "nuScenes", "short_description": "The **nuScenes** dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360\u00b0 coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.\r\n\r\nSource: [PointPainting: Sequential Fusion for 3D Object Detection](https://arxiv.org/abs/1911.10150)", "price": 585, "created_at": "2025-01-09 13:44:40.912685", "keyword": ["Object Detection", "Instance Segmentation", "3D Object Detection", "Semi-Supervised Semantic Segmentation", "Trajectory Prediction", "3D Semantic Segmentation", "Lane Detection", "3D Multi-Object Tracking", "LIDAR Semantic Segmentation", "Weather Forecasting", "Weakly supervised Semantic Segmentation", "HD semantic map learning", "Motion Detection", "Motion Planning", "Trajectory Planning", "Bird's-Eye View Semantic Segmentation"], "id": "b91c9c62-404e-407f-89a0-5a8e1c18cdec", "image_url": "https://production-media.paperswithcode.com/datasets/Screen_Shot_2021-01-28_at_5.57.43_PM.png"}, {"title": "smallNORB", "short_description": "The **smallNORB** dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).\r\nThe training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).\r\n\r\nSource: [https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/](https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/)\r\nImage Source: [https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10](https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10)", "price": 250, "created_at": "2025-01-09 13:44:40.912750", "keyword": ["Image Classification", "Disentanglement"], "id": "2ea28785-7add-4f1b-91f2-d543c358035d", "image_url": "https://production-media.paperswithcode.com/datasets/smallNORB-0000002308-a84bc9da_PWU5EIY.jpg"}, {"title": "xView", "short_description": "xView is one of the largest publicly available datasets of overhead imagery. It contains images from complex scenes around the world, annotated using bounding boxes. It contains over 1M object instances from 60 different classes.\r\n\r\nSource: [xView dataset](http://xviewdataset.org/)", "price": 600, "created_at": "2025-01-09 13:44:40.912814", "keyword": ["Object Detection", "Image Super-Resolution", "Disaster Response", "Geophysics"], "id": "8de75d2a-c810-469c-8067-7ab31f4a2416", "image_url": "https://production-media.paperswithcode.com/datasets/example6.jpg"}]}